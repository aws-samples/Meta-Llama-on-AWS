{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama 3 on Amazon SageMaker with TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Llama 3](https://llama.meta.com/llama3/) are pretrained models trained over 15 trillion tokens, â€“ a training dataset 7x larger than that used for Llama 2, with a 8k context length. The models excels at text summarization and accuracy, text classification and nuance, sentiment analysis and nuance reasoning, language modeling, dialogue systems, code generation, and following instructions. To learn more about Llama 3 models, click [here](https://llama.meta.com/llama3/).\n",
    "\n",
    "SageMaker has rolled out [TensorRT-LLM container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we combine the strengths of two powerful tools: [DJL](https://docs.djl.ai/) (Deep Java Library) for the serving framework and [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/) for distributed large language model inference on Nvidia. DJLServing, a high-performance universal model serving solution powered by DJL, handles the overall serving architecture.\n",
    "\n",
    "In our setup, vLLM handles the core LLM inference tasks, leveraging its optimizations to achieve high performance and low latency. DJLServing manages the broader serving infrastructure, handling incoming requests, load balancing, and coordinating with vLLM for efficient inference.\n",
    "\n",
    "This combination allows us to deploy the `Llama 3 8B` model across GPUs on the `ml.g5.12xlarge` instance with optimal resource utilization. vLLM's efficiencies in memory management and request handling enable us to serve this large model with improved throughput compared to traditional serving methods. To learn more about DJL, DJLServing, and TensorRT-LLM you can refer to this [blog post](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.132 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install sagemaker==2.224.2 -qU --force --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.224.2\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570598552974:role/txt2sql-SageMakerExecutionRole-PAgMr5TND4x0\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# execution role for the endpoint\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# sagemaker session for interacting with different AWS APIs\n",
    "sess = sagemaker.session.Session()\n",
    "\n",
    "# Region\n",
    "region_name = sess._region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image URI of the DJL Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMI DLCs offer a low-code interface that simplifies using state-of-the-art inference optimization techniques and hardware. LMI allows you to apply tensor parallelism; the latest efficient attention, batching, quantization, and memory management techniques; token streaming; and much more, by just requiring the model ID and optional model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See available Large Model Inference DLC's [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) and for more details [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/announcements/deepspeed-deprecation.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCL Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.28.0-tensorrtllm0.9.0-cu122\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-tensorrtllm\",\n",
    "    region=region_name,\n",
    "    version=\"0.28.0\"\n",
    ")\n",
    "print(f\"DCL Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Environment Variable Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of settings that we use in this configuration file:\n",
    "\n",
    "- `HF_MODEL_ID`: The model id of a pretrained model hosted inside a model repository on [huggingface.co](https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co. This is an optional setting and is not needed in the scenario where you are brining your own model. If you are getting your own model, you can include the URI of the Amazon S3 bucket that contains the model.\n",
    "- `HF_TOKEN`: Some models on the HuggingFace Hub are gated and require permission from the owner to access. To deploy a gated model from the HuggingFace Hub using LMI, you must provide an [Access Token](https://huggingface.co/docs/hub/security-tokens) via this environment variable.\n",
    "- `OPTION_ENGINE`: The engine for DJL to use. In this case, we intend to use [MPI](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html). MPI is used to operate on single machine multi-gpu or multiple machines multi-gpu use cases.\n",
    "- `OPTION_DTYPE`: The data type you plan to cast the model weights to. If not provided, LMI will use fp16.\n",
    "- `OPTION_TGI_COMPAT`: To get the same response output as HuggingFace's Text Generation Inference, you can use the env `OPTION_TGI_COMPAT=true`.\n",
    "- `OPTION_TASK`: The task used in Hugging Face for different pipelines. Default is text-generation. For further reading on DJL parameters on SageMaker, follow the [link](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/deepspeed_user_guide.html)\n",
    "- `OPTION_ROLLING_BATCH`: Enables continuous batching (iteration level batching) with one of the supported backends. Available backends differ by container, see [Inference Library Configurations](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/configurations.html#inference-library-configuration) for mappings.\n",
    "    - In the TensorRT-LLM Container:\n",
    "        - use `OPTION_ROLLING_BATCH=trtllm` to use TensorRT-LLM (this is the default)\n",
    "- `TENSOR_PARALLEL_DEGREE`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. Setting this to `max`, which will shard the model across all available GPUs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests.\n",
    "- `OPTION_MAX_INPUT_LEN`: Maximum input token size you expect the model to have per request. This is a compilation parameter that set to the model for Just-in-Time compilation. If you set this value too low, the model will unable to consume the long input. LMI also validates this at runtime for each request.\n",
    "- `OPTION_MAX_OUTPUT_LEN`: Maximum output token size you expect the model to have per request. This is a compilation parameter that set to the model for Just-in-Time compilation. If you set this value too low, the model will unable to produce tokens beyond the value you set.\n",
    "- `OPTION_TRUST_REMOTE_CODE`: If the model artifacts contain custom modeling code, you should set this to true after validating the custom code is not malicious. If you are using a HuggingFace Hub model id, you should also specify HF_REVISION to ensure you are using artifacts and code that you have validated.\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html) and [LMI Starting Guide](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/trt_llm_user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face Model Id\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Environment variables\n",
    "hf_token = \"<REPLACE WITH YOUR TOKEN>\" # Use for gated models\n",
    "rolling_batch = \"trtllm\"\n",
    "max_output_len = 8192\n",
    "\n",
    "env = {}\n",
    "env['HF_MODEL_ID'] = model_id\n",
    "env['OPTION_ROLLING_BATCH'] = rolling_batch\n",
    "env['OPTION_DTYPE'] = \"bf16\"\n",
    "env['OPTION_TGI_COMPAT'] = \"true\"\n",
    "env['OPTION_ENGINE'] = \"MPI\"\n",
    "env['OPTION_TASK'] = \"text-generation\"\n",
    "env['TENSOR_PARALLEL_DEGREE'] = \"max\"\n",
    "env['OPTION_MAX_INPUT_LEN'] = json.dumps(max_output_len - 1)\n",
    "env['OPTION_MAX_OUTPUT_LEN'] = json.dumps(max_output_len)\n",
    "env['OPTION_DEVICE_MAP'] = \"auto\"\n",
    "# env['OPTION_MAX_ROLLING_BATCH'] = \"\"\n",
    "# env['OPTION_TRUST_REMOTE_CODE'] = \"true\"\n",
    "    \n",
    "# Include HF token for gated models\n",
    "if hf_token != \"<REPLACE WITH YOUR TOKEN>\":\n",
    "    env['HF_TOKEN'] = hf_token\n",
    "else:\n",
    "    print(\"Llama models are gated, please add your HF token before you continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_type: ml.g5.12xlarge\n",
      "model_id: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "endpoint_name: llama3-8b-instruct-tensorrt-llm-2024-07-06-11-33-28-546\n"
     ]
    }
   ],
   "source": [
    "# SageMaker Instance Type\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Endpoint name\n",
    "endpoint_name_prefix = \"llama3-8b-instruct-tensorrt-llm\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(endpoint_name_prefix)\n",
    "\n",
    "print(f\"instance_type: {instance_type}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "model = sagemaker.Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    role=role,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference and chat with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code generation questions\n",
    "\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\"\n",
    "4. \"Develop a JavaScript module for a real-time collaborative text editor using operational transformation. Implement functions for handling concurrent edits, conflict resolution, and syncing with a backend server.\"\n",
    "5. \"Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging.\"\n",
    "6. \"Implement a microservices architecture in Go for a basic e-commerce platform. Include services for user authentication, product catalog, order processing, and inventory management. Use gRPC for inter-service communication and implement circuit breaking for resilience.\"\n",
    "7. \"Provide me with a python script to recompile huggingface models with optimum neuron for inferentia\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize sagemaker client with the endpoint created in the prior step\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's correct! Building a website can be a straightforward process if you break it down into manageable steps. Here are the 10 simple steps to build a website:\n",
      "\n",
      "1. **Plan your website's purpose and audience**: Determine the purpose of your website, who your target audience is, and what you want to achieve with your website.\n",
      "\n",
      "2. **Choose a domain name**: Register a unique and memorable domain name that reflects your website's brand and is easy to spell and remember.\n",
      "\n",
      "3. **Select a web hosting service**: Choose a reliable web hosting service that meets your needs, including storage space, bandwidth, and customer support.\n",
      "\n",
      "4. **Design your website's layout and structure**: Plan the layout and structure of your website, including the number of pages, navigation menu, and content organization.\n",
      "\n",
      "5. **Create your website's content**: Write and design the content for your website, including text, images, videos, and other multimedia elements.\n",
      "\n",
      "6. **Choose a website builder or CMS**: Decide whether to use a website builder like Wix, Squarespace, or Weebly, or a Content Management System (CMS) like WordPress, Joomla, or Drupal.\n",
      "\n",
      "7. **Build your website**: Use your chosen website builder or CMS to create your website, following the design and structure you planned earlier.\n",
      "\n",
      "8. **Customize your website's design and functionality**: Personalize your website's design and add features like contact forms, social media links, and e-commerce integrations.\n",
      "\n",
      "9. **Test and launch your website**: Test your website for functionality, usability, and performance, and launch it for public access.\n",
      "\n",
      "10. **Maintain and update your website**: Regularly update your website's content, fix broken links, and ensure your website remains secure and compatible with the latest browsers and devices.\n",
      "\n",
      "By following these 10 simple steps, you can build a professional-looking website that effectively communicates your message and achieves your online goals.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{message_prompt}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\".format(\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    message_prompt=\"Building a website can be done in 10 simple steps:\"\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(inputs)\n",
    "print(response[0]['generated_text'].strip().replace('<|eot_id|>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using Boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize sagemaker client with boto3 using the endpoint created from prior step\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break it down step by step!\n",
      "\n",
      "You bought 6 ice cream cones for $1.25 each, so the total cost of the ice cream is:\n",
      "\n",
      "6 cones x $1.25 per cone = $7.50\n",
      "\n",
      "You paid with a $10 bill, so to find out how much change you got back, we need to subtract the cost of the ice cream from the $10 bill:\n",
      "\n",
      "$10 (initial amount) - $7.50 (cost of ice cream) = $2.50\n",
      "\n",
      "So, you got $2.50 in change!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{message_prompt}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\".format(\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    message_prompt=\"\"\"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. \n",
    "               How many dollars did I get back? Explain first before answering.\"\"\"\n",
    ")\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"do_sample\": False\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "print(json.loads(response)[0]['generated_text'].strip().replace('<|eot_id|>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, we demonstrated how to use SageMaker large model inference containers to host Codestral 22B. We used DeepSpeedâ€™s model parallel techniques with multiple GPUs on a single SageMaker machine learning instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "sess.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case the end point failed we still want to delete the model\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
