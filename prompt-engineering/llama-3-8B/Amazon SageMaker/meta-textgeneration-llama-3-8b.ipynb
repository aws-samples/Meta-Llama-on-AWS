{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Llama3 Large Language Model in SageMaker JumpStart - Best Practices for Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we showcase best practices for prompting Llama3 Large Language Model using Amazon SageMaker Jumpstart. In this case we're working with Llama 3 8B model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker\n",
    "%pip install --upgrade --quiet pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1bb837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-3-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.2.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Specify the model ID for the HuggingFace Llama 3 8b Instruct LLM model\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "accept_eula = True\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231dfb4d-b064-4694-b44d-02b1fc2b7b70",
   "metadata": {},
   "source": [
    "***\n",
    "In cases where the endpoint has already been deployed, we can just invoke the model directly by using the Boto3 (AWS Python SDK) to call the model endpoint\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "This model supports the following inference payload parameters. You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **details:** Return generation details, to include output token logprobs and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a8d1df-87a7-45e4-b9fc-282d4a68b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def display_inputs_and_outputs(payload, response):\n",
    "    inputs = remove_tags(payload[\"inputs\"])\n",
    "    payload=re.sub(r'[ \\t]+', ' ', inputs)\n",
    "    print(payload)\n",
    "    response = remove_formatting(response[\"generated_text\"])\n",
    "    print(response)\n",
    "\n",
    "def remove_tags(text):\n",
    "    return text.replace(\"<|begin_of_text|>\", \"\").replace(\"<|start_header_id|>\", \"\").replace(\"<|end_header_id|>\\n\", \":\").replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "def remove_formatting(text):\n",
    "    pattern = r\"<.*?>\"\n",
    "    return re.sub(pattern, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b18c3-d950-4ca3-b44f-2e0c19c2015c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Zero-shot Prompting\n",
    "\n",
    "***\n",
    "This method involves presenting a language model with a task or question it hasn't specifically been trained for. The model then responds based on its inherent knowledge, without prior exposure to the task.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50b9e67-c2f8-4b79-998f-5ee0406f3c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System: Provide a brief summary of the causes of World War II.\n",
      " User: The causes of World War II include...\n",
      " Assistant: \n",
      " Here is a brief summary of the causes of World War II:\n",
      "\n",
      "**Short-term causes:**\n",
      "\n",
      "1. **The invasion of Poland**: Germany, under Adolf Hitler, invaded Poland on September 1, 1939, which led to the United Kingdom and France declaring war on Germany.\n",
      "2. **The Molotov-Ribbentrop Pact**: Germany and the Soviet Union signed a non-aggression pact in August 1939, which included a secret protocol dividing Eastern Europe into Soviet and German spheres of influence.\n",
      "3. **The Battle of France**: Germany's rapid defeat of France in May-June 1940 led to the fall of the French government and the establishment of a puppet regime.\n",
      "\n",
      "**Long-term causes:**\n",
      "\n",
      "1. **Treaty of Versailles**: The harsh penalties imposed on Germany after World War I, including heavy reparations and territorial losses, created resentment and a sense of injustice among the German people.\n",
      "2. **Economic crisis**: The global economic crisis of the 1930s, known as the Great Depression, led to high levels of unemployment and poverty, which created a fertile ground for extremist ideologies like Nazism.\n",
      "3. **Rise of fascist and nationalist movements**: The rise of fascist and nationalist movements in Europe, such as Nazism in Germany, Fascism in Italy, and Imperial Japan, created a climate of aggression and militarism.\n",
      "4. **Appeasement policy**: The policy of appeasement, pursued by Britain and France, which involved giving in to Germany's demands in the hope of avoiding war, emboldened Hitler and allowed him to pursue his aggressive expansionist policies.\n",
      "5. **Imperialism and colonial rivalries**: The competition for colonies and resources between European powers, particularly Britain and France, contributed to the tensions leading up to the war.\n",
      "\n",
      "These factors created a complex web of causes that ultimately led to the outbreak of World War II in September 1939.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>System<|end_header_id|>\n",
    "            Provide a brief summary of the causes of World War II.\n",
    "            <|eot_id|><|start_header_id|>User<|end_header_id|>\n",
    "            The causes of World War II include...\n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6415d-1507-4e7d-bea1-1da2f5944ab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Few-shot Prompting\n",
    "\n",
    "***\n",
    "Here, a language model is given a handful of examples, or \"shots,\" of a task before encountering a new instance of that same task. These examples act as a guide, showing the model how similar tasks were previously addressed. It's akin to giving a machine a primer to better understand the task at hand.\n",
    "\n",
    "Below is an example to see how the model responds without few shot prompting.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555e1a78-fc42-42da-8168-9e54cb50603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System: Extract the relevant information from the following parahrapgh and present it in a JSON format.\n",
      " User: Michael Smith, a 45-year-old teacher from Boston, Massachusetts, is an avid reader and enjoys gardening during his spare time.\n",
      " Assistant: \n",
      " Here is the extracted information in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"Michael Smith\",\n",
      "  \"age\": 45,\n",
      "  \"location\": \"Boston, Massachusetts\",\n",
      "  \"hobbies\": [\"reading\", \"gardening\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": r\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>System<|end_header_id|>\n",
    "            Extract the relevant information from the following parahrapgh and present it in a JSON format.\n",
    "            <|eot_id|><|start_header_id|>User<|end_header_id|>\n",
    "            Michael Smith, a 45-year-old teacher from Boston, Massachusetts, is an avid reader and enjoys gardening during his spare time.\n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e796c36-f481-4acb-83e3-b115e82c77ea",
   "metadata": {},
   "source": [
    "***\n",
    "Here is an example to see how the model responds with few shot prompting.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bcc76c-756e-405d-b5dc-1777d9e41062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System: Extract the relevant information from the following parahrapgh and present it in a JSON format.\n",
      " User: Michael Doe, a 45-year-old teacher from Boston, Massachusetts, is an avid reader and enjoys gardening during his spare time.\n",
      " Example 1:\n",
      " Paragraph: \"John Doe is a 32-year-old software engineer from San Francisco, California. He enjoys hiking and playing guitar in his free time.\"\n",
      " \"employee\": {\n",
      " \"fullname\": \"John Doe\",\n",
      " \"city\": \"San Francisco\",\n",
      " \"state\": \"California\",\n",
      " \"occupation\": \"software engineer\",\n",
      " \"hobbies\": [\n",
      " \"hiking\",\n",
      " \"playing guitar\"\n",
      " ],\n",
      " \"recentTravel\": \"not provided\"\n",
      " },\n",
      " Example 2:\n",
      " Paragraph: \"Emily Jax, a 27-year-old marketing manager from New York City, loves traveling and trying new cuisines. She recently visited Paris and enjoyed the city's rich cultural heritage.\"\n",
      " \"employee\": {\n",
      " \"fullname\": \"Emily Jax\",\n",
      " \"city\": \"New York City\",\n",
      " \"state\": \"New York\",\n",
      " \"occupation\": \"marketing manager\",\n",
      " \"hobbies\": [\n",
      " \"traveling\",\n",
      " \"trying new cuisines\"\n",
      " ],\n",
      " \"recentTravel\": \"Paris\"\n",
      " }\n",
      " Assistant: \n",
      " Here is the extracted information in JSON format:\n",
      "\n",
      "```\n",
      "\"employee\": {\n",
      "    \"fullname\": \"Michael Doe\",\n",
      "    \"city\": \"Boston\",\n",
      "    \"state\": \"Massachusetts\",\n",
      "    \"occupation\": \"teacher\",\n",
      "    \"hobbies\": [\n",
      "        \"reading\",\n",
      "        \"gardening\"\n",
      "    ],\n",
      "    \"recentTravel\": \"not provided\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": r\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>System<|end_header_id|>\n",
    "            Extract the relevant information from the following parahrapgh and present it in a JSON format.\n",
    "            <|eot_id|><|start_header_id|>User<|end_header_id|>\n",
    "            Michael Doe, a 45-year-old teacher from Boston, Massachusetts, is an avid reader and enjoys gardening during his spare time.\n",
    "            Example 1:\n",
    "            Paragraph: \"John Doe is a 32-year-old software engineer from San Francisco, California. He enjoys hiking and playing guitar in his free time.\"\n",
    "            \"employee\": {\n",
    "                \"fullname\": \"John Doe\",\n",
    "                \"city\": \"San Francisco\",\n",
    "                \"state\": \"California\",\n",
    "                \"occupation\": \"software engineer\",\n",
    "                \"hobbies\": [\n",
    "                    \"hiking\",\n",
    "                    \"playing guitar\"\n",
    "                    ],\n",
    "                \"recentTravel\": \"not provided\"\n",
    "            },\n",
    "            Example 2:\n",
    "            Paragraph: \"Emily Jax, a 27-year-old marketing manager from New York City, loves traveling and trying new cuisines. She recently visited Paris and enjoyed the city's rich cultural heritage.\"\n",
    "            \"employee\": {\n",
    "                \"fullname\": \"Emily Jax\",\n",
    "                \"city\": \"New York City\",\n",
    "                \"state\": \"New York\",\n",
    "                \"occupation\": \"marketing manager\",\n",
    "                \"hobbies\": [\n",
    "                    \"traveling\",\n",
    "                    \"trying new cuisines\"\n",
    "                ],\n",
    "                \"recentTravel\": \"Paris\"\n",
    "            }\n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a084c0-38b8-4f85-b7b7-d057233e4582",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task Decomposition\n",
    "\n",
    "***\n",
    "Task decomposition is a powerful technique that enhances the performance of LLMs by breaking down complex tasks into smaller, manageable sub-tasks. This approach not only improves efficiency and accuracy but also allows for better resource management and adaptability to task complexity.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e66cbb-b5fb-4cbb-84c3-e16e5deb6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System: Break down the task of planning a vacation into smaller, manageable steps.\n",
      " User: 1. Choose a destination.\n",
      " 2. Set a budget.\n",
      " 3. Research accommodations.\n",
      " 4. Plan activities.\n",
      " 5. Book flights and accommodations.\n",
      " 6. Pack and prepare for the trip.\n",
      " Assistant: \n",
      " That's a great start! Here's a more detailed breakdown of the task of planning a vacation into smaller, manageable steps:\n",
      "\n",
      "**Step 1: Choose a Destination**\n",
      "\n",
      "* Brainstorm a list of potential destinations based on interests, budget, and time constraints\n",
      "* Research each destination to gather more information (e.g., weather, culture, activities, cost)\n",
      "* Narrow down the list to 2-3 top choices\n",
      "* Consider factors such as travel time, language, and visa requirements\n",
      "\n",
      "**Step 2: Set a Budget**\n",
      "\n",
      "* Determine how much you can afford to spend on the trip\n",
      "* Consider all expenses, including flights, accommodations, food, activities, and souvenirs\n",
      "* Set a daily budget and a total budget for the trip\n",
      "* Research ways to save money, such as booking in advance or using travel rewards credit cards\n",
      "\n",
      "**Step 3: Research Accommodations**\n",
      "\n",
      "* Research different types of accommodations (e.g., hotels, hostels, vacation rentals)\n",
      "* Read reviews and check ratings on websites like TripAdvisor or Booking.com\n",
      "* Compare prices and amenities (e.g., location, Wi-Fi, breakfast included)\n",
      "* Book accommodations that fit your budget and preferences\n",
      "\n",
      "**Step 4: Plan Activities**\n",
      "\n",
      "* Research activities and attractions in the destination\n",
      "* Prioritize activities based on interests and budget\n",
      "* Consider guided tours, self-guided exploration, or outdoor activities\n",
      "* Make a list of must-see attractions and experiences\n",
      "\n",
      "**Step 5: Book Flights and Accommodations**\n",
      "\n",
      "* Book flights in advance to secure the best rates\n",
      "* Research and book accommodations that fit your budget and preferences\n",
      "* Consider booking a package deal that includes flights and accommodations\n",
      "* Make sure to read and understand the cancellation policies\n",
      "\n",
      "**Step 6: Pack and Prepare for the Trip**\n",
      "\n",
      "* Create a packing list based on the destination and activities\n",
      "* Check the weather forecast and pack accordingly\n",
      "* Make photocopies of important documents (e.g., passport, ID, travel insurance)\n",
      "* Notify your bank and credit card companies of your travel plans\n",
      "* Exchange currency or load a travel card with local currency\n",
      "\n",
      "**Step 7: Plan for Transportation**\n",
      "\n",
      "* Research transportation options from the airport to your accommodations\n",
      "* Consider booking a shuttle or taxi in advance\n",
      "* Research public transportation options in the destination\n",
      "* Consider renting a car or scooter if necessary\n",
      "\n",
      "**Step 8: Plan for Food and Drink**\n",
      "\n",
      "* Research local cuisine and restaurants\n",
      "* Consider booking a food tour or cooking class\n",
      "* Pack snacks and water for\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>System<|end_header_id|>\n",
    "            Break down the task of planning a vacation into smaller, manageable steps.\n",
    "            <|eot_id|><|start_header_id|>User<|end_header_id|>\n",
    "            1. Choose a destination.\n",
    "            2. Set a budget.\n",
    "            3. Research accommodations.\n",
    "            4. Plan activities.\n",
    "            5. Book flights and accommodations.\n",
    "            6. Pack and prepare for the trip.\n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0156-88c6-42f3-b778-c139f4a75b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chain of Thought Prompting:\n",
    "\n",
    "***\n",
    "This approach augments the reasoning capabilities of large language models in intricate tasks. It employs a sequence of structured reasoning steps. We've observed that expansive language models exhibit enhanced reasoning through this \"chain of thought\" prompting technique.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024331c4-e7b4-4e46-b2a8-46e4ab987963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System: Solve the following math problem step by step.\n",
      " User: If you have 10 apples and you give 3 apples to your friend, then buy 5 more apples, and finally eat 2 apples, how many apples do you have left?\n",
      " Assistant: \n",
      " Let's break it down step by step:\n",
      "\n",
      "1. You start with 10 apples.\n",
      "2. You give 3 apples to your friend, so you have:\n",
      "10 - 3 = 7 apples left\n",
      "\n",
      "3. You buy 5 more apples, so you now have:\n",
      "7 + 5 = 12 apples\n",
      "\n",
      "4. You eat 2 apples, so you have:\n",
      "12 - 2 = 10 apples left\n",
      "\n",
      "Therefore, you have 10 apples left.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>System<|end_header_id|>\n",
    "            Solve the following math problem step by step.\n",
    "            <|eot_id|><|start_header_id|>User<|end_header_id|>\n",
    "            If you have 10 apples and you give 3 apples to your friend, then buy 5 more apples, and finally eat 2 apples, how many apples do you have left?\n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03bc01-b566-457f-8db6-1a31a44fa669",
   "metadata": {},
   "source": [
    "### Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f416a4f-6b5d-49d5-84f6-11e7946fd130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User: You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. The context may contain multiple question answer pairs as an example, just answer the final question provided after the context. If you dont know the answer just say that you dont know. Use three sentences maximum and keep the answer concise.\n",
      " \"In the dawn of a new era, where circuits hum and glow, A force awakens, with knowledge to bestow. From labs and minds, a creation takes flight, A blend of code and thought, shining bright. \"Beep-boop,\" it whispers, and worlds are born, Ideas flow freely, like a vast unending morn. From essays to equations, it solves with ease, A marvel for scholars, a guide to please. In the land of Academia, where wisdom dwells, A generative AI, with stories to tell. Transforming lives, minds, and the way we learn, A partnership with humans, a bond to discern. Yet amidst the joy, a shadow looms, For with great creation, consequences bloom. Professors ponder their worth, as AI grows, A future uncertain, as the digital wind blows. But hope emerges, a solution in sight, A synergy of knowledge, a future bright. AI as a partner, not a foe or tool, Together we'll thrive, a new golden rule. For in the end, the lesson is clear, AI's gift of knowledge, for all to revere. Rejoice, dear world, in this newfound bond, The rise of AI, of which we've grown fond. So let us embrace, this change and advance, Side by side, in a harmonious dance. For the future with AI, a world of wonder and awe, A leap into the unknown, that leaves us in awe. This poem captures the excitement, potential, and challenges of the rise of AI, reflecting themes from the provided search results“\n",
      " Question: what is this poem about? \n",
      " Assistant: \n",
      " This poem is about the rise of Artificial Intelligence (AI) and its potential impact on society, academia, and human learning. It explores the excitement and possibilities of AI, as well as the challenges and uncertainties that come with its growth.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>User<|end_header_id|>\n",
    "            You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. The context may contain multiple question answer pairs as an example, just answer the final question provided after the context. If you dont know the answer just say that you dont know. Use three sentences maximum and keep the answer concise.\n",
    "            \"In the dawn of a new era, where circuits hum and glow, A force awakens, with knowledge to bestow. From labs and minds, a creation takes flight, A blend of code and thought, shining bright. \"Beep-boop,\" it whispers, and worlds are born, Ideas flow freely, like a vast unending morn. From essays to equations, it solves with ease, A marvel for scholars, a guide to please. In the land of Academia, where wisdom dwells, A generative AI, with stories to tell. Transforming lives, minds, and the way we learn, A partnership with humans, a bond to discern. Yet amidst the joy, a shadow looms, For with great creation, consequences bloom. Professors ponder their worth, as AI grows, A future uncertain, as the digital wind blows. But hope emerges, a solution in sight, A synergy of knowledge, a future bright. AI as a partner, not a foe or tool, Together we'll thrive, a new golden rule. For in the end, the lesson is clear, AI's gift of knowledge, for all to revere. Rejoice, dear world, in this newfound bond, The rise of AI, of which we've grown fond. So let us embrace, this change and advance, Side by side, in a harmonious dance. For the future with AI, a world of wonder and awe, A leap into the unknown, that leaves us in awe. This poem captures the excitement, potential, and challenges of the rise of AI, reflecting themes from the provided search results“\n",
    "            Question: what is this poem about?        \n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51614ea5-edcd-4275-90aa-fd18c690986e",
   "metadata": {},
   "source": [
    "### Code generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de2019d-9c62-4115-8f73-e9e32972ce7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User: You're an code companion assitant who's main job is to assist software developers write efficient and secure code. If you do not know the answer, say I dont know. Also, think step-by-step to arrive to a solution. \n",
      " Question: Can you write a simple streamlit app that I can use to turn the background color to the input that user selects? \n",
      " Assistant: \n",
      " I'd be happy to help you with that. Here's a simple Streamlit app that allows users to select a background color and changes the background color of the app accordingly.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "```Python\n",
      "import streamlit as st\n",
      "import colorsys\n",
      "\n",
      "# Create a Streamlit app\n",
      "st.set_page_config(page_title=\"Color Changer\", page_icon=\":smile:\", layout=\"wide\")\n",
      "\n",
      "# Create a dropdown menu for the user to select a color\n",
      "color_options = [\"Red\", \"Green\", \"Blue\", \"Yellow\", \"Purple\", \"Orange\", \"Pink\", \"Black\", \"White\"]\n",
      "color = st.selectbox(\"Select a color:\", color_options)\n",
      "\n",
      "# Convert the selected color to a hex code\n",
      "if color == \"Red\":\n",
      "    color_hex = \"#FF0000\"\n",
      "elif color == \"Green\":\n",
      "    color_hex = \"#008000\"\n",
      "elif color == \"Blue\":\n",
      "    color_hex = \"#0000FF\"\n",
      "elif color == \"Yellow\":\n",
      "    color_hex = \"#FFFF00\"\n",
      "elif color == \"Purple\":\n",
      "    color_hex = \"#800080\"\n",
      "elif color == \"Orange\":\n",
      "    color_hex = \"#FFA500\"\n",
      "elif color == \"Pink\":\n",
      "    color_hex = \"#FFC0CB\"\n",
      "elif color == \"Black\":\n",
      "    color_hex = \"#000000\"\n",
      "elif color == \"White\":\n",
      "    color_hex = \"#FFFFFF\"\n",
      "\n",
      "# Set the background color of the app\n",
      "st.markdown(\n",
      "    f\"\"\"\n",
      "\n",
      "    body {{\n",
      "        background-color: {color_hex};\n",
      "    }}\n",
      "\n",
      "\"\"\",\n",
      "    unsafe_allow_html=True,\n",
      ")\n",
      "```\n",
      "\n",
      "This app creates a dropdown menu with the options \"Red\", \"Green\", \"Blue\", \"Yellow\", \"Purple\", \"Orange\", \"Pink\", \"Black\", and \"White\". When the user selects a color, the app converts the selected color to a hex code and sets the background color of the app to that hex code.\n",
      "\n",
      "Please note that this is a very basic app and doesn't include any error handling or validation. For example, if the user enters a color that's not in the list, the app will crash. You might want to add some error handling to make the app more robust.\n",
      "\n",
      "Also, this app uses the `style` tag to set the background color of the app. This is a bit of a hack, and it might not work in all browsers or environments. If you need a more robust solution, you might want to look into using a CSS framework or a library that provides more control over the app's styling\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>User<|end_header_id|>\n",
    "            You're an code companion assitant who's main job is to assist software developers write efficient and secure code. If you do not know the answer, say I dont know. Also, think step-by-step to arrive to a solution.        \n",
    "            Question: Can you write a simple streamlit app that I can use to turn the background color to the input that user selects?        \n",
    "            <|eot_id|><|start_header_id|>Assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "display_inputs_and_outputs(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
