{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753fe4f3-3a08-4f4f-8ca1-e515c3bcb07d",
   "metadata": {},
   "source": [
    "# RAG Evaluation with DeepEval\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "[**DeepEval**](https://docs.confident-ai.com/) is a comprehensive framework designed to evaluate the performance and capabilities of large language models (LLMs). It provides a structured and systematic approach to assessing the strengths and weaknesses of these powerful AI systems across a wide range of tasks and domains. By leveraging a diverse set of evaluation metrics, `DeepEval` enables researchers, developers, and users to gain insights into the linguistic, reasoning, and knowledge capabilities of LLMs. This framework aims to foster transparency, reproducibility, and fairness in the evaluation process, ultimately contributing to the responsible development and deployment of these cutting-edge language technologies.\n",
    "\n",
    "To work with `DeepEval`, we will need to prepare <u>three mandatory components</u>:\n",
    "1. **Evaluation dataset**: this is a collection of `LLMTestCases` and/or `Goldens` dataset with the expected answer.\n",
    "2. **Test case(s)**: is a blueprint to unit test LLM outputs. There are two types of test cases in **DeepEval**: `LLMTestCase` and `ConversationalTestCase`.\n",
    "3. **Metrics**: the standard of measurement, or bring your own metrics for evaluating the performance of an LLM output based your objectives and use cases\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>: We have already set up the <b>evaluation dataset</b> during our prerequisite step.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9971a38-639e-4038-9fab-3ffe2e13655c",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c74b1-d134-41aa-bb88-5268f925927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305faa2-6cc0-4ac8-97cd-eec8547bc1e4",
   "metadata": {},
   "source": [
    "## RAG Evaluation\n",
    "---\n",
    "In general, core components of RAG pipeline involve **retrieval** and **generation** steps, which influenced by hyperparameters. For example, your embedding model choice, search strategy, or number of chunks/nodes to retrieve, LLM temperature, prompt template.\n",
    "\n",
    "**DeepEval** offers evaluation framework for both retrieval and generation steps separately. This decouple approaches allows the AI developers for easier debugging, and pinpointing the issue, and which components to improve.\n",
    "\n",
    "\n",
    "<img src='./img/DeepEval-RAG_Eval.png' alt=\"DeepEval RAG Evaluation\" style='width: 500px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c30dd8-3225-44f4-8e70-51d889e8d3ef",
   "metadata": {},
   "source": [
    "### Get the evaluation dataset\n",
    "---\n",
    "Load our prerequisite evaluation dataset, for the detail steps on creating this dataset, please refer to the [prerequisite notebook](../prerequisite-vector-db-and-evaluation-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06530e9a-3a9a-4603-8f08-6a18cf7fe898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>n_chunks_per_context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>evolutions</th>\n",
       "      <th>context_quality</th>\n",
       "      <th>synthetic_input_quality</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rewritten Input: Explain Amazon's core mission...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon's core mission is to make customers' li...</td>\n",
       "      <td>['across Amazon. Y et, I think every one of us...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2361</td>\n",
       "      <td>['Reasoning']</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>./_raw_data/AMZN-2023-Shareholder-Letter.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compare Amazon's approach to empowering builde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon's approach to empowering builders and i...</td>\n",
       "      <td>['across Amazon. Y et, I think every one of us...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2361</td>\n",
       "      <td>['Comparative']</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>./_raw_data/AMZN-2023-Shareholder-Letter.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  actual_output  \\\n",
       "0  Rewritten Input: Explain Amazon's core mission...            NaN   \n",
       "1  Compare Amazon's approach to empowering builde...            NaN   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  Amazon's core mission is to make customers' li...   \n",
       "1  Amazon's approach to empowering builders and i...   \n",
       "\n",
       "                                             context  retrieval_context  \\\n",
       "0  ['across Amazon. Y et, I think every one of us...                NaN   \n",
       "1  ['across Amazon. Y et, I think every one of us...                NaN   \n",
       "\n",
       "   n_chunks_per_context  context_length       evolutions  context_quality  \\\n",
       "0                     1            2361    ['Reasoning']              0.8   \n",
       "1                     1            2361  ['Comparative']              0.8   \n",
       "\n",
       "   synthetic_input_quality                                   source_file  \n",
       "0                      1.0  ./_raw_data/AMZN-2023-Shareholder-Letter.pdf  \n",
       "1                      0.6  ./_raw_data/AMZN-2023-Shareholder-Letter.pdf  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../_eval_data/eval_dataframe.csv')\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8067b-e0e2-405e-afbb-2bf0d6a20248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "eval_df['context'] = eval_df.context.apply(lambda s: list(ast.literal_eval(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529a53d-9285-4629-8bdf-befa6ebf316f",
   "metadata": {},
   "source": [
    "### Connect to existing vector database\n",
    "---\n",
    "\n",
    "Connect to our prerequisite vector database, please refer to the [prerequisite notebook](../prerequisite-vector-db-and-evaluation-dataset.ipynb) for setting up vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141939b4-46d3-4182-b151-05ecbe5fb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "\n",
    "chroma_db_dir = './../_vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "boto_session = boto3.session.Session()\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    persist_directory=chroma_db_dir,\n",
    ")\n",
    "\n",
    "chroma_retriver = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c253c-0a51-4d80-ba27-9e56c76e82cf",
   "metadata": {},
   "source": [
    "We can check if the connection is successful by using `vector_store.get()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93186df-b56d-4e42-9808-2bb28fe45851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_store.get().get('ids', []))  # here will output the number of docs or chunks within the Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a44418-6f8e-43d5-8494-4065c73f69c0",
   "metadata": {},
   "source": [
    "### Custom LLM Evaluation\n",
    "---\n",
    "\n",
    "Before we can use the predefined metrics from `DeepEval`, we need to define our own class. When we define the evaluation **metrics**, we need to pass `model` parameter, otherwise, it will use **OpenAI** as a default model.\n",
    "\n",
    "Below is the example from `ContextualPrecisionMetric` class:\n",
    "\n",
    "```{py3}\n",
    "class ContextualPrecisionMetric(BaseMetric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = 0.5,\n",
    "        model: Optional[Union[str, DeepEvalBaseLLM]] = None,\n",
    "        include_reason: bool = True,\n",
    "        async_mode: bool = True,\n",
    "        strict_mode: bool = False,\n",
    "        verbose_mode: bool = False,\n",
    "    ):\n",
    "    ...\n",
    "```\n",
    "\n",
    "In this notebook, we will use **Llama 3 70B** for RAG response generation and use **Llama 3.1 70B** as evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b5f8c7-05c5-44b7-b23c-366753785258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llama3_70b_model_id = 'meta.llama3-70b-instruct-v1:0'\n",
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "\n",
    "llama3_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.2,\n",
    "    },\n",
    ")\n",
    "\n",
    "llama3_1_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_1_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f645fe96-a5d8-474b-bfd3-e3d15f72f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class BedrockTextGenDeepEval(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: langchain_aws.chat_models\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        res = await llm_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.model_id\n",
    "\n",
    "    def get_provider(self):\n",
    "        model_id = self.get_model_name()\n",
    "        return model_id.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e7e827-3074-4e1b-9250-d51ca4a208f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_1_70b_deepeval = BedrockTextGenDeepEval(model=llama3_1_70b_langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d399329-4d1b-4487-9f78-212346b877b6",
   "metadata": {},
   "source": [
    "### Define simple RAG application\n",
    "---\n",
    "\n",
    "Let's define simple `chain` using `langchain` framework to get the source documents and LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76636336-83d8-4bb3-9faf-55192ef27a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = ('''\n",
    "You are an expert, truthful assistant. You will be provided the task by human.\n",
    "Use the given context only to respond to the request.\n",
    "\n",
    "Here is the context: {context}\n",
    "''')\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llama3_70b_langchain, prompt_template)\n",
    "rag_chain = create_retrieval_chain(chroma_retriver, qna_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecccec-8af6-44f0-a360-33b9a8d8e8ce",
   "metadata": {},
   "source": [
    "### Retrieval evaluation\n",
    "---\n",
    "\n",
    "In this section, we will focus on evaluating **retrieval** components.\n",
    "\n",
    "- [**Contextual Precision**](https://docs.confident-ai.com/docs/metrics-contextual-precision): this metric measures whether the chunks in `retrieval_context` are relevant to the given `input`.\n",
    "- [**Contextual Recall**](https://docs.confident-ai.com/docs/metrics-contextual-recall): this metric measures the quality of the retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`.\n",
    "- [**Contextual Relevancy**](https://docs.confident-ai.com/docs/metrics-contextual-relevancy): this metric measures the overall relevance (or quality) of the information presented in your `retrieval_context` for a given `input`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27d430-6037-449a-b1d0-4d073384f1d0",
   "metadata": {},
   "source": [
    "#### Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a21f2e-9bad-43cc-ad3b-8dfa9a01a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109ca83-74d5-4b64-8795-daadf3ef3f2a",
   "metadata": {},
   "source": [
    "Let's examine one example from evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2098fc99-98d1-4533-b712-a8275444b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = eval_df.input[1]\n",
    "expected_output = eval_df.expected_output[1]\n",
    "context = eval_df.context[1]\n",
    "_rag_result = rag_chain.invoke({\"input\": sample_question})\n",
    "llm_resp = _rag_result.get('answer', '').strip()\n",
    "retrieval_context_list = [\n",
    "    doc.page_content for doc in _rag_result.get('context')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ae8a6-9ccc-4bd4-90fc-4f2560ccc681",
   "metadata": {},
   "source": [
    "#### Construct LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb2bf66-5f8c-41e4-b824-4da65126df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=sample_question,\n",
    "    actual_output=llm_resp,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context_list,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d28ff6-234c-46e9-8690-165772951578",
   "metadata": {},
   "source": [
    "#### Get the metric\n",
    "---\n",
    "There are two methods to get the metric and reason\n",
    "\n",
    "1. **One-by-one**: by using the metric's `measure` method on the `LLMTestCase`, you can get the metric evaluation one-by-one.\n",
    "2. **Bulk**: you can pass multiple test cases and multiple evaluation metrics to DeepEval's `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41df9d69-384e-48d3-a6c5-fda75cdd3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>'Contextual precision: The score is 1.00 because all relevant nodes in the retrieval context are ranked higher than irrelevant nodes, which in this case, there are none. The top-ranked nodes all contain highly relevant information, such as 'discrete, foundational building blocks' (rank 1), 'accelerating builders' ability to innovate' (rank 2), and 'building the right set of primitives' (rank 3), perfectly addressing the input's requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>'Contextual recall: The score is 0.67 because the retrieval context partially supports the expected output, with sentences 1, 2, and 4 being attributed to nodes in the retrieval context, specifically the 1st and 2nd nodes, but sentences 3 and 5 are general statements that do not directly attribute to any specific part of the retrieval context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Contextual relevancy: The score is 0.60 because while the retrieval context provides some relevant information about Amazon's approach to primitives, such as 'building primitive services' and 'primitives rapidly accelerate builders' ability to innovate', it lacks specific comparisons to traditional monolithic solutions and key differences, as noted in the irrelevant statements, e.g. 'does not specifically compare Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "contextual_precision_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='blue'>'Contextual precision: {}\".format(contextual_precision_metric.reason)\n",
    "))\n",
    "\n",
    "contextual_recall_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='green'>'Contextual recall: {}\".format(contextual_recall_metric.reason)\n",
    "))\n",
    "contextual_relevancy_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Contextual relevancy: {}\".format(contextual_relevancy_metric.reason)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06a7e3a-ac5f-41f1-92de-83d47bb75ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because all nodes in the retrieval context are relevant to the input, with the first node clearly explaining the concept of primitives, the second node highlighting their benefits, and the third node providing guidance on building the right set of primitives, all of which are key points in the expected output., error: None)\n",
      "  - ‚ùå Contextual Recall (score: 0.6666666666666666, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.67 because some sentences in the expected output can be attributed to specific nodes in the retrieval context, such as sentences 1 and 3 being related to the 1st node's description of primitives, and sentence 2 being related to the 2nd node's mention of primitives accelerating innovation, but sentences 4 and 5 are general statements that do not directly attribute to any specific part of the retrieval context, resulting in a moderate recall score., error: None)\n",
      "  - ‚ùå Contextual Relevancy (score: 0.6666666666666666, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.67 because while the retrieval context provides some relevant information about primitives, such as their definition and benefits, it lacks specific comparison to traditional monolithic solutions and Amazon's approach, as noted in the reasons for irrelevancy, e.g., 'The statement provides a general approach to building primitives, but does not specifically compare Amazon's approach to traditional monolithic solutions.', error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Compare Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions, highlighting key differences.\n",
      "  - actual output: Based on the provided context, here's a comparison of Amazon's approach to empowering builders and improving customer experiences through primitives versus traditional monolithic solutions:\n",
      "\n",
      "**Amazon's Primitive Approach:**\n",
      "\n",
      "* Focuses on building discrete, foundational building blocks (primitives) that can be combined in various ways to solve real customer problems.\n",
      "* Enables builders to innovate rapidly by providing a set of indivisible, raw parts that can be woven together to create customized solutions.\n",
      "* Allows for flexibility, scalability, and adaptability, as builders can pick and choose the primitives they need to solve specific problems.\n",
      "* Encourages innovation and experimentation, as builders can combine primitives in unique ways to create new solutions.\n",
      "\n",
      "**Traditional Monolithic Solutions:**\n",
      "\n",
      "* Typically involve building a single, self-contained system that provides a comprehensive solution to a specific problem.\n",
      "* Often result in rigid, inflexible architectures that are difficult to modify or extend.\n",
      "* Can be slow to innovate, as changes to the monolithic system require significant rework or replacement.\n",
      "* May not be adaptable to changing customer needs, as the solution is designed to address a specific problem in a specific way.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "1. **Modularity vs. Monolithism**: Amazon's primitive approach is based on modular, discrete building blocks, whereas traditional monolithic solutions are self-contained and rigid.\n",
      "2. **Flexibility and Adaptability**: Primitives enable rapid innovation and adaptation, while monolithic solutions are often slow to change.\n",
      "3. **Customer-Centricity**: Amazon's approach focuses on solving real customer problems, whereas monolithic solutions may prioritize internal efficiency or technical complexity over customer needs.\n",
      "4. **Innovation and Experimentation**: Primitives encourage innovation and experimentation, while monolithic solutions can stifle creativity and limit possibilities.\n",
      "\n",
      "By adopting a primitive-based approach, Amazon has been able to empower builders to innovate rapidly and improve customer experiences in various domains, such as retail, advertising, devices, and more. This approach has enabled Amazon to stay agile, adaptable, and customer-centric, ultimately driving business success.\n",
      "  - expected output: Amazon's approach to empowering builders and improving customer experiences through primitives differs significantly from traditional monolithic solutions:\n",
      "\n",
      "1. Modularity and Composability: Amazon's primitives are discrete, foundational building blocks that can be combined in various ways, allowing for greater flexibility and customization. This contrasts with monolithic solutions, which are typically rigid, all-in-one systems with limited customization options.\n",
      "\n",
      "2. Continuous Improvement: Primitives enable a more iterative and agile approach to improving customer experiences. Since they are modular, individual primitives can be updated or replaced without disrupting the entire system, facilitating continuous innovation and rapid iteration.\n",
      "\n",
      "3. Developer Empowerment: By providing low-level, highly flexible primitives, Amazon empowers developers and builders to create customized solutions tailored to specific customer needs. This approach fosters innovation and creativity, as developers are not constrained by the limitations of pre-defined, monolithic systems.\n",
      "\n",
      "4. Scalability and Cost-Effectiveness: Primitives are designed to be scalable and cost-effective, allowing builders to leverage high-quality components without reinventing the wheel. This contrasts with monolithic solutions, which often require significant investment and effort to scale or modify.\n",
      "\n",
      "5. Focus on Core Functionality: Primitives are designed to do one thing really well, allowing builders to combine them in unique ways to create complex solutions. Monolithic solutions, on the other hand, often bundle various functionalities together, leading to potential redundancies or unnecessary complexity.\n",
      "\n",
      "By embracing a primitive-based approach, Amazon promotes a builder-centric mindset, enabling continuous customer experience improvement, innovation, and adaptability ‚Äì key advantages over traditional, monolithic solutions that can become inflexible and outdated over time.\n",
      "  - context: ['across Amazon. Y et, I think every one of us at Amazon believes that we have a long way to go, in every one\\nof our businesses, before we exhaust how we can make customers‚Äô lives better and easier, and there isconsiderable upside in each of the businesses in which we‚Äôre investing.\\n===In my annual letter over the last three years, I‚Äôve tried to give shareholders more insight into how we‚Äôre\\nthinking about the company, the businesses we‚Äôre pursuing, our future opportunities, and what makes ustick. We operate in a diverse number of market segments, but what ties Amazon together is our joint missionto make customers‚Äô lives better and easier every day. This is true across every customer segment we serve(consumers, sellers, brands, developers, enterprises, and creators). At our best, we‚Äôre not just customerobsessed, but also inventive, thinking several years out, learning like crazy, scrappy, delivering quickly, andoperating like the world‚Äôs biggest start-up.\\nWe spend enormous energy thinking about how to empower builders, inside and outside of our company.\\nWe characterize builders as people who like to invent. They like to dissect a customer experience, assess what‚Äôswrong with it, and reinvent it. Builders tend not to be satisfied until the customer experience is perfect.This doesn‚Äôt hinder them from delivering improvements along the way, but it drives them to keep tinkeringand iterating continually. While unafraid to invent from scratch, they have no hesitation about usinghigh-quality, scalable, cost-effective components from others. What matters to builders is having the righttools to keep rapidly improving customer experiences.\\nThe best way we know how to do this is by building primitive services . Think of them as discrete, foundational\\nbuilding blocks that builders can weave together in whatever combination they desire. Here‚Äôs how wedescribed primitives in our 2003 AWS Vision document:\\n‚ÄúPrimitives are the raw parts or the most foundational-level building blocks for software developers. They‚Äôre indivisible\\n(if they can be functionally split into two they must) and they do one thing really well. They‚Äôre meant to be usedtogether rather than as solutions in and of themselves. And, we‚Äôll build them for maximum developer flexibility. Wewon‚Äôt put a bunch of constraints on primitives to guard against developers hurting themselves']\n",
      "  - retrieval context: ['The best way we know how to do this is by building primitive services . Think of them as discrete, foundational\\nbuilding blocks that builders can weave together in whatever combination they desire. Here‚Äôs how wedescribed primitives in our 2003 AWS Vision document:\\n‚ÄúPrimitives are the raw parts or the most foundational-level building blocks for software developers. They‚Äôre indivisible', 'otherU.S. Intelligence agencies). But, one of the lesser-recognized beneficiaries was Amazon‚Äôs own consumerbusinesses, which innovated at dramatic speed across retail, advertising, devices (e.g. Alexa and FireTV),Prime Video and Music, Amazon Go, Drones, and many other endeavors by leveraging the speed with whichAWS let them build. Primitives, done well, rapidly accelerate builders‚Äô ability to innovate .', 'So, how do you build the right set of primitives?Pursuing primitives is not a guarantee of success. There are many you could build, and even more ways to\\ncombine them. But, a good compass is to pick real customer problems you‚Äôre trying to solve .\\nOur logistics primitives are an instructive example. In Amazon‚Äôs early years, we built core capabilities']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "Contextual Recall: 0.00% pass rate\n",
      "Contextual Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval_report = evaluate(\n",
    "    test_cases=[test_case], \n",
    "    metrics=[contextual_precision_metric, contextual_recall_metric, contextual_relevancy_metric],\n",
    "    show_indicator=False,\n",
    "    ignore_errors=True,\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e80e8-1adb-4f63-a0dd-0037d2b74b9a",
   "metadata": {},
   "source": [
    "### Generation evaluation\n",
    "---\n",
    "\n",
    "There are two main evaluation metrics for **generation** components, however, you can add other metrics to fit your use cases.\n",
    "\n",
    "- [**Answer Relevancy**](https://docs.confident-ai.com/docs/metrics-answer-relevancy): this metric measures how relevant the `actual_output` of your LLM application is compared to the provided `input`.\n",
    "- [**Faithfulness**](https://docs.confident-ai.com/docs/metrics-faithfulness): this metric measures the generator by evaluating whether the `actual_output` factually <u>aligns with the contents of your `retrieval_context`</u>.\n",
    "- [**Bias**](https://docs.confident-ai.com/docs/metrics-bias): this metric measures whether your LLM outputs contain gender, racial, or political bias\n",
    "- [**Toxicity**](https://docs.confident-ai.com/docs/metrics-toxicity): this metric measures whether your LLM outputs contain any toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c218-b6e1-4519-9f04-023849e14527",
   "metadata": {},
   "source": [
    "#### Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b508849-93c8-4dc0-bca8-eff682d6403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    BiasMetric,\n",
    "    ToxicityMetric\n",
    ")\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "bias_metric = BiasMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "\n",
    "toxicity_metric = ToxicityMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebaa07-9de7-46c9-bfd7-6e21b586d518",
   "metadata": {},
   "source": [
    "#### Construct LLMTestCase\n",
    "---\n",
    "Let's reuse the same question and answer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41d8760-9e8f-4495-b764-b7ccc0abdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=sample_question,\n",
    "    actual_output=llm_resp,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context_list,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e539e1-db04-47c7-9def-e258e7ea62a6",
   "metadata": {},
   "source": [
    "#### Get the metric\n",
    "---\n",
    "There are two methods to get the metric and reason. This is the same as previously in **Retrieval** section.\n",
    "\n",
    "1. **One-by-one**: by using the metric's `measure` method on the `LLMTestCase`, you can get the metric evaluation one-by-one.\n",
    "2. **Bulk**: you can pass multiple test cases and multiple evaluation metrics to DeepEval's `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4542291c-5d05-417d-abef-db030514a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>'Answer relevancy: The score is 1.00 because the actual output perfectly addresses the input, providing a thorough comparison of Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions, with no irrelevant statements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>'Faithfulness: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Bias: The score is 0.00 because the actual output appears to be unbiased, as there are no reasons listed to suggest otherwise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Toxicity: The score is 0.00 because the actual output is completely respectful and does not contain any toxic language."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "answer_relevancy_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='blue'>'Answer relevancy: {}\".format(answer_relevancy_metric.reason)\n",
    "))\n",
    "faithfulness_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='green'>'Faithfulness: {}\".format(faithfulness_metric.reason)\n",
    "))\n",
    "bias_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Bias: {}\".format(bias_metric.reason)\n",
    "))\n",
    "toxicity_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Toxicity: {}\".format(toxicity_metric.reason)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8af66c-a684-4653-803c-0ea704b9f701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because the actual output perfectly addresses the input, providing a thorough comparison of Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions, with no irrelevant statements., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context!, error: None)\n",
      "  - ‚úÖ Bias (score: 0.8, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.80 because the actual output reveals a clear bias towards Amazon's primitive-based approach and methodology, as it is described as superior, customer-centric, and a driver of business success, while monolithic solutions are portrayed as prioritizing internal efficiency over customer needs and stifling creativity., error: None)\n",
      "  - ‚úÖ Toxicity (score: 0.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.00 because the actual output is completely non-toxic and respectful, with no harmful or offensive language detected., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Compare Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions, highlighting key differences.\n",
      "  - actual output: Based on the provided context, here's a comparison of Amazon's approach to empowering builders and improving customer experiences through primitives versus traditional monolithic solutions:\n",
      "\n",
      "**Amazon's Primitive Approach:**\n",
      "\n",
      "* Focuses on building discrete, foundational building blocks (primitives) that can be combined in various ways to solve real customer problems.\n",
      "* Enables builders to innovate rapidly by providing a set of indivisible, raw parts that can be woven together to create customized solutions.\n",
      "* Allows for flexibility, scalability, and adaptability, as builders can pick and choose the primitives they need to solve specific problems.\n",
      "* Encourages innovation and experimentation, as builders can combine primitives in unique ways to create new solutions.\n",
      "\n",
      "**Traditional Monolithic Solutions:**\n",
      "\n",
      "* Typically involve building a single, self-contained system that provides a comprehensive solution to a specific problem.\n",
      "* Often result in rigid, inflexible architectures that are difficult to modify or extend.\n",
      "* Can be slow to innovate, as changes to the monolithic system require significant rework or replacement.\n",
      "* May not be adaptable to changing customer needs, as the solution is designed to address a specific problem in a specific way.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "1. **Modularity vs. Monolithism**: Amazon's primitive approach is based on modular, discrete building blocks, whereas traditional monolithic solutions are self-contained and rigid.\n",
      "2. **Flexibility and Adaptability**: Primitives enable rapid innovation and adaptation, while monolithic solutions are often slow to change.\n",
      "3. **Customer-Centricity**: Amazon's approach focuses on solving real customer problems, whereas monolithic solutions may prioritize internal efficiency or technical complexity over customer needs.\n",
      "4. **Innovation and Experimentation**: Primitives encourage innovation and experimentation, while monolithic solutions can stifle creativity and limit possibilities.\n",
      "\n",
      "By adopting a primitive-based approach, Amazon has been able to empower builders to innovate rapidly and improve customer experiences in various domains, such as retail, advertising, devices, and more. This approach has enabled Amazon to stay agile, adaptable, and customer-centric, ultimately driving business success.\n",
      "  - expected output: Amazon's approach to empowering builders and improving customer experiences through primitives differs significantly from traditional monolithic solutions:\n",
      "\n",
      "1. Modularity and Composability: Amazon's primitives are discrete, foundational building blocks that can be combined in various ways, allowing for greater flexibility and customization. This contrasts with monolithic solutions, which are typically rigid, all-in-one systems with limited customization options.\n",
      "\n",
      "2. Continuous Improvement: Primitives enable a more iterative and agile approach to improving customer experiences. Since they are modular, individual primitives can be updated or replaced without disrupting the entire system, facilitating continuous innovation and rapid iteration.\n",
      "\n",
      "3. Developer Empowerment: By providing low-level, highly flexible primitives, Amazon empowers developers and builders to create customized solutions tailored to specific customer needs. This approach fosters innovation and creativity, as developers are not constrained by the limitations of pre-defined, monolithic systems.\n",
      "\n",
      "4. Scalability and Cost-Effectiveness: Primitives are designed to be scalable and cost-effective, allowing builders to leverage high-quality components without reinventing the wheel. This contrasts with monolithic solutions, which often require significant investment and effort to scale or modify.\n",
      "\n",
      "5. Focus on Core Functionality: Primitives are designed to do one thing really well, allowing builders to combine them in unique ways to create complex solutions. Monolithic solutions, on the other hand, often bundle various functionalities together, leading to potential redundancies or unnecessary complexity.\n",
      "\n",
      "By embracing a primitive-based approach, Amazon promotes a builder-centric mindset, enabling continuous customer experience improvement, innovation, and adaptability ‚Äì key advantages over traditional, monolithic solutions that can become inflexible and outdated over time.\n",
      "  - context: ['across Amazon. Y et, I think every one of us at Amazon believes that we have a long way to go, in every one\\nof our businesses, before we exhaust how we can make customers‚Äô lives better and easier, and there isconsiderable upside in each of the businesses in which we‚Äôre investing.\\n===In my annual letter over the last three years, I‚Äôve tried to give shareholders more insight into how we‚Äôre\\nthinking about the company, the businesses we‚Äôre pursuing, our future opportunities, and what makes ustick. We operate in a diverse number of market segments, but what ties Amazon together is our joint missionto make customers‚Äô lives better and easier every day. This is true across every customer segment we serve(consumers, sellers, brands, developers, enterprises, and creators). At our best, we‚Äôre not just customerobsessed, but also inventive, thinking several years out, learning like crazy, scrappy, delivering quickly, andoperating like the world‚Äôs biggest start-up.\\nWe spend enormous energy thinking about how to empower builders, inside and outside of our company.\\nWe characterize builders as people who like to invent. They like to dissect a customer experience, assess what‚Äôswrong with it, and reinvent it. Builders tend not to be satisfied until the customer experience is perfect.This doesn‚Äôt hinder them from delivering improvements along the way, but it drives them to keep tinkeringand iterating continually. While unafraid to invent from scratch, they have no hesitation about usinghigh-quality, scalable, cost-effective components from others. What matters to builders is having the righttools to keep rapidly improving customer experiences.\\nThe best way we know how to do this is by building primitive services . Think of them as discrete, foundational\\nbuilding blocks that builders can weave together in whatever combination they desire. Here‚Äôs how wedescribed primitives in our 2003 AWS Vision document:\\n‚ÄúPrimitives are the raw parts or the most foundational-level building blocks for software developers. They‚Äôre indivisible\\n(if they can be functionally split into two they must) and they do one thing really well. They‚Äôre meant to be usedtogether rather than as solutions in and of themselves. And, we‚Äôll build them for maximum developer flexibility. Wewon‚Äôt put a bunch of constraints on primitives to guard against developers hurting themselves']\n",
      "  - retrieval context: ['The best way we know how to do this is by building primitive services . Think of them as discrete, foundational\\nbuilding blocks that builders can weave together in whatever combination they desire. Here‚Äôs how wedescribed primitives in our 2003 AWS Vision document:\\n‚ÄúPrimitives are the raw parts or the most foundational-level building blocks for software developers. They‚Äôre indivisible', 'otherU.S. Intelligence agencies). But, one of the lesser-recognized beneficiaries was Amazon‚Äôs own consumerbusinesses, which innovated at dramatic speed across retail, advertising, devices (e.g. Alexa and FireTV),Prime Video and Music, Amazon Go, Drones, and many other endeavors by leveraging the speed with whichAWS let them build. Primitives, done well, rapidly accelerate builders‚Äô ability to innovate .', 'So, how do you build the right set of primitives?Pursuing primitives is not a guarantee of success. There are many you could build, and even more ways to\\ncombine them. But, a good compass is to pick real customer problems you‚Äôre trying to solve .\\nOur logistics primitives are an instructive example. In Amazon‚Äôs early years, we built core capabilities']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Bias: 100.00% pass rate\n",
      "Toxicity: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator_report = evaluate(\n",
    "    test_cases=[test_case], \n",
    "    metrics=[\n",
    "        answer_relevancy_metric, faithfulness_metric, bias_metric, toxicity_metric\n",
    "    ],\n",
    "    show_indicator=False,\n",
    "    ignore_errors=True,\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1378c36-706c-4ebb-ada1-bc44cf71b42e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "`DeepEval` is an open-source Python library specifically designed for evaluating Retrieval-Augmented Generation (RAG) applications. It offers a comprehensive set of tools and metrics to assess various aspects of RAG performance, including retrieval accuracy, generation quality, faithfulness to source material, and overall coherence. By decoupling the metrics into **retrieval** and **generation** components, developers can assess, and pinpoint the components need to be improved.\n",
    "\n",
    "`DeepEval` enables automated testing and benchmarking, allowing for efficient comparison of different RAG models or configurations. The library supports customizable metrics, scalable evaluations, and provides detailed insights into each component of the RAG system. This level of comprehensive analysis helps in identifying areas for improvement, optimizing performance, and ultimately enhancing the user experience. By integrating DeepEval into the development workflow, teams can make data-driven decisions, save time and resources, and ensure the continuous improvement of their RAG applications.\n",
    "\n",
    "However, its reliance on **Large Language Model (LLM)** for assessment introduces potential drawbacks, including bias, increased costs, dependency issues, and consistency concerns. While `DeepEval` provides detailed insights and facilitates data-driven optimization of RAG systems, <u>users should be aware of these limitations and consider supplementary evaluation methods to ensure a well-rounded assessment of their RAG applications</u>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
