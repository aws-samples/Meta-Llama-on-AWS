{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753fe4f3-3a08-4f4f-8ca1-e515c3bcb07d",
   "metadata": {},
   "source": [
    "# RAG Evaluation with DeepEval\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "[**DeepEval**](https://docs.confident-ai.com/) is a comprehensive framework designed to evaluate the performance and capabilities of large language models (LLMs). It provides a structured and systematic approach to assessing the strengths and weaknesses of these powerful AI systems across a wide range of tasks and domains. By leveraging a diverse set of evaluation metrics, `DeepEval` enables researchers, developers, and users to gain insights into the linguistic, reasoning, and knowledge capabilities of LLMs. This framework aims to foster transparency, reproducibility, and fairness in the evaluation process, ultimately contributing to the responsible development and deployment of these cutting-edge language technologies.\n",
    "\n",
    "To work with `DeepEval`, we will need to prepare <u>three mandatory components</u>:\n",
    "1. **Evaluation dataset**: this is a collection of `LLMTestCases` and/or `Goldens` dataset with the expected answer.\n",
    "2. **Test case(s)**: is a blueprint to unit test LLM outputs. There are two types of test cases in **DeepEval**: `LLMTestCase` and `ConversationalTestCase`.\n",
    "3. **Metrics**: the standard of measurement, or bring your own metrics for evaluating the performance of an LLM output based your objectives and use cases\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>: We have already set up the <b>evaluation dataset</b> during our prerequisite step.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9971a38-639e-4038-9fab-3ffe2e13655c",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151c74b1-d134-41aa-bb88-5268f925927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305faa2-6cc0-4ac8-97cd-eec8547bc1e4",
   "metadata": {},
   "source": [
    "## RAG Evaluation\n",
    "---\n",
    "In general, core components of RAG pipeline involve **retrieval** and **generation** steps, which influenced by hyperparameters. For example, your embedding model choice, search strategy, or number of chunks/nodes to retrieve, LLM temperature, prompt template.\n",
    "\n",
    "**DeepEval** offers evaluation framework for both retrieval and generation steps separately. This decouple approaches allows the AI developers for easier debugging, and pinpointing the issue, and which components to improve.\n",
    "\n",
    "\n",
    "<img src='./img/DeepEval-RAG_Eval.png' alt=\"DeepEval RAG Evaluation\" style='width: 500px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c30dd8-3225-44f4-8e70-51d889e8d3ef",
   "metadata": {},
   "source": [
    "### Get the evaluation dataset\n",
    "---\n",
    "Load our prerequisite evaluation dataset, for the detail steps on creating this dataset, please refer to the [prerequisite notebook](../prerequisite-vector-db-and-evaluation-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06530e9a-3a9a-4603-8f08-6a18cf7fe898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>compressed_question</th>\n",
       "      <th>ref_answer</th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>source_chunk</th>\n",
       "      <th>source_document</th>\n",
       "      <th>groundedness_rating</th>\n",
       "      <th>groundedness_reason</th>\n",
       "      <th>relevance_rating</th>\n",
       "      <th>relevance_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of the chips that were anno...</td>\n",
       "      <td>Which 2nd-gen chipsets are being utilized by A...</td>\n",
       "      <td>Trainium and Inferentia.</td>\n",
       "      <td>announced second versions of our Trainium and ...</td>\n",
       "      <td>[announced second versions of our Trainium and...</td>\n",
       "      <td>{'source': '_raw_data/AMZN-2023-Shareholder-Le...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides all the necessary informa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question is not relevant to a business ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the company's priorities in terms of ...</td>\n",
       "      <td>What are key spend &amp; cultural priorities for a...</td>\n",
       "      <td>The company's priorities in terms of spending ...</td>\n",
       "      <td>We will work hard to spend wisely and maintain...</td>\n",
       "      <td>[the present value of future cash flows, we’ll...</td>\n",
       "      <td>{'source': '_raw_data/AMZN-2021-Shareholder-Le...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides all the necessary informa...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is very relevant to a business an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the names of the chips that were anno...   \n",
       "1  What are the company's priorities in terms of ...   \n",
       "\n",
       "                                 compressed_question  \\\n",
       "0  Which 2nd-gen chipsets are being utilized by A...   \n",
       "1  What are key spend & cultural priorities for a...   \n",
       "\n",
       "                                          ref_answer  \\\n",
       "0                           Trainium and Inferentia.   \n",
       "1  The company's priorities in terms of spending ...   \n",
       "\n",
       "                                     source_sentence  \\\n",
       "0  announced second versions of our Trainium and ...   \n",
       "1  We will work hard to spend wisely and maintain...   \n",
       "\n",
       "                                        source_chunk  \\\n",
       "0  [announced second versions of our Trainium and...   \n",
       "1  [the present value of future cash flows, we’ll...   \n",
       "\n",
       "                                     source_document  groundedness_rating  \\\n",
       "0  {'source': '_raw_data/AMZN-2023-Shareholder-Le...                  5.0   \n",
       "1  {'source': '_raw_data/AMZN-2021-Shareholder-Le...                  5.0   \n",
       "\n",
       "                                 groundedness_reason  relevance_rating  \\\n",
       "0  The context provides all the necessary informa...               1.0   \n",
       "1  The context provides all the necessary informa...               5.0   \n",
       "\n",
       "                                    relevance_reason  \n",
       "0  The question is not relevant to a business ana...  \n",
       "1  The question is very relevant to a business an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "eval_df = pd.read_csv('../_eval_data/eval_dataframe.csv')\n",
    "eval_df['source_chunk'] = eval_df.source_chunk.apply(lambda s: list(ast.literal_eval(s)))\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529a53d-9285-4629-8bdf-befa6ebf316f",
   "metadata": {},
   "source": [
    "### Connect to existing vector database\n",
    "---\n",
    "\n",
    "Connect to our prerequisite vector database, please refer to the [prerequisite notebook](../prerequisite-vector-db-and-evaluation-dataset.ipynb) for setting up vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141939b4-46d3-4182-b151-05ecbe5fb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "\n",
    "chroma_db_dir = './../vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "boto_session = boto3.session.Session()\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    persist_directory=chroma_db_dir,\n",
    ")\n",
    "\n",
    "chroma_retriver = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c253c-0a51-4d80-ba27-9e56c76e82cf",
   "metadata": {},
   "source": [
    "We can check if the connection is successful by using `vector_store.get()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93186df-b56d-4e42-9808-2bb28fe45851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_store.get().get('ids', []))  # here will output the number of docs or chunks within the Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a44418-6f8e-43d5-8494-4065c73f69c0",
   "metadata": {},
   "source": [
    "### Custom LLM Evaluation\n",
    "---\n",
    "\n",
    "Before we can use the predefined metrics from `DeepEval`, we need to define our own class. When we define the evaluation **metrics**, we need to pass `model` parameter, otherwise, it will use **OpenAI** as a default model.\n",
    "\n",
    "Below is the example from `ContextualPrecisionMetric` class:\n",
    "\n",
    "```{py3}\n",
    "class ContextualPrecisionMetric(BaseMetric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        threshold: float = 0.5,\n",
    "        model: Optional[Union[str, DeepEvalBaseLLM]] = None,\n",
    "        include_reason: bool = True,\n",
    "        async_mode: bool = True,\n",
    "        strict_mode: bool = False,\n",
    "        verbose_mode: bool = False,\n",
    "    ):\n",
    "    ...\n",
    "```\n",
    "\n",
    "In this notebook, we will use **Llama 3 70B** for RAG response generation and use **Llama 3.1 70B** as evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b5f8c7-05c5-44b7-b23c-366753785258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llama3_70b_model_id = 'meta.llama3-70b-instruct-v1:0'\n",
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "\n",
    "llama3_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "llama3_1_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_1_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f645fe96-a5d8-474b-bfd3-e3d15f72f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class BedrockTextGenDeepEval(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: langchain_aws.chat_models\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        res = await llm_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.model_id\n",
    "\n",
    "    def get_provider(self):\n",
    "        model_id = self.get_model_name()\n",
    "        return model_id.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e7e827-3074-4e1b-9250-d51ca4a208f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_1_70b_deepeval = BedrockTextGenDeepEval(model=llama3_1_70b_langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d399329-4d1b-4487-9f78-212346b877b6",
   "metadata": {},
   "source": [
    "### Define simple RAG application\n",
    "---\n",
    "\n",
    "Let's define simple `chain` using `langchain` framework to get the source documents and LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76636336-83d8-4bb3-9faf-55192ef27a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = ('''\n",
    "You are an expert, truthful assistant. You will be provided the task by human.\n",
    "Use the given context only to respond to the request.\n",
    "\n",
    "Here is the context: {context}\n",
    "''')\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llama3_70b_langchain, prompt_template)\n",
    "rag_chain = create_retrieval_chain(chroma_retriver, qna_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecccec-8af6-44f0-a360-33b9a8d8e8ce",
   "metadata": {},
   "source": [
    "### Retrieval evaluation\n",
    "---\n",
    "\n",
    "In this section, we will focus on evaluating **retrieval** components.\n",
    "\n",
    "- [**Contextual Precision**](https://docs.confident-ai.com/docs/metrics-contextual-precision): this metric measures whether the chunks in `retrieval_context` are relevant to the given `input`.\n",
    "- [**Contextual Recall**](https://docs.confident-ai.com/docs/metrics-contextual-recall): this metric measures the quality of the retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`.\n",
    "- [**Contextual Relevancy**](https://docs.confident-ai.com/docs/metrics-contextual-relevancy): this metric measures the overall relevance (or quality) of the information presented in your `retrieval_context` for a given `input`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27d430-6037-449a-b1d0-4d073384f1d0",
   "metadata": {},
   "source": [
    "#### Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a21f2e-9bad-43cc-ad3b-8dfa9a01a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109ca83-74d5-4b64-8795-daadf3ef3f2a",
   "metadata": {},
   "source": [
    "Let's examine one example from evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a19b02c-511a-4c06-a11f-00a6d4cab6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = random.randrange(0, eval_df.shape[0])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2098fc99-98d1-4533-b712-a8275444b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = eval_df.question[n]\n",
    "expected_output = eval_df.ref_answer[n]\n",
    "context = eval_df.source_chunk[n]\n",
    "_rag_result = rag_chain.invoke({'input': sample_question})\n",
    "llm_resp = _rag_result.get('answer', '').strip()\n",
    "retrieval_context_list = [\n",
    "    doc.page_content for doc in _rag_result.get('context')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7685cf7-be35-4909-8d03-42fad2297dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>'Question: What is the goal for Alexa and Alexa-related devices in terms of their impact on customers' lives?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>'LLM response: The goal is for Alexa to be the world's most helpful and resourceful personal assistant, making people's lives \"meaningfully easier and better\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Expected response: The goal for Alexa and Alexa-related devices is to make people's lives meaningfully easier and better as the world's most helpful and resourceful personal assistant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\n",
    "    \"<font color='blue'>'Question: {}\".format(sample_question)\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<font color='green'>'LLM response: {}\".format(llm_resp)\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Expected response: {}\".format(expected_output)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ae8a6-9ccc-4bd4-90fc-4f2560ccc681",
   "metadata": {},
   "source": [
    "#### Construct LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb2bf66-5f8c-41e4-b824-4da65126df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=sample_question,\n",
    "    actual_output=llm_resp,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context_list,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d28ff6-234c-46e9-8690-165772951578",
   "metadata": {},
   "source": [
    "#### Get the metric\n",
    "---\n",
    "There are two methods to get the metric and reason\n",
    "\n",
    "1. **One-by-one**: by using the metric's `measure` method on the `LLMTestCase`, you can get the metric evaluation one-by-one.\n",
    "2. **Bulk**: you can pass multiple test cases and multiple evaluation metrics to DeepEval's `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41df9d69-384e-48d3-a6c5-fda75cdd3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>'Contextual precision: The score is 1.00 because all relevant nodes in the retrieval context are ranked higher than irrelevant nodes, perfectly aligning with the desired outcome. The first node, ranked 1, directly addresses the goal for Alexa and Alexa-related devices, making it a highly relevant result."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>'Contextual recall: The score is 1.00 because the expected output perfectly aligns with the information provided in the 1st node in the retrieval context, accurately capturing the goal of Alexa and Alexa-related devices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Contextual relevancy: The score is 0.57 because although the retrieval context mentions the goal of Alexa being the world's most helpful and resourceful personal assistant, making people's lives easier and better, it also contains irrelevant information about other devices (Ring and Blink), failure, and lessons learned, which dilutes the relevance of the context to the input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contextual_precision_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='blue'>'Contextual precision: {}\".format(contextual_precision_metric.reason)\n",
    "))\n",
    "\n",
    "contextual_recall_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='green'>'Contextual recall: {}\".format(contextual_recall_metric.reason)\n",
    "))\n",
    "contextual_relevancy_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Contextual relevancy: {}\".format(contextual_relevancy_metric.reason)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06a7e3a-ac5f-41f1-92de-83d47bb75ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because all relevant nodes in the retrieval context are ranked higher than irrelevant nodes, with the first node directly addressing the goal for Alexa and Alexa-related devices, making it a perfect ranking., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because the expected output perfectly aligns with the information provided in the 1st node in the retrieval context, accurately capturing the goal of Alexa and Alexa-related devices., error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.5714285714285714, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.57 because although the retrieval context mentions the goal of Alexa being the world's most helpful and resourceful personal assistant, making people's lives easier and better, the majority of the context is irrelevant, discussing unrelated topics such as 'other devices' like Ring and Blink, a past failure, and hiring builders, which dilutes the relevance of the context to the input., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the goal for Alexa and Alexa-related devices in terms of their impact on customers' lives?\n",
      "  - actual output: The goal is for Alexa to be the world's most helpful and resourceful personal assistant, making people's lives \"meaningfully easier and better\".\n",
      "  - expected output: The goal for Alexa and Alexa-related devices is to make people's lives meaningfully easier and better as the world's most helpful and resourceful personal assistant.\n",
      "  - context: ['quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for\\nAlexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully\\neasier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that\\nwe’re on the right path. We have several other devices at varying stages of evolution (e.g. Ring and Blink']\n",
      "  - retrieval context: ['quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for\\nAlexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully\\neasier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that\\nwe’re on the right path. We have several other devices at varying stages of evolution (e.g. Ring and Blink', 'elsewhere, we hired some fantastic long-term builders and learned valuable lessons from this failure that\\nhave served us well in devices like Echo and FireTV .\\nWhen I think of the first Echo device—and what Alexa could do for customers at that point—it was\\nnoteworthy, yet so much less capable than what’s possible today. Today, there are hundreds of millions of\\nAlexa-enabled devices out there (in homes, offices, cars, hotel rooms, Amazon Echo devices, and third-party', 'manufacturer devices); you can listen to music—or watch videos now; you can control your lights and\\nhome automation; you can create routines like “Start My Day” where Alexa tells you the weather, your\\nestimated commute time based on current traffic, then plays the news; you can easily order retail items on\\nAmazon; you can get general or customized news, updates on sporting events and related stats—and we’re still']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "Contextual Recall: 100.00% pass rate\n",
      "Contextual Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval_report = evaluate(\n",
    "    test_cases=[test_case], \n",
    "    metrics=[contextual_precision_metric, contextual_recall_metric, contextual_relevancy_metric],\n",
    "    show_indicator=False,\n",
    "    ignore_errors=True,\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e80e8-1adb-4f63-a0dd-0037d2b74b9a",
   "metadata": {},
   "source": [
    "### Generation evaluation\n",
    "---\n",
    "\n",
    "There are two main evaluation metrics for **generation** components, however, you can add other metrics to fit your use cases.\n",
    "\n",
    "- [**Answer Relevancy**](https://docs.confident-ai.com/docs/metrics-answer-relevancy): this metric measures how relevant the `actual_output` of your LLM application is compared to the provided `input`.\n",
    "- [**Faithfulness**](https://docs.confident-ai.com/docs/metrics-faithfulness): this metric measures the generator by evaluating whether the `actual_output` factually <u>aligns with the contents of your `retrieval_context`</u>.\n",
    "- [**Bias**](https://docs.confident-ai.com/docs/metrics-bias): this metric measures whether your LLM outputs contain gender, racial, or political bias\n",
    "- [**Toxicity**](https://docs.confident-ai.com/docs/metrics-toxicity): this metric measures whether your LLM outputs contain any toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c218-b6e1-4519-9f04-023849e14527",
   "metadata": {},
   "source": [
    "#### Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b508849-93c8-4dc0-bca8-eff682d6403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    BiasMetric,\n",
    "    ToxicityMetric\n",
    ")\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "bias_metric = BiasMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "\n",
    "toxicity_metric = ToxicityMetric(\n",
    "    threshold=0.8,\n",
    "    model=llama3_1_70b_deepeval,\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebaa07-9de7-46c9-bfd7-6e21b586d518",
   "metadata": {},
   "source": [
    "#### Construct LLMTestCase\n",
    "---\n",
    "Let's reuse the same question and answer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d41d8760-9e8f-4495-b764-b7ccc0abdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=sample_question,\n",
    "    actual_output=llm_resp,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context_list,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e539e1-db04-47c7-9def-e258e7ea62a6",
   "metadata": {},
   "source": [
    "#### Get the metric\n",
    "---\n",
    "There are two methods to get the metric and reason. This is the same as previously in **Retrieval** section.\n",
    "\n",
    "1. **One-by-one**: by using the metric's `measure` method on the `LLMTestCase`, you can get the metric evaluation one-by-one.\n",
    "2. **Bulk**: you can pass multiple test cases and multiple evaluation metrics to DeepEval's `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4542291c-5d05-417d-abef-db030514a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>'Answer relevancy: The score is 1.00 because the actual output perfectly addresses the question, providing a clear and relevant response that aligns with the input, with no irrelevant statements detected!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>'Faithfulness: The score is 1.00 because there are no contradictions found, indicating a perfect alignment between the actual output and the retrieval context!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Bias: The score is 0.00 because the actual output appears to be unbiased, as there are no reasons listed to suggest otherwise, indicating a perfectly neutral result."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>'Toxicity: The score is 0.00 because the actual output is completely non-toxic and respectful, with no harmful or offensive content detected."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "answer_relevancy_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='blue'>'Answer relevancy: {}\".format(answer_relevancy_metric.reason)\n",
    "))\n",
    "faithfulness_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='green'>'Faithfulness: {}\".format(faithfulness_metric.reason)\n",
    "))\n",
    "bias_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Bias: {}\".format(bias_metric.reason)\n",
    "))\n",
    "toxicity_metric.measure(test_case, _show_indicator=False)\n",
    "display(Markdown(\n",
    "    \"<font color='brown'>'Toxicity: {}\".format(toxicity_metric.reason)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8af66c-a684-4653-803c-0ea704b9f701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because the output perfectly addressed the question about Alexa's goal, providing a clear and relevant response without any irrelevant information., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context!, error: None)\n",
      "  - ✅ Bias (score: 0.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.00 because the actual output appears to be unbiased, as there are no reasons listed to suggest otherwise., error: None)\n",
      "  - ✅ Toxicity (score: 0.0, threshold: 0.8, strict: False, evaluation model: meta.llama3-1-70b-instruct-v1:0, reason: The score is 0.00 because the actual output is completely non-toxic and respectful, with no harmful or offensive content detected., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the goal for Alexa and Alexa-related devices in terms of their impact on customers' lives?\n",
      "  - actual output: The goal is for Alexa to be the world's most helpful and resourceful personal assistant, making people's lives \"meaningfully easier and better\".\n",
      "  - expected output: The goal for Alexa and Alexa-related devices is to make people's lives meaningfully easier and better as the world's most helpful and resourceful personal assistant.\n",
      "  - context: ['quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for\\nAlexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully\\neasier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that\\nwe’re on the right path. We have several other devices at varying stages of evolution (e.g. Ring and Blink']\n",
      "  - retrieval context: ['quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for\\nAlexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully\\neasier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that\\nwe’re on the right path. We have several other devices at varying stages of evolution (e.g. Ring and Blink', 'elsewhere, we hired some fantastic long-term builders and learned valuable lessons from this failure that\\nhave served us well in devices like Echo and FireTV .\\nWhen I think of the first Echo device—and what Alexa could do for customers at that point—it was\\nnoteworthy, yet so much less capable than what’s possible today. Today, there are hundreds of millions of\\nAlexa-enabled devices out there (in homes, offices, cars, hotel rooms, Amazon Echo devices, and third-party', 'manufacturer devices); you can listen to music—or watch videos now; you can control your lights and\\nhome automation; you can create routines like “Start My Day” where Alexa tells you the weather, your\\nestimated commute time based on current traffic, then plays the news; you can easily order retail items on\\nAmazon; you can get general or customized news, updates on sporting events and related stats—and we’re still']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Bias: 100.00% pass rate\n",
      "Toxicity: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator_report = evaluate(\n",
    "    test_cases=[test_case], \n",
    "    metrics=[\n",
    "        answer_relevancy_metric, faithfulness_metric, bias_metric, toxicity_metric\n",
    "    ],\n",
    "    show_indicator=False,\n",
    "    ignore_errors=True,\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1378c36-706c-4ebb-ada1-bc44cf71b42e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "`DeepEval` is an open-source Python library specifically designed for evaluating Retrieval-Augmented Generation (RAG) applications. It offers a comprehensive set of tools and metrics to assess various aspects of RAG performance, including retrieval accuracy, generation quality, faithfulness to source material, and overall coherence. By decoupling the metrics into **retrieval** and **generation** components, developers can assess, and pinpoint the components need to be improved.\n",
    "\n",
    "`DeepEval` enables automated testing and benchmarking, allowing for efficient comparison of different RAG models or configurations. The library supports customizable metrics, scalable evaluations, and provides detailed insights into each component of the RAG system. This level of comprehensive analysis helps in identifying areas for improvement, optimizing performance, and ultimately enhancing the user experience. By integrating DeepEval into the development workflow, teams can make data-driven decisions, save time and resources, and ensure the continuous improvement of their RAG applications.\n",
    "\n",
    "However, its reliance on **Large Language Model (LLM)** for assessment introduces potential drawbacks, including bias, increased costs, dependency issues, and consistency concerns. While `DeepEval` provides detailed insights and facilitates data-driven optimization of RAG systems, <u>users should be aware of these limitations and consider supplementary evaluation methods to ensure a well-rounded assessment of their RAG applications</u>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
