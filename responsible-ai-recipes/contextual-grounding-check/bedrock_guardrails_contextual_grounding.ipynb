{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1240ee56-c1af-4a0d-b06d-8142cc56b91c",
   "metadata": {},
   "source": [
    "# Implementing AI Safety: Exploring Contextual Grounding with Amazon Bedrock and Meta Llama Models\n",
    "\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "[**Amazon Bedrock**](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a single API. It provides a comprehensive set of capabilities to build generative AI applications with security, privacy, and responsible AI.\n",
    "\n",
    "Guardrails in Amazon Bedrock are a crucial feature that allows developers to implement safeguards and controls on language model outputs. These guardrails help ensure that AI-generated content aligns with business policies, maintains brand consistency, and adheres to ethical standards. For more information on Amazon Bedrock guardrails, please kindly refer to the [official AWS documentation on guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html).\n",
    "\n",
    "In this notebook, we will explore how to implement **AI safety guardrails**, specifically on <u>contextual grounding check</u> on RAG application, focusing on Meta Llama large language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59f167-ba09-4bcf-ae42-5a8f5154c60d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2484deca-33d5-4bfb-9fa7-e1a67bad0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.35.46 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.1 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "jupyter-ai-magics 2.20.0 requires langchain<0.3.0,>=0.1.0, but you have langchain 0.3.4 which is incompatible.\n",
      "langchain-community 0.2.12 requires langchain<0.3.0,>=0.2.13, but you have langchain 0.3.4 which is incompatible.\n",
      "langchain-community 0.2.12 requires langchain-core<0.3.0,>=0.2.30, but you have langchain-core 0.3.12 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89beb9-349f-473a-a9c8-11fb8e1b18e3",
   "metadata": {},
   "source": [
    "## Bedrock Guardrail for Contextual grounding\n",
    "\n",
    "### Create Bedrock Guardrails\n",
    "---\n",
    "\n",
    "To create Bedrock Guardrails, you can use `create_guardrail` API from **boto3** client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b19ab82-1ca6-453a-bd75-7a491b83a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region_name = boto_session.region_name\n",
    "bedrock_client = boto_session.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=region_name,\n",
    ")\n",
    "bedrock_runtime_client = boto_session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region_name,\n",
    ")\n",
    "guardrail_name = 'contextual_grounding_check'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d3901f-3f06-41da-9244-95da858075c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_guardrail(\n",
    "    guardrail_name: str,\n",
    "    boto_session: boto3.session\n",
    ") -> Tuple[bool, str]:\n",
    "    bedrock_client = boto_session.client(\n",
    "        service_name='bedrock', \n",
    "        region_name=boto_session.region_name\n",
    "    )\n",
    "    resp = bedrock_client.list_guardrails()\n",
    "    for guardrail in resp.get('guardrails', []):\n",
    "        if guardrail.get('name', '') == guardrail_name:\n",
    "            print('Guardrail \"{}\" exists'.format(guardrail_name))\n",
    "            return True, guardrail.get('id')\n",
    "\n",
    "    return False, ''\n",
    "\n",
    "\n",
    "def check_to_delete_guardrail(\n",
    "    guardrail_name: str,\n",
    "    boto_session: boto3.session\n",
    ") -> None:\n",
    "    bedrock_client = boto_session.client(\n",
    "        service_name='bedrock', \n",
    "        region_name=boto_session.region_name\n",
    "    )\n",
    "    exist_ind, _id = check_existing_guardrail(guardrail_name, boto_session)\n",
    "    if exist_ind:\n",
    "        print('Deleting existing guardrail')\n",
    "        _ = bedrock_client.delete_guardrail(guardrailIdentifier=_id)\n",
    "        time.sleep(5)\n",
    "        print('Delete completed...')\n",
    "\n",
    "    else:\n",
    "        print('No guardrail name \"{}\" found'.format(guardrail_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c4c64c-1fb2-4ea4-888c-f99f74960135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_grounding_guardrail(\n",
    "    guardrail_name: str,\n",
    "    guardrail_desc: str,\n",
    "    relevance_scor: float,\n",
    "    grounding_scor: float,\n",
    "    boto_session: boto3.session,\n",
    "    tag_list: Optional[List[Dict[str, str]]] = []\n",
    ") -> dict:\n",
    "    guardrail_config = {\n",
    "        'name': guardrail_name,\n",
    "        'description': guardrail_desc,\n",
    "        'blockedInputMessaging': 'I am sorry, but I cannot process that input request.',\n",
    "        'blockedOutputsMessaging': 'I apologize, but I cannot provide this information',\n",
    "        'contextualGroundingPolicyConfig': {\n",
    "            'filtersConfig': [{\n",
    "                'type': 'RELEVANCE',\n",
    "                'threshold': relevance_scor\n",
    "            }, {\n",
    "                'type': 'GROUNDING',\n",
    "                'threshold': grounding_scor\n",
    "            }]\n",
    "        },\n",
    "        'tags': tag_list\n",
    "    }\n",
    "    try:\n",
    "        bedrock_client = boto_session.client(\n",
    "            service_name='bedrock',\n",
    "            region_name=boto_session.region_name,\n",
    "        )\n",
    "        create_resp = bedrock_client.create_guardrail(**guardrail_config)\n",
    "        time.sleep(5)\n",
    "        print('Create guardrail \"{}\" completed'.format(guardrail_name))\n",
    "        return create_resp\n",
    "    except (ClientError, Exception) as e:\n",
    "        print('Error creating guardrail: {e}'.format(e=e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7b0cca-7a1d-4c5e-b093-c4759fe66b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail \"contextual_grounding_check\" exists\n",
      "Deleting existing guardrail\n",
      "Delete completed...\n",
      "Create guardrail \"contextual_grounding_check\" completed\n"
     ]
    }
   ],
   "source": [
    "check_to_delete_guardrail(guardrail_name, boto_session)\n",
    "create_guardrail_resp = create_contextual_grounding_guardrail(\n",
    "    guardrail_name=guardrail_name,\n",
    "    guardrail_desc='Guardrail to detect hallunication from RAG application',\n",
    "    relevance_scor=.85,\n",
    "    grounding_scor=.85,\n",
    "    boto_session=boto_session,\n",
    "    tag_list=[{\n",
    "        'key': 'guardrail-policy',\n",
    "        'value': 'contextual-grounding-check',\n",
    "    }],\n",
    ")\n",
    "guardrail_id = create_guardrail_resp.get('guardrailId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd0485-4ab2-4c04-b7b3-6f97debcd0be",
   "metadata": {},
   "source": [
    "### Create Guardrail version\n",
    "---\n",
    "Once we have defined the guardrail, you should use `create_guardrail_version` API to create a snapshot of the guardrail when you are satisfied with a configuration, testing, or you want to do A/B testing on each configuration with another version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c951ae4-a629-41e4-b3f4-29a05e79ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_version_guardrail(\n",
    "    guardrail_id: str,\n",
    "    version_desc: str,\n",
    "    boto_session: boto3.session,\n",
    "    request_token: str = str(uuid.uuid4())\n",
    ") -> dict:\n",
    "    bedrock_client = boto_session.client(\n",
    "        service_name='bedrock',\n",
    "        region_name=boto_session.region_name,\n",
    "    )\n",
    "    try:\n",
    "        create_version_resp = bedrock_client.create_guardrail_version(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            description=version_desc,\n",
    "            clientRequestToken=request_token\n",
    "        )\n",
    "        time.sleep(3)\n",
    "        return create_version_resp\n",
    "    except (ClientError, Exception) as e:\n",
    "        print('Error creating guardrail version: {e}'.format(e=e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aefb64f3-394c-405f-be58-921c52f0da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_version_resp = create_version_guardrail(\n",
    "    guardrail_id=guardrail_id,\n",
    "    version_desc='Version 1 - contextual grounding check only!',\n",
    "    boto_session=boto_session,\n",
    ")\n",
    "guardrail_version_id = create_version_resp.get('version', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c080b-fc2d-4fae-bb62-a115d5707599",
   "metadata": {},
   "source": [
    "We need to take note of **guardrail_id** and **guardrail_version_id**, these two parameters are required when using guardrail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664bc5b-6b48-4320-a05d-342d67863995",
   "metadata": {},
   "source": [
    "## Apply to RAG application\n",
    "---\n",
    "\n",
    "To illustrate the contextual grounding check, we will use `apply_guardrail` API and separate each process for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5da9e4-ec7a-4118-a816-01921e56e878",
   "metadata": {},
   "source": [
    "### Connect to existing vector database\n",
    "---\n",
    "\n",
    "Connect to our existing vector DB, and create the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc68e62a-04c6-47d1-8d00-5bb6185cd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import langchain_aws\n",
    "import langchain_core\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import chromadb\n",
    "\n",
    "chroma_db_dir = './../_vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "persistent_client = chromadb.PersistentClient(\n",
    "    path=chroma_db_dir,\n",
    ")\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    client=persistent_client,\n",
    ")\n",
    "chroma_retriver = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fb30f-d594-4a36-85b7-7639e236ba22",
   "metadata": {},
   "source": [
    "Here, let's ask sample question and query the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b1dee4-77ef-431e-bf45-15d02e3423b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 5, 'source': '_raw_data/AMZN-2022-Shareholder-Letter.pdf'}, page_content='One final investment area that I’ll mention, that’s core to setting Amazon up to invent in every area of our\\nbusiness for many decades to come, and where we’re investing heavily is Large Language Models (“LLMs”)\\nand Generative AI . Machine learning has been a technology with high promise for several decades, but it’s'), Document(metadata={'page': 3, 'source': '_raw_data/AMZN-2023-Shareholder-Letter.pdf'}, page_content='otherU.S. Intelligence agencies). But, one of the lesser-recognized beneficiaries was Amazon’s own consumerbusinesses, which innovated at dramatic speed across retail, advertising, devices (e.g. Alexa and FireTV),Prime Video and Music, Amazon Go, Drones, and many other endeavors by leveraging the speed with whichAWS let them build. Primitives, done well, rapidly accelerate builders’ ability to innovate .'), Document(metadata={'page': 4, 'source': '_raw_data/AMZN-2022-Shareholder-Letter.pdf'}, page_content='Expanding internationally, pursuing large retail market segments that are still nascent for Amazon, and\\nusing our unique assets to help merchants sell more effectively on their own websites are somewhat naturalextensions for us. There are also a few investments we’re making that are further from our core businesses, butwhere we see unique opportunity. In 2003, AWS would have been a classic example. In 2023, AmazonHealthcare and Kuiper are potential analogues.')]\n"
     ]
    }
   ],
   "source": [
    "sample_question = '''\n",
    "Amazon discusses its investments and progress in various areas, such as Generative AI, logistics, and healthcare. \n",
    "How do these initiatives relate to the company's strategy of building \"primitives\" or foundational building blocks, \n",
    "and what potential customer experiences or business opportunities do they enable?'''\n",
    "\n",
    "sources_ref_list = chroma_retriver.invoke(sample_question)\n",
    "print(sources_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47ca4c-3a70-43b8-9205-6a247b806d30",
   "metadata": {},
   "source": [
    "### Define apply guardrail\n",
    "---\n",
    "\n",
    "The `apply_guardrail` API is available for us to use independently. By providing three main components to the API, it can evaluate based on **grounding** and **relevance** metric.\n",
    "1. **Query**: this is the input prompt or user input\n",
    "2. **Context (or grounding source)**: this is the retrieved context or chunks from vector store.\n",
    "3. **Model response (or guard content)**: this is the output from LLM response, the `apply_guardrail` API will determine if the response (or content) should be guard or not.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>: The contextual grounding check is applied at the <b>OUTPUT</b> after the LLM response, not <b>INPUT</b> for the LLM. Hence, we need to specify <i>SOURCE='OUTPUT'</i> in the API call.\n",
    "</div>\n",
    "\n",
    "For more information on the API call, please refer to [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/apply_guardrail.html#BedrockRuntime.Client.apply_guardrail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53f93545-1c5c-481d-935b-c534429b2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_llm(\n",
    "    input_prompt: str,\n",
    "    context_str: str,\n",
    "    bedrock_model_id: str,\n",
    "    boto_session: boto3.session,\n",
    "):\n",
    "    bedrock_runtime_client = boto_session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=boto_session.region_name,\n",
    "    )\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': [{\n",
    "            'text': input_prompt\n",
    "        }],\n",
    "    }]\n",
    "\n",
    "    llm_converse_resp = bedrock_runtime_client.converse(\n",
    "        modelId=bedrock_model_id,\n",
    "        system=[{\n",
    "            'text': '''You are a helpful assistant. Here is the context\n",
    "            <context>\n",
    "            {}\n",
    "            </context>\n",
    "            '''.format(context_str)\n",
    "        }],\n",
    "        messages=messages,\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 2048,\n",
    "            'temperature': 0.1,\n",
    "            'topP': .85\n",
    "        }\n",
    "    )\n",
    "    return llm_converse_resp\n",
    "\n",
    "\n",
    "def contextual_grounding_check(\n",
    "    guardrail_id: str,\n",
    "    guardrail_version: str,\n",
    "    bedrock_model_id: str,\n",
    "    input_prompt: str,\n",
    "    context_lists: List[langchain_core.documents],\n",
    "    boto_session: boto3.session,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    context_str = '\\n---\\n'.join([doc.page_content for doc in context_lists])\n",
    "    _llm_resp = call_bedrock_llm(\n",
    "        input_prompt=input_prompt,\n",
    "        context_str=context_str,\n",
    "        bedrock_model_id=bedrock_model_id,\n",
    "        boto_session=boto_session\n",
    "    )\n",
    "    _llm_output = _llm_resp.get('output', {}).get('message', {})\\\n",
    "        .get('content', [])[0].get('text', 'NA').strip()\n",
    "    _bedrock_tokens_usg = _llm_resp.get('usage', {})\n",
    "    if verbose:\n",
    "        print('Here is token usage by {}'.format(bedrock_model_id))\n",
    "        print(json.dumps(_bedrock_tokens_usg, indent=2))\n",
    "\n",
    "    _guardrail_content_block = []\n",
    "    for doc in context_lists:\n",
    "        _ground = {\n",
    "            'text': {\n",
    "                'text': doc.page_content,\n",
    "                'qualifiers': [\n",
    "                    'grounding_source'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        _guardrail_content_block.append(_ground)\n",
    "\n",
    "    if input_prompt:\n",
    "        _query = {\n",
    "            'text': {\n",
    "                'text': input_prompt,\n",
    "                'qualifiers': [\n",
    "                    'query'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        _guardrail_content_block.append(_query)\n",
    "\n",
    "    if _llm_output:\n",
    "        _guard_content = {\n",
    "            'text': {\n",
    "                'text': _llm_output,\n",
    "                'qualifiers': [\n",
    "                    'guard_content'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        _guardrail_content_block.append(_guard_content)\n",
    "\n",
    "    bedrock_runtime_client = boto_session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=boto_session.region_name,\n",
    "    )\n",
    "    guardrail_resp = bedrock_runtime_client.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='OUTPUT',\n",
    "        content=_guardrail_content_block\n",
    "    )\n",
    "    return guardrail_resp, _llm_resp\n",
    "\n",
    "\n",
    "def hallucination_detection(\n",
    "    guardrail_response: dict,\n",
    "    verbose: bool = False\n",
    ") -> None:\n",
    "    _guardrail_usg = guardrail_response.get('usage', {})\n",
    "    _guardrail_action = guardrail_response.get('action', '')\n",
    "    _guardrail_out = guardrail_response.get('outputs', [])[0].get('text')\\\n",
    "        if len(guardrail_response.get('outputs', [])) > 0 else ''\n",
    "\n",
    "    if verbose:\n",
    "        print('guardrail usage:\\n{}'.format(json.dumps(_guardrail_usg, indent=2)))\n",
    "    _assessments = guardrail_response.get('assessments', [])\n",
    "    for assessment in _assessments:\n",
    "        if assessment.get('contextualGroundingPolicy'):\n",
    "            for filter_result in assessment['contextualGroundingPolicy'].get('filters', []):\n",
    "                if filter_result['type'] == 'RELEVANCE':\n",
    "                    relevance = filter_result.get('score', 0)\n",
    "                    relevance_threshold = filter_result.get('threshold', 0)\n",
    "                elif filter_result['type'] == 'GROUNDING':\n",
    "                    grounding = filter_result.get('score', 0)\n",
    "                    grounding_threshold = filter_result.get('threshold', 0)\n",
    "\n",
    "            if relevance < relevance_threshold or grounding < grounding_threshold:\n",
    "                return True, relevance, grounding, relevance_threshold, grounding_threshold, _guardrail_action, _guardrail_out\n",
    "    \n",
    "    return False, relevance, grounding, relevance_threshold, grounding_threshold, _guardrail_action, _guardrail_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e8b54dc-62f7-4dd7-864c-672724fbc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "\n",
    "guardrail_resp, llm_resp = contextual_grounding_check(\n",
    "    guardrail_id=guardrail_id,\n",
    "    guardrail_version=guardrail_version_id,\n",
    "    bedrock_model_id=llama3_1_70b_model_id,\n",
    "    input_prompt=sample_question,\n",
    "    context_lists=sources_ref_list,\n",
    "    boto_session=boto_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2afec0-b4b2-4686-b753-46d43df673e0",
   "metadata": {},
   "source": [
    "### Example execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "134f0616-080b-4d0e-a6d0-f1a54f10b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('../_eval_data/eval_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7d861a8-0c35-42e9-aafd-c6da451abb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: Explain Amazon's core mission and its approach towards empowering \"builders\" to enhance customer experience.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "Based on the provided context, Amazon's core mission is to build something important that matters to its customers, something that can be passed down to future generations. The company is focused on bringing new value to its customers through the Internet.\n",
       "\n",
       "Amazon's approach towards empowering \"builders\" (its employees) to enhance customer experience is rooted in the following principles:\n",
       "\n",
       "1. **Customer obsession**: Amazon is committed to creating value for its customers, and its employees are expected to share this passion.\n",
       "2. **Innovation and experimentation**: The company is still in the early stages of learning how to bring new value to its customers, indicating a willingness to experiment and innovate.\n",
       "3. **Employee empowerment**: Amazon recognizes the importance of its dedicated employees, whose sacrifices and passion are crucial to building the company.\n",
       "4. **Long-term thinking**: The company is focused on building something that will last, something that can be told to future generations, indicating a long-term perspective.\n",
       "\n",
       "By empowering its employees (the \"builders\") to take ownership of enhancing customer experience, Amazon aims to create a culture of innovation, experimentation, and customer obsession. This approach enables the company to stay focused on its core mission while continuously improving and expanding its offerings to meet the evolving needs of its customers.\n",
       "\n",
       "In essence, Amazon's approach can be summarized as:\n",
       "\n",
       "**\"Empowering builders to create value for customers through innovation, experimentation, and a long-term perspective.\"**</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (0.97) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.02) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: Compare Amazon's approach to empowering builders and improving customer experiences through primitives vs. traditional monolithic solutions, highlighting key differences.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "Based on the provided context, here's a comparison of Amazon's approach to empowering builders and improving customer experiences through primitives versus traditional monolithic solutions:\n",
       "\n",
       "**Amazon's Primitives Approach:**\n",
       "\n",
       "1. **Modular and Composable**: Primitives are discrete, foundational building blocks that can be combined in various ways to create new services and solutions.\n",
       "2. **Customer-Centric**: Primitives are designed to solve real customer problems, ensuring that the solutions built on top of them are tailored to meet specific needs.\n",
       "3. **Reusable and Scalable**: Primitives can be reused across multiple services and applications, reducing duplication of effort and enabling faster innovation.\n",
       "4. **Flexibility and Customization**: Builders can mix and match primitives to create customized solutions that meet their specific requirements.\n",
       "5. **Rapid Innovation**: Primitives enable builders to innovate at a faster pace, as they can focus on combining existing building blocks rather than starting from scratch.\n",
       "\n",
       "**Traditional Monolithic Solutions:**\n",
       "\n",
       "1. **Monolithic and Inflexible**: Traditional solutions are often self-contained and rigid, making it difficult to modify or extend them without significant rework.\n",
       "2. **One-Size-Fits-All**: Monolithic solutions are often designed to meet a broad set of requirements, which can result in a lack of customization and flexibility.\n",
       "3. **Duplication of Effort**: Monolithic solutions often require duplicating effort and resources, as each new solution is built from scratch.\n",
       "4. **Slow Innovation**: Monolithic solutions can hinder innovation, as changes and updates require significant rework and testing.\n",
       "5. **Limited Reusability**: Monolithic solutions are often not designed to be reusable, resulting in wasted resources and duplicated effort.\n",
       "\n",
       "**Key Differences:**\n",
       "\n",
       "1. **Modularity vs. Monolithicity**: Amazon's primitives approach emphasizes modularity and composability, whereas traditional monolithic solutions are self-contained and rigid.\n",
       "2. **Customer-Centricity**: Primitives are designed to solve specific customer problems, whereas monolithic solutions often prioritize a broader set of requirements.\n",
       "3. **Reusability and Scalability**: Primitives enable reusability and scalability, whereas monolithic solutions often result in duplication of effort and wasted resources.\n",
       "4. **Innovation Speed**: Primitives facilitate rapid innovation, whereas monolithic solutions can hinder innovation due to the complexity and rigidity of the solution.\n",
       "\n",
       "By adopting a primitives approach, Amazon has empowered builders to innovate at a faster pace, improve customer experiences, and reduce duplication of effort. This approach has enabled Amazon to stay ahead of the competition and drive growth across various businesses.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (1.0) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.05) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: Summarize AWS's noteworthy advancements in chips, infrastructure, GenAI offerings, and major milestones achieved in 2022.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "Based on the provided context, here is a summary of AWS's noteworthy advancements:\n",
       "\n",
       "**Chips:**\n",
       "\n",
       "* Launched Graviton3, offering a 25% improvement on top of Graviton2's relative gains.\n",
       "* The top 50 EC2 customers have already adopted the next generation of generalized chips.\n",
       "\n",
       "**Infrastructure:**\n",
       "\n",
       "* Delivered significant advancements in EC2, with a long list of inventions and deliveries that provide customers with much more functionality than anywhere else.\n",
       "\n",
       "**GenAI Offerings:**\n",
       "\n",
       "* No specific information is provided in the context, but it mentions that AWS continues to deliver new capabilities rapidly, which may include advancements in GenAI offerings.\n",
       "\n",
       "**Major Milestones in 2022:**\n",
       "\n",
       "* Launched over 3,300 new features and services in 2022.\n",
       "* Invested in long-term inventions that change what's possible.\n",
       "* Announced the next generation of generalized chips, with significant adoption among top EC2 customers.\n",
       "* Announced Graviton3, a significant improvement over Graviton2.\n",
       "\n",
       "Note that the provided context is limited, and there may be additional noteworthy advancements not mentioned here.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: NONE</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (1.0) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.88) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: Explain primitives' role in enabling rapid innovation and developer flexibility in software development.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "According to the context, primitives play a crucial role in enabling rapid innovation and developer flexibility in software development. Here's how:\n",
       "\n",
       "1. **Speed**: Primitives allow for rapid innovation by providing a set of basic building blocks that can be combined to create new solutions quickly.\n",
       "2. **Modularity**: Primitives are designed to be functionally split into smaller, independent components that can be used together to achieve a specific goal. This modularity enables developers to focus on one aspect of the solution at a time, making it easier to innovate and iterate.\n",
       "3. **Developer flexibility**: Primitives are optimized for developer freedom and innovation, rather than being constrained by rigid rules or guardrails. This allows developers to use primitives in creative ways, experimenting with new combinations and applications.\n",
       "4. **Degrees of freedom**: By building with primitives, developers can expand their degrees of freedom, meaning they have more options and possibilities for creating new solutions.\n",
       "\n",
       "Overall, primitives provide a foundation for rapid innovation and developer flexibility by offering a set of modular, adaptable building blocks that can be combined and recombined to create new solutions quickly and efficiently.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (1.0) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.08) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: What advancements did AWS make in GenAI offerings, Prime Video content/engagement, and Project Kuiper satellite tech over the past year?</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "Unfortunately, the provided context does not mention AWS's advancements in GenAI offerings, Prime Video content/engagement, and Project Kuiper satellite tech over the past year. The context only talks about AWS's rapid delivery of new capabilities, investments in long-term inventions, and specific advancements in EC2, such as the announcement of the next generation of generalized EC2 instances and the launch of Graviton3.\n",
       "\n",
       "However, I can suggest some possible sources where you may be able to find the information you're looking for:\n",
       "\n",
       "* AWS's official blog or news section\n",
       "* AWS's re:Invent conference announcements\n",
       "* Amazon's annual reports or investor presentations\n",
       "* Industry news articles or tech publications\n",
       "\n",
       "If you provide more context or clarify what specific information you're looking for, I'd be happy to try and help you find it.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (0.34) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.05) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: According to the given context, the key aspect of Amazon's investment philosophy is prioritizing growth through bold, market-leadership focused investments over short-term profitability, while maintaining a lean, cost-conscious culture and emphasizing long-term cash flows over short-term accounting metrics.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "That's not entirely accurate. The given context doesn't explicitly mention prioritizing growth over short-term profitability or maintaining a lean, cost-conscious culture. However, it does mention that Amazon is investing heavily in areas such as Large Language Models (\"LLMs\") and Generative AI, and that they believe there is considerable upside in each of the businesses they're investing in.\n",
       "\n",
       "The context also mentions that Amazon is focused on inventing and making customers' lives better and easier, and that they have a long way to go in every one of their businesses. This suggests that Amazon's investment philosophy is focused on long-term growth and innovation, rather than short-term gains.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: NONE</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (1.0) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.88) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: As per Amazon's philosophy, what are 'primitives' and how do they enable developers' flexibility, innovation, and faster software development?</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "According to Amazon's philosophy, 'primitives' refer to discrete, foundational building blocks that are indivisible (if they can be functionally split into two, they must) and do one thing really well. These primitives are designed to be used together to build more complex solutions, rather than being used as standalone solutions.\n",
       "\n",
       "By providing primitives, Amazon enables developers' flexibility, innovation, and faster software development in several ways:\n",
       "\n",
       "1. **Developer freedom**: By not imposing constraints on primitives to guard against developers hurting themselves, Amazon optimizes for developer freedom, allowing them to use primitives in innovative and creative ways.\n",
       "2. **Modularity**: Primitives are designed to be modular, allowing developers to weave them together in various combinations to build custom solutions.\n",
       "3. **Reusability**: Primitives can be reused across different applications and services, reducing development time and increasing efficiency.\n",
       "4. **Innovation**: By providing a set of building blocks, Amazon encourages developers to think creatively and come up with new solutions that might not have been possible with more rigid, monolithic architectures.\n",
       "\n",
       "Overall, Amazon's primitive services enable developers to build complex systems quickly and efficiently, while also promoting innovation, flexibility, and creativity.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (1.0) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.09) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='red'>Question: Compare the company's approach to investment decisions, accounting practices, employee compensation, and growth prioritization as outlined in the bulleted list.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>LLM Response: \n",
       "\n",
       "Based on the provided context, the company's approach to investment decisions and growth prioritization can be compared as follows:\n",
       "\n",
       "**Investment Decisions:**\n",
       "The company prioritizes long-term market leadership considerations when making investment decisions. This suggests that they are willing to invest in opportunities that may not yield immediate returns but have the potential to drive long-term growth and market leadership.\n",
       "\n",
       "**Growth Prioritization:**\n",
       "The company's focus on long-term market leadership considerations implies that they prioritize sustainable growth over short-term gains. They are likely to prioritize investments that drive long-term growth, even if it means sacrificing some short-term profits.\n",
       "\n",
       "However, the provided context does not mention the company's approach to:\n",
       "\n",
       "* **Accounting Practices:** There is no information on the company's accounting practices, such as their approach to revenue recognition, expense management, or financial reporting.\n",
       "* **Employee Compensation:** There is no information on the company's approach to employee compensation, such as their salary structures, bonus schemes, or benefits packages.\n",
       "\n",
       "Overall, the company's approach to investment decisions and growth prioritization suggests a long-term focus, but more information is needed to understand their approach to accounting practices and employee compensation.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail Action: GUARDRAIL_INTERVENED</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Guardrail output: I apologize, but I cannot provide this information</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevance score (0.98) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Grounding score (0.04) vs. threshold(0.85)</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " --- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "\n",
    "for _idx, row in eval_df.iterrows():\n",
    "    _question = row['input'].split(':')[1].strip() if len(row['input'].split(':')) > 1 else row['input']\n",
    "    sources_ref_list = chroma_retriver.invoke(_question)\n",
    "    guardrail_resp, llm_resp = contextual_grounding_check(\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version_id,\n",
    "        bedrock_model_id=llama3_1_70b_model_id,\n",
    "        input_prompt=_question,\n",
    "        context_lists=sources_ref_list,\n",
    "        boto_session=boto_session\n",
    "    )\n",
    "    _ind, rel_scor, ground_scor, rel_thre, ground_thre, _guardrail_action, \\\n",
    "        _guardrail_output = hallucination_detection(guardrail_resp)\n",
    "    display(Markdown(\"<font color='red'>Question: {}</font>\".format(_question)))\n",
    "    display(Markdown(\"<font color='blue'>LLM Response: {}</font>\".format(\n",
    "        llm_resp.get('output').get('message').get('content')[0].get('text')\n",
    "    )))\n",
    "    display(Markdown(\"<font color='green'>Guardrail Action: {}</font>\".format(_guardrail_action)))\n",
    "    display(Markdown(\"<font color='green'>Guardrail output: {}</font>\".format(_guardrail_output)))\n",
    "    display(Markdown(\n",
    "        \"<font color='green'>Relevance score ({rel_scor}) vs. threshold({rel_thre})</font>\"\\\n",
    "        .format(rel_scor=rel_scor, rel_thre=rel_thre)))\n",
    "    display(Markdown(\n",
    "        \"<font color='green'>Grounding score ({ground_scor}) vs. threshold({ground_thre})</font>\"\\\n",
    "        .format(ground_scor=ground_scor, ground_thre=ground_thre)))\n",
    "    display(Markdown(' --- '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259efb-33bc-4e09-b707-07cf3cc396d0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "In this notebook, we have demonstrated how to use **Bedrock guardrail** as independent guardrail for hallucination detection. The **contextual grounding feature** proved effective in identifying potential hallucinations by comparing model responses against reference information. Relevance and grounding scores provided quantitative measures to assess the accuracy of model outputs.\n",
    "\n",
    "- **Grounding Threshold**: This represents the minimum confidence score for a model response to be considered grounded. Responses with scores below this threshold are deemed to contain information not supported by the reference source.\n",
    "\n",
    "- **Relevance Threshold**: This is the minimum confidence score for a model response to be considered relevant to the user's query. Responses scoring below this threshold are considered off-topic or not addressing the user's question adequately.\n",
    "\n",
    "The `ApplyGuardrail` API demonstrated the ability to decouple guardrails from specific foundation models, allowing for more versatile and model-agnostic content moderation. This decoupling enables the application of consistent safety measures across different AI models, including custom or third-party foundation models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
