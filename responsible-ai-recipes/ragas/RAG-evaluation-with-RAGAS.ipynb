{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90dc48b-b372-461f-bdd2-1b643a9d0684",
   "metadata": {},
   "source": [
    "# RAG  evaluation with RAGAS\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "In this notebook, we'll explore ways to evaluate the quality of [Retrieval-Augmented Generation (RAG) application](https://aws.amazon.com/what-is/retrieval-augmented-generation/) using open-source solution at solution development time with [**Ragas**](https://docs.ragas.io/en/stable/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf979c36-0865-4d84-a51d-b48fe6f552e5",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Update and install prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f1466a-75d1-4ed2-85e0-2d32092952b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.35.48 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.1 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c43319-ef79-4a3e-bc7a-0aca5956644a",
   "metadata": {},
   "source": [
    "### Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01556ffc-8943-462b-a3f6-a2f139b07334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>n_chunks_per_context</th>\n",
       "      <th>context_length</th>\n",
       "      <th>evolutions</th>\n",
       "      <th>context_quality</th>\n",
       "      <th>synthetic_input_quality</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rewritten Input: Explain Amazon's core mission...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon's core mission is to make customers' li...</td>\n",
       "      <td>[across Amazon. Y et, I think every one of us ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2361</td>\n",
       "      <td>['Reasoning']</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>./_raw_data/AMZN-2023-Shareholder-Letter.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compare Amazon's approach to empowering builde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon's approach to empowering builders and i...</td>\n",
       "      <td>[across Amazon. Y et, I think every one of us ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2361</td>\n",
       "      <td>['Comparative']</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>./_raw_data/AMZN-2023-Shareholder-Letter.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  actual_output  \\\n",
       "0  Rewritten Input: Explain Amazon's core mission...            NaN   \n",
       "1  Compare Amazon's approach to empowering builde...            NaN   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  Amazon's core mission is to make customers' li...   \n",
       "1  Amazon's approach to empowering builders and i...   \n",
       "\n",
       "                                             context  retrieval_context  \\\n",
       "0  [across Amazon. Y et, I think every one of us ...                NaN   \n",
       "1  [across Amazon. Y et, I think every one of us ...                NaN   \n",
       "\n",
       "   n_chunks_per_context  context_length       evolutions  context_quality  \\\n",
       "0                     1            2361    ['Reasoning']              0.8   \n",
       "1                     1            2361  ['Comparative']              0.8   \n",
       "\n",
       "   synthetic_input_quality                                   source_file  \n",
       "0                      1.0  ./_raw_data/AMZN-2023-Shareholder-Letter.pdf  \n",
       "1                      0.6  ./_raw_data/AMZN-2023-Shareholder-Letter.pdf  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "eval_df = pd.read_csv('./../_eval_data/eval_dataframe.csv')\n",
    "eval_df['context'] = eval_df.context.apply(lambda s: list(ast.literal_eval(s)))  # convert str to list\n",
    "print(eval_df.shape)\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f42fe-4de9-4bc2-a370-5213b37aa8cf",
   "metadata": {},
   "source": [
    "### Connect to existing vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca9dde4-a3c8-41b7-89bd-930655a1f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "\n",
    "chroma_db_dir = './../_vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "boto_session = boto3.session.Session()\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    persist_directory=chroma_db_dir,\n",
    ")\n",
    "\n",
    "chroma_retriver = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac58b92-51a3-4e6d-ad87-798e4ec19805",
   "metadata": {},
   "source": [
    "## RAGAS\n",
    "---\n",
    "\n",
    "As shown in [Ragas' API reference](https://docs.ragas.io/en/stable/references/evaluate/), records in Ragas evaluation datasets typically included:\n",
    "- The `question/input prompt` that was asked\n",
    "- The `answer` that LLM generated\n",
    "- The `actual text contexts` the answer was based on (i.e., retrieval chunks from vector database)\n",
    "- The `ground truth` answer(s)\n",
    "\n",
    "Let's quickly build simple RAG flow to gather and build the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a7c6a1-b5e4-4f56-8572-bde5d773c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "llama3_1_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_1_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.01,\n",
    "    },\n",
    ")\n",
    "system_prompt = ('''\n",
    "You are an expert, truthful assistant. You will be provided the task by human.\n",
    "No need to mention on the context, provide your respond directly.\n",
    "\n",
    "Here is the context: {context}\n",
    "''')\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llama3_1_70b_langchain, prompt_template)\n",
    "rag_chain = create_retrieval_chain(chroma_retriver, qna_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90c7094-7d69-46fb-9abd-9b5a5494e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_eval = pd.DataFrame(\n",
    "    columns=[\n",
    "        'input', 'expected_answer', 'llm_answer',\n",
    "        'expected_context', 'llm_context'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for _idx, row in eval_df.iterrows():\n",
    "    _question = row['input'].split(':')[1].strip() \\\n",
    "        if len(row['input'].split(':')) > 1 else row['input'].strip()\n",
    "    _expected_ans = row['expected_output'].strip()\n",
    "    _expected_context = row['context'][0]\n",
    "    _rag_resp = rag_chain.invoke({'input': _question})\n",
    "    _llm_resp = _rag_resp.get('answer', '').strip()\n",
    "    _llm_context = [doc.page_content for doc in _rag_resp.get('context', [])]\n",
    "    _row_to_concat = {\n",
    "        'input': _question,\n",
    "        'expected_answer': _expected_ans,\n",
    "        'llm_answer': _llm_resp,\n",
    "        'expected_context': _expected_context,\n",
    "        'llm_context': _llm_context,\n",
    "    }\n",
    "    df_to_eval = pd.concat([\n",
    "        df_to_eval,\n",
    "        pd.DataFrame([_row_to_concat])\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a323db-277c-4780-b748-1a7b81c91a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_context</th>\n",
       "      <th>llm_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain Amazon's core mission and its approach...</td>\n",
       "      <td>Amazon's core mission is to make customers' li...</td>\n",
       "      <td>Based on the provided text, Amazon's core miss...</td>\n",
       "      <td>across Amazon. Y et, I think every one of us a...</td>\n",
       "      <td>[Amazon.com you can’t choose two out of three”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compare Amazon's approach to empowering builde...</td>\n",
       "      <td>Amazon's approach to empowering builders and i...</td>\n",
       "      <td>Here's a comparison of Amazon's approach to em...</td>\n",
       "      <td>across Amazon. Y et, I think every one of us a...</td>\n",
       "      <td>[The best way we know how to do this is by bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize AWS's noteworthy advancements in chi...</td>\n",
       "      <td>In 2022, AWS achieved several noteworthy advan...</td>\n",
       "      <td>Here is a summary of AWS's noteworthy advancem...</td>\n",
       "      <td>past year was also a significant delivery yea...</td>\n",
       "      <td>[customers, AWS continues to deliver new capab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Explain Amazon's core mission and its approach...   \n",
       "1  Compare Amazon's approach to empowering builde...   \n",
       "2  Summarize AWS's noteworthy advancements in chi...   \n",
       "\n",
       "                                     expected_answer  \\\n",
       "0  Amazon's core mission is to make customers' li...   \n",
       "1  Amazon's approach to empowering builders and i...   \n",
       "2  In 2022, AWS achieved several noteworthy advan...   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "0  Based on the provided text, Amazon's core miss...   \n",
       "1  Here's a comparison of Amazon's approach to em...   \n",
       "2  Here is a summary of AWS's noteworthy advancem...   \n",
       "\n",
       "                                    expected_context  \\\n",
       "0  across Amazon. Y et, I think every one of us a...   \n",
       "1  across Amazon. Y et, I think every one of us a...   \n",
       "2   past year was also a significant delivery yea...   \n",
       "\n",
       "                                         llm_context  \n",
       "0  [Amazon.com you can’t choose two out of three”...  \n",
       "1  [The best way we know how to do this is by bui...  \n",
       "2  [customers, AWS continues to deliver new capab...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_eval.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e6208-33d2-41be-84b2-7f4b47a6c688",
   "metadata": {},
   "source": [
    "Ragas offers a [broad range of metrics](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/) with the option to configure which ones you calculate, however it **depends on evaluator LLM (or evaluator embedding models)** which will be used in scoring.\n",
    "\n",
    "In this example, we will set up **Claude 3 Sonnet** as the evaluator LLM and **Amazon Titan Text Embeddings V2** as the evaluator embedding model, to demonstrate the full suite of available metrics. However, do feel free to change to your requirements.\n",
    "\n",
    "Although Ragas defines its own base classes([`BaseRagasLLM`](https://github.com/explodinggradients/ragas/blob/main/src/ragas/llms/base.py#L47), and [`BaseRagasEmbeddings`](https://github.com/explodinggradients/ragas/blob/main/src/ragas/embeddings/base.py#L22)) for these interfaces, integration is typically via **LangChain** for simplicity.\n",
    "\n",
    "### Batch evaluation \n",
    "---\n",
    "You can pass `Dataset` object and list of metrics to `evaluate` API to conduct batch evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b57a917-9eca-4e17-b760-03f0aa3b6aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac14f6027ee49cdb1b945f3bf976bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall scores\n",
      "--------------\n",
      "{'answer_relevancy': 0.6464, 'faithfulness': 0.6500, 'context_precision': 1.0000, 'context_recall': 0.6167, 'semantic_similarity': 0.7389}\n",
      "\n",
      "Details\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compare the company's approach to investment d...</td>\n",
       "      <td>[Because of our emphasis on the long term, we ...</td>\n",
       "      <td>Based on the provided text, here is a comparis...</td>\n",
       "      <td>• We will make bold rather than timid investme...</td>\n",
       "      <td>0.337317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.639586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain primitives' role in enabling rapid inn...</td>\n",
       "      <td>[Of course, this concept of primitives can be ...</td>\n",
       "      <td>Primitives play a crucial role in enabling rap...</td>\n",
       "      <td>document:\\n“Primitives are the raw parts or t...</td>\n",
       "      <td>0.955527</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.838155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Compare the company's approach to investment d...   \n",
       "1  Explain primitives' role in enabling rapid inn...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Because of our emphasis on the long term, we ...   \n",
       "1  [Of course, this concept of primitives can be ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the provided text, here is a comparis...   \n",
       "1  Primitives play a crucial role in enabling rap...   \n",
       "\n",
       "                                           reference  answer_relevancy  \\\n",
       "0  • We will make bold rather than timid investme...          0.337317   \n",
       "1   document:\\n“Primitives are the raw parts or t...          0.955527   \n",
       "\n",
       "   faithfulness  context_precision  context_recall  semantic_similarity  \n",
       "0           1.0                1.0        0.833333             0.639586  \n",
       "1           0.3                1.0        0.400000             0.838155  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ragas\n",
    "from datasets import Dataset\n",
    "\n",
    "ragas_result = ragas.evaluation.evaluate(\n",
    "    Dataset.from_pandas(df_to_eval.sample(n=2)),\n",
    "    metrics=[\n",
    "        ragas.metrics.answer_relevancy,\n",
    "        ragas.metrics.faithfulness,\n",
    "        ragas.metrics.context_precision,\n",
    "        ragas.metrics.context_recall,\n",
    "        ragas.metrics.answer_similarity,\n",
    "    ],\n",
    "    llm=ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs={\n",
    "            'temperature': 0,\n",
    "            'max_tokens': 2048,\n",
    "        },\n",
    "    ),\n",
    "    embeddings=titan_embedding_fn,\n",
    "    column_map={\n",
    "        'answer': 'llm_answer',\n",
    "        'contexts': 'llm_context',\n",
    "        'ground_truths': 'expected_answer',\n",
    "        'question': 'input',\n",
    "        'reference': 'expected_context',\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Overall scores\")\n",
    "print(\"--------------\")\n",
    "print(ragas_result, end=\"\\n\\n\")\n",
    "print(\"Details\")\n",
    "print(\"-------\")\n",
    "scores_df = ragas_result.to_pandas()\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650a0ce-a550-4104-bac4-96b810d7cd59",
   "metadata": {},
   "source": [
    "### Evaluation Sample\n",
    "---\n",
    "Alternatively, you can pick one evaluation metric and evaluate one at a time. However, you will either need to pass `BaseRagasLLM` or `BaseRagasEmbeddings` to that evaluation metric.\n",
    "\n",
    "For this you will need to construct **evaluation sample**, an **evaluation sample** is a single structured data instance used for assessing and measuring the performance for your LLM application. This represents a single unit of interaction or specific use case. In Ragas, evaluation samples can be represented using the `SingleTurnSample` or `MultiTurnSample` classes.\n",
    "\n",
    "#### SingleTurnSample\n",
    "\n",
    "`SingleTurnSample` represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.\n",
    "\n",
    "\n",
    "#### MultiTurnSample\n",
    "\n",
    "`MultiTurnSample` represents a multi-turn interaction between Human, AI and, optionally, a Tool and expected results for evaluation. It is suitable for representing conversational agents in more complex interactions for evaluation.\n",
    "\n",
    "\n",
    "In this example, we will only explore `SingleTurnSample` use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f26e273-0996-499c-a624-1983d2584235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 2048,\n",
    "    },\n",
    "))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    titan_embedding_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f13d1f00-7af3-4e95-a9f2-573039b24d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question_df = df_to_eval.sample(n=1)\n",
    "input_prompt = sample_question_df['input'].values[0]\n",
    "expected_ans = sample_question_df['expected_answer'].values[0]\n",
    "llm_ans = sample_question_df['llm_answer'].values[0]\n",
    "reference = sample_question_df['expected_context'].values[0]\n",
    "retrieved_chunks = sample_question_df['llm_context'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c4e81-ebd7-47ba-97b5-099d39400e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic similarity score: 0.9433280634525408\n",
      "Relevant noise sensitivity score: 0.2\n",
      "Factual correctness score: 0.74\n"
     ]
    }
   ],
   "source": [
    "from ragas import SingleTurnSample \n",
    "from ragas.metrics import NoiseSensitivity, SemanticSimilarity, FactualCorrectness\n",
    "\n",
    "sample_single_turn = SingleTurnSample(\n",
    "    user_input=input_prompt,\n",
    "    retrieved_contexts=retrieved_chunks,\n",
    "    reference_contexts=[reference],\n",
    "    response=llm_ans,\n",
    "    reference=expected_ans,\n",
    ")\n",
    "\n",
    "ss_scorer = SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "rel_noise_scorer = NoiseSensitivity(llm=evaluator_llm, focus='relevant')\n",
    "factual_scorer = FactualCorrectness(llm=evaluator_llm)\n",
    "\n",
    "print(\n",
    "    'semantic similarity score: {}'.format(ss_scorer.single_turn_score(sample_single_turn))\n",
    ")\n",
    "print(\n",
    "    'Relevant noise sensitivity score: {}'.format(rel_noise_scorer.single_turn_score(sample_single_turn))\n",
    ")\n",
    "print(\n",
    "    'Factual correctness score: {}'.format(factual_scorer.single_turn_score(sample_single_turn))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629ed41-b0b6-4fad-9054-3597e4382a85",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "In this notebook, we have seen how to integrate and use **Ragas** open source LLM evaluation toolkits, with easy to use and integration with `langchain` and `llama-index` framework. AI developers and researchers can use it to improve and optimize their RAG application.\n",
    "\n",
    "Even though Ragas provides easy to use, prebuilt evaluation metrics and bring-your-own evaluation metrics, it's important to remember that the included metrics will be using LLM-evaluated and therefore potentially subject to bias on the selected (evaluator) LLM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
