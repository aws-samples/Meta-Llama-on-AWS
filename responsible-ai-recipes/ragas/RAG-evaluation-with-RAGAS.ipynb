{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90dc48b-b372-461f-bdd2-1b643a9d0684",
   "metadata": {},
   "source": [
    "# RAG  evaluation with RAGAS\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "In this notebook, we'll explore ways to evaluate the quality of [Retrieval-Augmented Generation (RAG) application](https://aws.amazon.com/what-is/retrieval-augmented-generation/) using open-source solution at solution development time with [**Ragas**](https://docs.ragas.io/en/stable/).\n",
    "\n",
    "You can utilize **Ragas** to generate synthetic dataset, and do RAG evaluation. In this notebook, we will explore on evaluating RAG workflow with **Meta Llama** foundation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf979c36-0865-4d84-a51d-b48fe6f552e5",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Update and install prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1466a-75d1-4ed2-85e0-2d32092952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c43319-ef79-4a3e-bc7a-0aca5956644a",
   "metadata": {},
   "source": [
    "### Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01556ffc-8943-462b-a3f6-a2f139b07334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>compressed_question</th>\n",
       "      <th>ref_answer</th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>source_chunk</th>\n",
       "      <th>source_document</th>\n",
       "      <th>groundedness_rating</th>\n",
       "      <th>groundedness_reason</th>\n",
       "      <th>relevance_rating</th>\n",
       "      <th>relevance_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the names of the chips that were anno...</td>\n",
       "      <td>Which 2nd-gen chipsets are being utilized by A...</td>\n",
       "      <td>Trainium and Inferentia.</td>\n",
       "      <td>announced second versions of our Trainium and ...</td>\n",
       "      <td>[announced second versions of our Trainium and...</td>\n",
       "      <td>{'source': '_raw_data/AMZN-2023-Shareholder-Le...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides all the necessary informa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The question is not relevant to a business ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the company's priorities in terms of ...</td>\n",
       "      <td>What are key spend &amp; cultural priorities for a...</td>\n",
       "      <td>The company's priorities in terms of spending ...</td>\n",
       "      <td>We will work hard to spend wisely and maintain...</td>\n",
       "      <td>[the present value of future cash flows, we’ll...</td>\n",
       "      <td>{'source': '_raw_data/AMZN-2021-Shareholder-Le...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context provides all the necessary informa...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The question is very relevant to a business an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the names of the chips that were anno...   \n",
       "1  What are the company's priorities in terms of ...   \n",
       "\n",
       "                                 compressed_question  \\\n",
       "0  Which 2nd-gen chipsets are being utilized by A...   \n",
       "1  What are key spend & cultural priorities for a...   \n",
       "\n",
       "                                          ref_answer  \\\n",
       "0                           Trainium and Inferentia.   \n",
       "1  The company's priorities in terms of spending ...   \n",
       "\n",
       "                                     source_sentence  \\\n",
       "0  announced second versions of our Trainium and ...   \n",
       "1  We will work hard to spend wisely and maintain...   \n",
       "\n",
       "                                        source_chunk  \\\n",
       "0  [announced second versions of our Trainium and...   \n",
       "1  [the present value of future cash flows, we’ll...   \n",
       "\n",
       "                                     source_document  groundedness_rating  \\\n",
       "0  {'source': '_raw_data/AMZN-2023-Shareholder-Le...                  5.0   \n",
       "1  {'source': '_raw_data/AMZN-2021-Shareholder-Le...                  5.0   \n",
       "\n",
       "                                 groundedness_reason  relevance_rating  \\\n",
       "0  The context provides all the necessary informa...               1.0   \n",
       "1  The context provides all the necessary informa...               5.0   \n",
       "\n",
       "                                    relevance_reason  \n",
       "0  The question is not relevant to a business ana...  \n",
       "1  The question is very relevant to a business an...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "eval_df = pd.read_csv('./../_eval_data/eval_dataframe.csv')\n",
    "eval_df['source_chunk'] = eval_df.source_chunk.apply(\n",
    "    lambda s: list(ast.literal_eval(s))\n",
    ")\n",
    "print(eval_df.shape)\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f42fe-4de9-4bc2-a370-5213b37aa8cf",
   "metadata": {},
   "source": [
    "### Connect to existing vector database\n",
    "---\n",
    "\n",
    "Connect to existing vector store and set the retriever (`k=2` for this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca9dde4-a3c8-41b7-89bd-930655a1f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "\n",
    "chroma_db_dir = './../vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "boto_session = boto3.session.Session()\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    persist_directory=chroma_db_dir,\n",
    ")\n",
    "\n",
    "chroma_retriver = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac58b92-51a3-4e6d-ad87-798e4ec19805",
   "metadata": {},
   "source": [
    "## RAGAS\n",
    "---\n",
    "\n",
    "As shown in [Ragas' API reference](https://docs.ragas.io/en/stable/references/evaluate/), records in Ragas evaluation datasets typically included:\n",
    "- The `question/input prompt` that was asked\n",
    "- The `answer` that LLM generated\n",
    "- The `actual text contexts` the answer was based on (i.e., retrieval chunks from vector database)\n",
    "- The `ground truth` answer(s)\n",
    "\n",
    "Let's quickly build simple RAG flow to gather and build the evaluation dataset. We will use `Llama 3 70B` as candidate (generator) LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a7c6a1-b5e4-4f56-8572-bde5d773c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llama3_70b_model_id = 'meta.llama3-70b-instruct-v1:0'\n",
    "llama3_70b_langchain = ChatBedrock(\n",
    "    model_id=llama3_70b_model_id,\n",
    "    region_name=boto_session.region_name,\n",
    "    model_kwargs={\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.01,\n",
    "    },\n",
    ")\n",
    "system_prompt = ('''\n",
    "You are an expert, truthful assistant. You will be provided the task by human.\n",
    "No need to mention on the context, provide your respond directly.\n",
    "\n",
    "Here is the context: {context}\n",
    "''')\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qna_chain = create_stuff_documents_chain(llama3_70b_langchain, prompt_template)\n",
    "rag_chain = create_retrieval_chain(chroma_retriver, qna_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52457131-5e7d-48ea-a568-d448017c5a46",
   "metadata": {},
   "source": [
    "Let's evaluate one sample data from the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90c7094-7d69-46fb-9abd-9b5a5494e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_eval = pd.DataFrame(\n",
    "    columns=[\n",
    "        'input', 'expected_answer', 'llm_answer',\n",
    "        'expected_context', 'llm_context'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for _idx, row in eval_df.sample(n=1).iterrows():\n",
    "    _question = row['question'].strip()\n",
    "    _expected_ans = row['ref_answer'].strip()\n",
    "    _expected_context = row['source_chunk'][0]\n",
    "    _rag_resp = rag_chain.invoke({'input': _question})\n",
    "    _llm_resp = _rag_resp.get('answer', '').strip()\n",
    "    _llm_context = [doc.page_content for doc in _rag_resp.get('context', [])]\n",
    "    _row_to_concat = {\n",
    "        'input': _question,\n",
    "        'expected_answer': _expected_ans,\n",
    "        'llm_answer': _llm_resp,\n",
    "        'expected_context': _expected_context,\n",
    "        'llm_context': _llm_context,\n",
    "    }\n",
    "    df_to_eval = pd.concat([\n",
    "        df_to_eval,\n",
    "        pd.DataFrame([_row_to_concat])\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a323db-277c-4780-b748-1a7b81c91a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_context</th>\n",
       "      <th>llm_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many SKUs are housed in the new, same-day ...</td>\n",
       "      <td>The new, same-day fulfillment facilities in th...</td>\n",
       "      <td>According to the text, the new, same-day fulfi...</td>\n",
       "      <td>constrained by the primitives you’ve built and...</td>\n",
       "      <td>[constrained by the primitives you’ve built an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  How many SKUs are housed in the new, same-day ...   \n",
       "\n",
       "                                     expected_answer  \\\n",
       "0  The new, same-day fulfillment facilities in th...   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "0  According to the text, the new, same-day fulfi...   \n",
       "\n",
       "                                    expected_context  \\\n",
       "0  constrained by the primitives you’ve built and...   \n",
       "\n",
       "                                         llm_context  \n",
       "0  [constrained by the primitives you’ve built an...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e6208-33d2-41be-84b2-7f4b47a6c688",
   "metadata": {},
   "source": [
    "Ragas offers a [broad range of metrics](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/) with the option to configure which ones you calculate, however it **depends on evaluator LLM (or evaluator embedding models)** which will be used in scoring.\n",
    "\n",
    "In this example, we will set up **Llama 3.2 11B** as the evaluator LLM and **Amazon Titan Text Embeddings V2** as the evaluator embedding model, to demonstrate the full suite of available metrics. However, do feel free to change to your requirements.\n",
    "\n",
    "Although Ragas defines its own base classes([`BaseRagasLLM`](https://github.com/explodinggradients/ragas/blob/main/src/ragas/llms/base.py#L47), and [`BaseRagasEmbeddings`](https://github.com/explodinggradients/ragas/blob/main/src/ragas/embeddings/base.py#L22)) for these interfaces, integration is typically via **LangChain** for simplicity.\n",
    "\n",
    "### Batch evaluation \n",
    "---\n",
    "You can pass `Dataset` object and list of metrics to `evaluate` API to conduct batch evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f5cef1-ce88-4a82-b4e4-ab66f22ecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llama3_2_11b_langchain = ChatBedrockConverse(\n",
    "    model_id='us.meta.llama3-2-11b-instruct-v1:0',\n",
    "    region_name=boto_session.region_name,\n",
    "    temperature=0.,\n",
    "    max_tokens=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c07f7b4-01b7-45aa-8cb7-d71decf85b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "llama3_2_11b_evaluator = LangchainLLMWrapper(\n",
    "    langchain_llm=llama3_2_11b_langchain,\n",
    ")\n",
    "\n",
    "titan_embedding_evaluator = LangchainEmbeddingsWrapper(\n",
    "    titan_embedding_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b57a917-9eca-4e17-b760-03f0aa3b6aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1078804694344e48ff6221038920ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall scores\n",
      "--------------\n",
      "{'answer_relevancy': 0.8389, 'faithfulness': 1.0000, 'context_precision': 1.0000, 'context_recall': 1.0000}\n",
      "\n",
      "Details\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many SKUs are housed in the new, same-day ...</td>\n",
       "      <td>[constrained by the primitives you’ve built an...</td>\n",
       "      <td>According to the text, the new, same-day fulfi...</td>\n",
       "      <td>constrained by the primitives you’ve built and...</td>\n",
       "      <td>0.838868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How many SKUs are housed in the new, same-day ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [constrained by the primitives you’ve built an...   \n",
       "\n",
       "                                            response  \\\n",
       "0  According to the text, the new, same-day fulfi...   \n",
       "\n",
       "                                           reference  answer_relevancy  \\\n",
       "0  constrained by the primitives you’ve built and...          0.838868   \n",
       "\n",
       "   faithfulness  context_precision  context_recall  \n",
       "0           1.0                1.0             1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import ragas\n",
    "from datasets import Dataset\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "ragas_result = evaluate(\n",
    "    Dataset.from_pandas(df_to_eval),\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llama3_2_11b_evaluator,\n",
    "    embeddings=titan_embedding_evaluator,\n",
    "    column_map={\n",
    "        'answer': 'llm_answer',\n",
    "        'contexts': 'llm_context',\n",
    "        'ground_truths': 'expected_answer',\n",
    "        'question': 'input',\n",
    "        'reference': 'expected_context',\n",
    "    },\n",
    "    raise_exceptions=True,\n",
    "    run_config=ragas.RunConfig(\n",
    "        max_workers=1,\n",
    "        timeout=300,\n",
    "        max_retries=5,\n",
    "        max_wait=120,\n",
    "        seed=123,\n",
    "        log_tenacity=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Overall scores\")\n",
    "print(\"--------------\")\n",
    "print(ragas_result, end=\"\\n\\n\")\n",
    "print(\"Details\")\n",
    "print(\"-------\")\n",
    "scores_df = ragas_result.to_pandas()\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650a0ce-a550-4104-bac4-96b810d7cd59",
   "metadata": {},
   "source": [
    "### Evaluation Sample\n",
    "---\n",
    "Alternatively, you can pick one evaluation metric and evaluate one at a time. However, you will either need to pass `BaseRagasLLM` or `BaseRagasEmbeddings` to that evaluation metric.\n",
    "\n",
    "For this you will need to construct **evaluation sample**, an **evaluation sample** is a single structured data instance used for assessing and measuring the performance for your LLM application. This represents a single unit of interaction or specific use case. In Ragas, evaluation samples can be represented using the `SingleTurnSample` or `MultiTurnSample` classes.\n",
    "\n",
    "#### SingleTurnSample\n",
    "\n",
    "`SingleTurnSample` represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.\n",
    "\n",
    "\n",
    "#### MultiTurnSample\n",
    "\n",
    "`MultiTurnSample` represents a multi-turn interaction between Human, AI and, optionally, a Tool and expected results for evaluation. It is suitable for representing conversational agents in more complex interactions for evaluation.\n",
    "\n",
    "\n",
    "In this example, we will only explore `SingleTurnSample` use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13d1f00-7af3-4e95-a9f2-573039b24d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = df_to_eval['input'].values[0]\n",
    "expected_ans = df_to_eval['expected_answer'].values[0]\n",
    "llm_ans = df_to_eval['llm_answer'].values[0]\n",
    "reference = df_to_eval['expected_context'].values[0]\n",
    "retrieved_chunks = df_to_eval['llm_context'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0c4e81-ebd7-47ba-97b5-099d39400e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic similarity score: 0.9677737957766728\n",
      "Relevant noise sensitivity score: 0.0\n",
      "Factual correctness score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas import SingleTurnSample \n",
    "from ragas.metrics import NoiseSensitivity, SemanticSimilarity, FactualCorrectness\n",
    "\n",
    "sample_single_turn = SingleTurnSample(\n",
    "    user_input=input_prompt,\n",
    "    retrieved_contexts=retrieved_chunks,\n",
    "    reference_contexts=[reference],\n",
    "    response=llm_ans,\n",
    "    reference=expected_ans,\n",
    ")\n",
    "\n",
    "ss_scorer = SemanticSimilarity(embeddings=titan_embedding_evaluator)\n",
    "rel_noise_scorer = NoiseSensitivity(llm=llama3_2_11b_evaluator, focus='relevant')\n",
    "factual_scorer = FactualCorrectness(llm=llama3_2_11b_evaluator)\n",
    "\n",
    "print(\n",
    "    'semantic similarity score: {}'.format(ss_scorer.single_turn_score(sample_single_turn))\n",
    ")\n",
    "print(\n",
    "    'Relevant noise sensitivity score: {}'.format(rel_noise_scorer.single_turn_score(sample_single_turn))\n",
    ")\n",
    "print(\n",
    "    'Factual correctness score: {}'.format(factual_scorer.single_turn_score(sample_single_turn))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629ed41-b0b6-4fad-9054-3597e4382a85",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "In this notebook, we have seen how to integrate and use **Ragas** open source LLM evaluation toolkits, with easy to use and integration with `langchain` framework. AI developers and researchers can use it to improve and optimize their RAG application.\n",
    "\n",
    "Even though Ragas provides easy to use, prebuilt evaluation metrics and bring-your-own evaluation metrics, it's important to remember that the included metrics will be using LLM-evaluated and therefore potentially subject to bias on the selected (evaluator) LLM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
