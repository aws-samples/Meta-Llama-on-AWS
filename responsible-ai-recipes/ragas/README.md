# RAGAS

## Introduction

[**Ragas**](https://docs.ragas.io/en/stable/) is an open source toolkit for evaluating and optimizing Large Language Model (LLM) applications. It provides a broad range of pre-built metrics that can help summarize performance and pinpoint the limitations of RAG applications on validation datasets.

**Ragas** provides prebuilt evaluation metrics for various use cases (for example; RAG, Agents or tools use cases). Please refer to the [documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) for the full list of metrics provided by RAGAS.


## Objectives
In this repository, we will demonstrate how to use **Ragas** framework to evaluate RAG application. We will make use Meta Llama foundation models for our candidate and evaluator LLMs.


## Pricing
Given we will be using Amazon Bedrock as our candidate LLM and evaluator LLM, please refer to [Amazon Bedrock Pricing page](https://aws.amazon.com/bedrock/pricing/).