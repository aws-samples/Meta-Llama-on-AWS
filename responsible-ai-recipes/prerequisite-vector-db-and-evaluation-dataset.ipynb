{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149cbb30-b95e-4dc8-b9ad-db8d4dea09cb",
   "metadata": {},
   "source": [
    "# Prerequisite Step: Set up vector database and evaluation dataset\n",
    "---\n",
    "\n",
    "This notebook is requisite step for all the RAG evaluation task in this repository. It covers the aspects of;\n",
    "- Download the sample dataset, we will utilize Amazon Shareholder letters as our data sources.\n",
    "- Set up **Chroma** database as our vector database.\n",
    "- Use **DeepEval** library to generate the **golden or evaluation** dataset for our RAG application.\n",
    "\n",
    "\n",
    "You will need to have access to **Amazon Bedrock** foundation model for embedding the documents, please refer to the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) for more details.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note</b>: We will be using <b>Amazon Titan Text Embedding v2 model</b> (<i>amazon.titan-embed-text-v2:0</i>). Please refer to its capability <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a88a1e-9c4b-443a-94cb-10756050052b",
   "metadata": {},
   "source": [
    "## Set up\n",
    "---\n",
    "\n",
    "Install the dependency libraries for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba30a4-7906-496a-b191-6beef6bd4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b17c44-7867-4480-a387-085ebad2aef4",
   "metadata": {},
   "source": [
    "## Set up Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e4393-a14b-4f35-823e-8b4a078d46d8",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "---\n",
    "\n",
    "We will be using Amazon shareholder letter from 2021 to 2023 as our datasources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fafe11-3bf1-4ed0-a5e1-737ac9f7d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url_file_map = {\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf': 'AMZN-2023-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf': 'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf': 'AMZN-2021-Shareholder-Letter.pdf',\n",
    "}\n",
    "data_dir = './_raw_data/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "for _key in url_file_map.keys():\n",
    "    urlretrieve(_key, os.path.join(data_dir, url_file_map.get(_key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ee8bd-9645-4598-9ef8-3a33c07e9c47",
   "metadata": {},
   "source": [
    "### Chunking strategy\n",
    "---\n",
    "\n",
    "**Chunking data** before loading it into a vector database is often necessary because vector databases are optimized for **efficient similarity search and retrieval operations on high-dimensional vector data**. These databases typically have limitations on the maximum size or dimensionality of vectors they can store and process efficiently. \n",
    "\n",
    "By chunking or splitting large datasets into smaller, manageable chunks, it becomes easier to load and index the data within the vector database's constraints. Chunking also facilitates parallel processing, allowing multiple chunks to be loaded concurrently, improving overall performance and scalability. Additionally, it provides a way to manage and update the data incrementally, as new chunks can be added or existing ones can be modified without requiring a complete reload of the entire dataset.\n",
    "\n",
    "There are multiple chunking strategy, however for simplicity, we will use [`RecursiveCharacterTextSplitter()` as our chunking strategy](https://python.langchain.com/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "\n",
    "There are a few parameters we can configure for our `RecursiveCharacterTextSplitter`:\n",
    "\n",
    "- `chunk_size`: The maximum size of a chunk, where size is determined by the length_function.\n",
    "- `chunk_overlap`: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "- `length_function`: Function determining the chunk size.\n",
    "- `is_separator_regex`: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note</b>: You must take the <b>chunk_size</b> into account as each embedding model will have limitation on the length of input token.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e656e-a239-4688-8dba-242f8bd49249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(data_dir)\n",
    "pages = loader.load_and_split()\n",
    "print('Total document pages: {}'.format(len(pages)))\n",
    "print('Sample data load: {}'.format(pages[0].page_content[:100]))\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=102\n",
    ")\n",
    "rec_docs_splitted = rec_splitter.split_documents(pages)\n",
    "print(' ----- ')\n",
    "print('Total chunks: {}'.format(len(rec_docs_splitted)))\n",
    "print('Sample chunk:\\n{}'.format(rec_docs_splitted[3].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5509c-9c73-4663-bb42-4b45323d324b",
   "metadata": {},
   "source": [
    "### Prepare Vector Database\n",
    "---\n",
    "\n",
    "Once we have our chunk documents, next step, we will prepare vector database. In this example, we will utilize **Chroma database**. [**ChromaDB**](https://www.trychroma.com/) is an open-source vector database designed for building applications that require efficient semantic search and retrieval capabilities.\n",
    "\n",
    "But before creating ChromaDB, we will need to initialize the embedding model function. We can use [`BedrockEmbeddings` class](https://api.python.langchain.com/en/latest/embeddings/langchain_aws.embeddings.bedrock.BedrockEmbeddings.html) from **langchain_aws** to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d12ad-d4da-4930-b0d3-c59c981e7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "titan_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "titan_embedding_fn = BedrockEmbeddings(\n",
    "    model_id=titan_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "titan_embedding_fn.embed_query('Hello')[: 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab603791-e6bf-4da4-9445-04b163a25588",
   "metadata": {},
   "source": [
    "Now with embedding function, we can specify `Chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffeca9-fd79-46b2-8cfd-3fb0b7ddc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db_dir = './_vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "\n",
    "# Init from Chroma client\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=titan_embedding_fn,\n",
    "    persist_directory=chroma_db_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343113e-951d-47cd-837f-837be9463a82",
   "metadata": {},
   "source": [
    "Load the documents to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8524059-bfb4-404e-ab5c-ae482c672419",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(vector_store.get().get('ids')) == 0:\n",
    "    vector_store = Chroma.from_documents(\n",
    "        collection_name=chroma_collection_name,\n",
    "        documents=rec_docs_splitted,\n",
    "        persist_directory=chroma_db_dir,\n",
    "        embedding=titan_embedding_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d55e3-9931-4784-b76f-cf8e80e3ff32",
   "metadata": {},
   "source": [
    "### Test query our vector store\n",
    "---\n",
    "Now we have vector database with the data in it, let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51d213-31b1-4c9f-9585-09b84407df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = '''\n",
    "Amazon discusses its investments and progress in various areas, such as Generative AI, logistics, and healthcare. \n",
    "How do these initiatives relate to the company's strategy of building \"primitives\" or foundational building blocks, \n",
    "and what potential customer experiences or business opportunities do they enable?'''\n",
    "\n",
    "search_result = vector_store.similarity_search_with_relevance_scores(\n",
    "    query=sample_question.strip(),\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fc9fd-9be5-41f2-9018-eb80bc9d1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a05dda-d346-41b7-9827-4565f6fb8393",
   "metadata": {},
   "source": [
    "Now, we have establishing ChromaDB for our vector database, which will be used in the subseqent notebooks for RAG evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2efd2a-ff6e-40a8-9944-857e9d667e1b",
   "metadata": {},
   "source": [
    "## Synthetic Evaluation dataset\n",
    "\n",
    "In this section, we will generate the **golden** or **evaluation** dataset used to evaluate RAG application. We will utilize `DeepEval` library for this purpose.\n",
    "\n",
    "\n",
    "### Synthesizer\n",
    "---\n",
    "\n",
    "`DeepEval`'s **Synthesizer** offers a fast and easy to automatically get started with testing your LLM by generating high-quality evaluation datasets (inputs, expected outputs, and contexts) from scratch. The default of **Synthesizer** class will be using `OpenAI`, hence we will need to create two custom LLM handlers to use with our **Amazon Bedrock** model, one for embedding model, and one for language models.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Remark</b>: We will pass the LLMs in langchain form to DeepEval.\n",
    "</div>\n",
    "\n",
    "Please refer to [DeepEval's source code](https://github.com/confident-ai/deepeval/blob/main/deepeval/synthesizer/synthesizer.py) and [documentation](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data) to help with adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e496ff4-db14-4951-83c5-7f565e2574f0",
   "metadata": {},
   "source": [
    "#### Custom Bedrock Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c526b-1b97-4b51-a54f-af044b41e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepeval\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class BedrockEmbeddingDeepEval(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self, model: langchain_aws.embeddings):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.model_id\n",
    "\n",
    "    def get_provider(self) -> str:\n",
    "        model_id = self.get_model_name()\n",
    "        return model_id.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feace4bc-6635-4e66-84fe-41c8850ff807",
   "metadata": {},
   "outputs": [],
   "source": [
    "titan_embedding_deepeval = BedrockEmbeddingDeepEval(model=titan_embedding_fn)\n",
    "titan_embedding_deepeval.embed_text('Hello')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91edaf-2e70-4083-9c89-7f234029b136",
   "metadata": {},
   "source": [
    "#### Custom Bedrock LLM\n",
    "---\n",
    "For text generation model, we will use **Anthropic Claude 3 Sonnet on Amazon Bedrock**. However, please feel free to change to other LLMs like Llama 3.1 70B or 405B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f6d44-f96f-4a8e-b55b-3dd93409c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_aws\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "claude3_sonnet_model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "\n",
    "claude_sonnet_langchain = ChatBedrock(\n",
    "    model_id=claude3_sonnet_model_id,\n",
    "    region_name=boto_session.region_name\n",
    ")\n",
    "claude_sonnet_langchain.invoke('What is L in LLM?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54fb28-99f7-48dd-bf5b-28f675b97606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class BedrockTextGenDeepEval(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: langchain_aws.chat_models\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        llm_model = self.load_model()\n",
    "        res = await llm_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        llm_model = self.load_model()\n",
    "        return llm_model.model_id\n",
    "\n",
    "    def get_provider(self):\n",
    "        model_id = self.get_model_name()\n",
    "        return model_id.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a42d4-7843-435d-a781-918b8537dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_sonnet_deepeval = BedrockTextGenDeepEval(model=claude_sonnet_langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5d90-d7d2-4ca1-a299-bd918d840be9",
   "metadata": {},
   "source": [
    "#### Initialize Synthesizer with custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e825807-57a1-4709-ba38-64c8497fcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_synthesizer = Synthesizer(\n",
    "    model=claude_sonnet_deepeval,\n",
    "    critic_model=claude_sonnet_deepeval,\n",
    "    embedder=titan_embedding_deepeval,\n",
    "    context_quality_threshold=.8,\n",
    "    context_similarity_threshold=.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13aa88-5935-4e56-aa04-8bbd6494c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time;time.sleep(60)\n",
    "_out = custom_synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=[os.path.join(data_dir, 'AMZN-2023-Shareholder-Letter.pdf')],\n",
    "    include_expected_output=True,\n",
    "    max_contexts_per_document=4,\n",
    "    max_goldens_per_context=2,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=102,\n",
    "    _send_data=False,\n",
    "    num_evolutions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75106ac1-5b1f-42a0-9bfc-533eee4c5658",
   "metadata": {},
   "source": [
    "### Save evaluation to dataframe and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c04e10-3bc4-47cf-aa6c-64d91cf30636",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = custom_synthesizer.to_pandas()\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c3d24-1d04-4039-b527-8271362a3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_dir = './_eval_data'\n",
    "os.makedirs(eval_data_dir, exist_ok=True)\n",
    "eval_df.to_csv(os.path.join(eval_data_dir, 'eval_dataframe.csv'), index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
