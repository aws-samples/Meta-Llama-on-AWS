{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679b0256-f0c4-425a-bd71-4b2ec51934fa",
   "metadata": {},
   "source": [
    "# LlamaIndex Evaluation\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "[**LlamaIndex**](https://www.llamaindex.ai/) is an open-source framework that connect the data sources to large language models (LLMs). Developers can utilize LlamaIndex to build the generative AI application powered by large language model, particularly retrieval augmented generation (RAG).\n",
    "\n",
    "LlamaIndex offers evaluation based on the requirements and objectives based on what you want. If you want to get an overall idea of how [RAG](https://aws.amazon.com/what-is/retrieval-augmented-generation/) system is doing, you can start with **end-to-end evaluation (E2E)** as an overall sanity check.\n",
    "\n",
    "Otherwise, if you have an idea of which components you want to iterate step-by-step, you may want to start with a **component-wise evaluation**. However, you may run into the risk of <u>premature optimization</u> - making or optimizing model selection or parameter choices without assessing the overall application needs.\n",
    "\n",
    "There are no one-size-fit-all approaches, evaluation is somewhat you iterate, and experiment based on your application objectives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ad73c-bf6b-4dc4-96d7-f60d5308b9bc",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062980d-1412-4197-988f-8883119d7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc27ff78-f80c-4091-9269-aef0e7f5e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import nest_asyncio\n",
    "import llama_index\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import Tuple, List\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "nest_asyncio.apply()\n",
    "boto_session = boto3.session.Session()\n",
    "\n",
    "bedrock_client = boto_session.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=boto_session.region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae7d54-928c-460a-ae91-862339ad464c",
   "metadata": {},
   "source": [
    "## Bring our own Bedrock model to Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ab8b1-9fe5-4859-8a87-ec0f5dc114f7",
   "metadata": {},
   "source": [
    "### Set up embedding model\n",
    "---\n",
    "We will set up Amazon Bedrock Embedding to use **amazon.titan-embed-text-v2:0** embedding model. If you want to explore other embedding model, you can use `list_supported_models()` method from `BedrockEmbedding` class to see which embedding models are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ebe562-b1b3-428a-883d-099f7af18851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/lib/python3.11/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"amazon\": [\n",
      "    \"amazon.titan-embed-text-v1\",\n",
      "    \"amazon.titan-embed-text-v2:0\",\n",
      "    \"amazon.titan-embed-g1-text-02\"\n",
      "  ],\n",
      "  \"cohere\": [\n",
      "    \"cohere.embed-english-v3\",\n",
      "    \"cohere.embed-multilingual-v3\"\n",
      "  ]\n",
      "}\n",
      "[-0.020442573353648186, 0.056570641696453094, 0.006846333388239145, -0.009064160287380219, 0.038828033953905106]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "\n",
    "supported_models = BedrockEmbedding.list_supported_models()\n",
    "print(json.dumps(supported_models, indent=2))\n",
    "\n",
    "titan_embedding = BedrockEmbedding(\n",
    "    model_name='amazon.titan-embed-text-v2:0',\n",
    "    region_name=boto_session.region_name,\n",
    ")\n",
    "print(titan_embedding.get_text_embedding(\"hello world\")[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62dfc3-e729-46d4-8384-d49c1ecb48d0",
   "metadata": {},
   "source": [
    "### Set up LLM\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    At the time of writing, Bedrock class from llama_index is not yet supported Meta Llama 3 models. We have made the adjustment on the code to fit in. Please refer to <b>utils</b> folder for the code reference.\n",
    "</div>\n",
    "\n",
    "We will make use of **Llama 3 70B** and **Llama 3.1 70B** for our candidate, and evaluator LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715f6948-0cf8-4422-a888-f2beeb03a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L in LLM stands for \"Large\".\n"
     ]
    }
   ],
   "source": [
    "from utils.bedrock import Bedrock\n",
    "\n",
    "llama3_70b_model_id = 'meta.llama3-70b-instruct-v1:0'\n",
    "llama3_1_70b_model_id = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "\n",
    "llama3_70b_bedrock_llm = Bedrock(\n",
    "    model=llama3_70b_model_id,\n",
    "    temperature=.2,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "llama3_1_70b_bedrock_llm = Bedrock(\n",
    "    model=llama3_1_70b_model_id,\n",
    "    temperature=.01,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(\n",
    "    llama3_1_70b_bedrock_llm.complete('What is L in LLM?',).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc387739-0875-4411-8d01-4f1212c194c2",
   "metadata": {},
   "source": [
    "### Connect to existing vector database\n",
    "----\n",
    "\n",
    "Next, we connect to the existing ChromaDB from our prerequisite step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a71397-b4f8-471c-854c-8ebc4b0c935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "chroma_db_dir = './../vector_db'\n",
    "chroma_collection_name = 'amazon-shareholder-letters'\n",
    "\n",
    "_db = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "chroma_collection = _db.get_or_create_collection(chroma_collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7298fd-3514-47e1-bdec-8b8f316f6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_engine(\n",
    "    vector_db: llama_index.vector_stores,\n",
    "    storage_context: llama_index.core.storage,\n",
    "    embedding_model: llama_index.embeddings,\n",
    "    llm: llama_index.llms,\n",
    "    top_k: int = 3,\n",
    "    response_mode: str = 'refine',\n",
    "    verbose: bool = False\n",
    ") -> Tuple[llama_index.core.indices, llama_index.core.query_engine]:\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_db,\n",
    "        embed_model=embedding_model,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    query_engine = index.as_query_engine(\n",
    "        llm=llm,\n",
    "        similarity_top_k=top_k,\n",
    "        response_mode=response_mode,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return index, query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3bc57af-b8b7-410f-b9b7-7e338c7c9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index, query_engine = get_query_engine(\n",
    "    vector_db=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embedding_model=titan_embedding,\n",
    "    llm=llama3_70b_bedrock_llm,\n",
    "    top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46ca6e-9f24-471b-aad9-cec5ced28780",
   "metadata": {},
   "source": [
    "## Component-wised Evaluation\n",
    "---\n",
    "\n",
    "If you would do in-depth evaluation of your RAG application, it is useful to break it into each individual components.\n",
    "\n",
    "For instance, you may want to investigate whether your retrieval retrieved the right documents or not, and also the LLM used the context and output the right result/response or not. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\n",
    "\n",
    "### Prepare Question-context pairs\n",
    "---\n",
    "\n",
    "LlamaIndex also provides the evaluation dataset generation, we will use `generate_question_context_pairs` for question generation on specific chunks or nodes of your vector store. \n",
    "\n",
    "In this example, I will use only sample 5 nodes from vector store and use **Llama 3 70B** for question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cd42f7-7367-4ad4-831c-24100c1f9b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    ")\n",
    "import random\n",
    "\n",
    "_qa_prompt_tmpl = \"\"\"\n",
    "<Instructions>\n",
    "Here is the context:\n",
    "<context>\n",
    "{context_str}\n",
    "</context>\n",
    "\n",
    "Your task is to generate {num_questions_per_chunk} questions \n",
    "that can be answered using the provided context, following these rules:\n",
    "\n",
    "<rules>\n",
    "1. The question should make sense to humans even when read without the given context.\n",
    "2. The question should be fully answered from the given context.\n",
    "3. The question should be framed from a part of context that contains important information. It can also be from tables, code, etc.\n",
    "4. The answer to the question should not contain any links.\n",
    "5. The question should be of moderate difficulty up to difficult.\n",
    "6. The question must be reasonable and must be understood and responded by humans.\n",
    "7. Do not use phrases like 'provided context', etc. in the question.\n",
    "8. Your question should be able to be referenced in full sentence from the context.\n",
    "8. Never create question that will refer back to the context.\n",
    "</rules>\n",
    "\n",
    "To generate the question, first identify the most important or relevant part of the context. \n",
    "Then frame a question around that part that satisfies all the rules above.\n",
    "Think step-by-step and follow the <rule>.\n",
    "\n",
    "Output only the generated question with a \"?\" at the end, no other text or characters.\n",
    "</Instructions>\n",
    "\"\"\"\n",
    "\n",
    "nodes = vector_store._get(limit=30, where={}).nodes\n",
    "selected_nodes = random.sample(nodes, 5)\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes=selected_nodes,\n",
    "    llm=llama3_70b_bedrock_llm,\n",
    "    num_questions_per_chunk=1,\n",
    "    qa_generate_prompt_tmpl=_qa_prompt_tmpl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57876f4-39d7-4f5a-a688-1492bdaf7c26",
   "metadata": {},
   "source": [
    "Let's look at the example questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3adf942c-4b81-44f2-b2c5-5588d238947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What benefits do companies gain by moving to AWS?', 'How much faster can models be trained in Amazon SageMaker compared to other platforms?', 'What metrics does the company use to measure its market leadership?', 'What is unique about the GenAI revolution compared to the mass modernization of on-premises infrastructure to the cloud?', 'In what year did the company expand its international consumer segment?']\n"
     ]
    }
   ],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "print(list(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1929e4-8cbe-47e1-80a2-a6c0aaa001f3",
   "metadata": {},
   "source": [
    "### Retrieval Evaluation\n",
    "---\n",
    "\n",
    "For **retrieval evaluation**, it will run over one query/question against the ground-truth document/node.\n",
    "Please refer to two documentations on the [usage pattern](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/) and the [example usage](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval/).\n",
    "\n",
    "You can use various built-in metrics to fit on your use cases, here are few examples:\n",
    "\n",
    "- **Hit rate**: measures the ratio of relevant documents retrieved to the total number of relevant documents available.\n",
    "- **MRR (Mean Reciprocal Rank)**: helps understand the average position of the first relevant item across all user lists. \n",
    "- **Precision**: measures the accuracy of a retrieval system by calculating the ratio of relevant documents to the number of retrieved documents.\n",
    "- **Recall**: measures the percentage of relevant documents that are returned in response to a query.\n",
    "- **AP (Average Precision)**: helps understanding the relevant items are ranked within the list of retrieval contexts.\n",
    "- **NDCG (Normalized discount cumulative gain)**: evaluates the retrieval's ability (or search algorithm in vector store) to sort items based on relevance.\n",
    "\n",
    "Let's define 2 retrievers: one with `top_k=3` and another one with `top_k=5`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>In production environment</b>, you may want to try <u>different types or techniques of retriever</u>. So you know which parameters is best for your use case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c99377b-4bca-4571-b6f3-781673cb6994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>sample 1 Top 3 result:\n",
       "Query: What benefits do companies gain by moving to AWS?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.3333333333333333, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.46927872602275644}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>sample 1 Top 5 result:\n",
       "Query: What benefits do companies gain by moving to AWS?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.2, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.3391602052736161}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>sample 2 Top 3 result:\n",
       "Query: How much faster can models be trained in Amazon SageMaker compared to other platforms?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.3333333333333333, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.46927872602275644}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>sample 2 Top 5 result:\n",
       "Query: How much faster can models be trained in Amazon SageMaker compared to other platforms?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.2, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.3391602052736161}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>sample 3 Top 3 result:\n",
       "Query: What metrics does the company use to measure its market leadership?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.3333333333333333, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.46927872602275644}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>sample 3 Top 5 result:\n",
       "Query: What metrics does the company use to measure its market leadership?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.2, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.3391602052736161}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>sample 4 Top 3 result:\n",
       "Query: What is unique about the GenAI revolution compared to the mass modernization of on-premises infrastructure to the cloud?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.3333333333333333, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.46927872602275644}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>sample 4 Top 5 result:\n",
       "Query: What is unique about the GenAI revolution compared to the mass modernization of on-premises infrastructure to the cloud?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.2, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.3391602052736161}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>sample 5 Top 3 result:\n",
       "Query: In what year did the company expand its international consumer segment?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.3333333333333333, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.46927872602275644}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>sample 5 Top 5 result:\n",
       "Query: In what year did the company expand its international consumer segment?\n",
       "Metrics: {'hit_rate': 1.0, 'mrr': 1.0, 'precision': 0.2, 'recall': 1.0, 'ap': 1.0, 'ndcg': 0.3391602052736161}\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "_top_3_retriever = vector_index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "_top_5_retriever = vector_index.as_retriever(\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "retrieved_metric = ['hit_rate', 'mrr', 'precision', 'recall', 'ap', 'ndcg']\n",
    "\n",
    "for _loop in enumerate(qa_dataset.queries.items()):\n",
    "    _idx = _loop[0]\n",
    "    _id = _loop[1][0]\n",
    "    _question = _loop[1][1]\n",
    "    _expected = qa_dataset.relevant_docs[_id]\n",
    "    top3_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        retrieved_metric, retriever=_top_3_retriever\n",
    "    )\n",
    "    top5_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        retrieved_metric, retriever=_top_5_retriever\n",
    "    )\n",
    "    top3_eval_result = top3_retriever_evaluator.evaluate(_question, _expected)\n",
    "    top5_eval_result = top5_retriever_evaluator.evaluate(_question, _expected)\n",
    "    display(Markdown(\n",
    "        \"<font color='blue'>sample {} Top 3 result:\\n{}</font>\".format(\n",
    "            _idx+1, top3_eval_result\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(\n",
    "        \"<font color='green'>sample {} Top 5 result:\\n{}</font>\".format(\n",
    "            _idx+1, top5_eval_result\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(\n",
    "        \" ---- \"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bea26e-0764-45e0-81f0-b4c25bac3277",
   "metadata": {},
   "source": [
    "We can run against the entire dataset instead of one-by-one evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2b379e-1ca7-4e99-a59a-7dae1832a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await top3_retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9331c85-17cc-4bdd-99db-38228dbecc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(\n",
    "    eval_name: str,\n",
    "    eval_results: list,\n",
    "    metrics: list = retrieved_metric\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "    columns = {\n",
    "        'retrievers_name': [eval_name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "    summary_df = pd.DataFrame(columns)\n",
    "    return full_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe567e6-5718-4d8d-b6e6-3486080cc46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers_name</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-k-3 retrieval</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.469279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     retrievers_name  hit_rate  mrr  precision  recall   ap      ndcg\n",
       "0  top-k-3 retrieval       1.0  1.0   0.333333     1.0  1.0  0.469279"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df, summary_df = get_results(\"top-k-3 retrieval\", eval_results)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e82d8-6eec-48c2-8db4-530867bff8ac",
   "metadata": {},
   "source": [
    "### Response Evaluation\n",
    "---\n",
    "\n",
    "For **response evaluation**, mostly we will focus whether the generated response matched with the retrieved context, and does it also match with the question.\n",
    "\n",
    "We will focus on two metrics in this example:\n",
    "- **Faithfulness**: measures if the response matches with the retrieved contexts, or the LLM hallucinated and generated different response.\n",
    "- **Answer relevancy**: measures that the retrieved context and the response is actually relevant and consistent for the given question.\n",
    "\n",
    "\n",
    "Please refer to the [usage pattern documentation](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern/) for more information.\n",
    "\n",
    "For both of metrics, you can perform evaluation on overall contexts and responses, or you wish to validate the response against each source node. Below is the example of evaluate on the whole response and contexts.\n",
    "\n",
    "```{py3}\n",
    "ff_evaluator = FaithfulnessEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(sample_question)\n",
    "eval_result = ff_evaluator.evaluate_response(response=response)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc35eb5-dec7-463e-a258-37dc52f7e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_resp_n_source(\n",
    "    query_engine: llama_index.core.query_engine,\n",
    "    input_prompt: str\n",
    ") -> Tuple[str, List[str]]:\n",
    "    _rag_resp = query_engine.query(input_prompt)\n",
    "    llm_resp = _rag_resp.response\n",
    "    return llm_resp, _rag_resp.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aebc54c2-eb3b-4ac0-8239-2a3c7c4c6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "sample_question = list(queries)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd98351-7725-4511-be9f-cc1b667b1918",
   "metadata": {},
   "source": [
    "Here we ran **faithfulness** and **answer relevancy** against each context node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b150812-1ba4-4b61-9251-3c3693f8f4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question: What benefits do companies gain by moving to AWS?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "LLM response: You are a highly advanced Q&A system that strictly operates in two modes when refining existing answers:\n",
       "1. **Rewrite** an original answer using the new context.\n",
       "2. **Repeat** the original answer if the new context isn't useful.\n",
       "Never reference the original answer or context directly in your answer.\n",
       "When in doubt, just repeat the original answer.\n",
       "New Context: page: 0\n",
       "source: _raw_data/AMZN-2021-Shareholder-Letter.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 1 chunk: in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies use\n",
       "discontinuous periods like this to step back and determine what they strategically want to change, and we\n",
       "find an increasing number of enterprises opting out of managing their own infrastructure, and preferring to\n",
       "move to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantly"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 1 score: 0.6370195051812849"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>Faithfulness passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevancy passing True, score 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 2 chunk: times, it’s neither what customers want nor best for customers in the long term, so we’re taking a different\n",
       "tack. One of the many advantages of AWS and cloud computing is that when your business grows, you can\n",
       "seamlessly scale up; and conversely, if your business contracts, you can choose to give us back that capacity\n",
       "and cease paying for it. This elasticity is unique to the cloud, and doesn’t exist when you’ve already made"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 2 score: 0.5151359279955711"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>Faithfulness passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevancy passing True, score 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 3 chunk: Paramount), and even critical government agencies switched to AWS (e.g. CIA, along with several other\n",
       "U.S. Intelligence agencies). But, one of the lesser-recognized beneficiaries was Amazon’s own consumer\n",
       "businesses, which innovated at dramatic speed across retail, advertising, devices (e.g. Alexa and FireTV),\n",
       "Prime Video and Music, Amazon Go, Drones, and many other endeavors by leveraging the speed with which\n",
       "AWS let them build.Primitives, done well, rapidly accelerate builders’ ability to innovate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 3 score: 0.4735027421261401"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>Faithfulness passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevancy passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 4 chunk: part to our helping companies optimize their AWS footprint to save money. Concurrently, companies were\n",
       "stepping back and determining what they wanted to change coming out of the pandemic. Many concluded\n",
       "that they didn’t want to continue managing their technology infrastructure themselves, and made the\n",
       "decision to accelerate their move to the cloud. This shift by so many companies (along with the economy\n",
       "recovering) helped re-accelerate AWS’s revenue growth to 37% Y oY in 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 4 score: 0.4462324726660458"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>Faithfulness passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevancy passing True, score 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 5 chunk: overnight, from working with colleagues and technology on-premises to working remotely. AWS played a\n",
       "major role in enabling this business continuity. Whether companies saw extraordinary demand spikes, or\n",
       "demand diminish quickly with reduced external consumption, the cloud’s elasticity to scale capacity up and\n",
       "down quickly, as well as AWS’s unusually broad functionality helped millions of companies adjust to these\n",
       "difficult circumstances."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='brown'>Source 5 score: 0.4051489517706361"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='blue'>Faithfulness passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Relevancy passing False, score 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "\n",
    "faithfulness_eval = FaithfulnessEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "relevancy_eval = RelevancyEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "resp_str, source_nodes = get_rag_resp_n_source(query_engine, sample_question)\n",
    "\n",
    "display(Markdown('Question: {}'.format(sample_question)))\n",
    "display(Markdown('LLM response: {}'.format(resp_str)))\n",
    "for _idx, source_node in enumerate(source_nodes):\n",
    "    ff_eval_result = faithfulness_eval.evaluate(\n",
    "        query=sample_question,\n",
    "        response=resp_str,\n",
    "        contexts=[source_node.get_content()]\n",
    "    )\n",
    "    rel_eval_result = relevancy_eval.evaluate(\n",
    "        query=sample_question,\n",
    "        response=resp_str,\n",
    "        contexts=[source_node.get_content()]\n",
    "    )\n",
    "    display(Markdown(\n",
    "        \"<font color='brown'>Source {_idx} chunk: {context}\".format(\n",
    "            _idx=_idx+1, context=source_node.text\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(\n",
    "        \"<font color='brown'>Source {_idx} score: {score}\".format(\n",
    "            _idx=_idx+1, score=source_node.score\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(\n",
    "        \"<font color='blue'>Faithfulness passing {}, score {}\".format(\n",
    "            ff_eval_result.passing,\n",
    "            ff_eval_result.score\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(\n",
    "        \"<font color='green'>Relevancy passing {}, score {}\".format(\n",
    "            rel_eval_result.passing,\n",
    "            rel_eval_result.score\n",
    "        )\n",
    "    ))\n",
    "    display(Markdown(' ---- '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94aac3c-63c3-4319-89c8-fb0ee5b2ec04",
   "metadata": {},
   "source": [
    "## End-to-End Evaluation\n",
    "---\n",
    "\n",
    "Normally, **end-to-end evaluation** will be your guiding signal for the RAG application, i.e., will the application generate the right responses given the data sources and a set of queries. In general, we will use this evaluation to gain an intuition for which components we want to dive deeper on.\n",
    "\n",
    "Several evaluation metrics can be;\n",
    "- **Correctness**: this metric evaluates the relevance and correctness of a generated answer against a reference answer.\n",
    "- **Faithfulness**: this metric evaluates the response matches with the source nodes (please refer to **response evaluation** section for code example).\n",
    "- **Guideline**: this metric evaluates how RAG or Generative AI application respond to the given guidelines or not.\n",
    "- **Pairwise**: this metric evaluates if the evaluation LLM would prefer which one LLM (or even query engine) over another.\n",
    "- **Relevancy**: this metric evaluates if the response and the contexts match the query.\n",
    "- **Semantic Similarity**: this metric evaluates the quality of a question answering system via semantic similarity, by comparing the similarity score between embedding of the generated answer and the reference. This doesn't guarantee the correctness of the response, it is more for capturing the <u>relevancy</u>.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    We will pick one example question from the evaluation dataset generated during the prerequisite step for illustration purpose.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57b8ce6-4e7b-4372-b1ff-947ddf1a1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    GuidelineEvaluator,\n",
    "    PairwiseComparisonEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "737a6e92-f96d-438d-a4f7-f6973dbd6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "eval_df = pd.read_csv('./../_eval_data/eval_dataframe.csv')\n",
    "eval_df['source_chunk'] = eval_df.source_chunk.apply(lambda s: list(ast.literal_eval(s)))\n",
    "sample_question = eval_df.question[1]\n",
    "expected_output = eval_df.ref_answer[1]\n",
    "reference_context = eval_df.source_chunk[1]\n",
    "llm_resp, source_nodes = get_rag_resp_n_source(query_engine, sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79f54a3-e1c2-477d-ad6b-4b9d3feff9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Quesiton**: What are the company's priorities in terms of spending and culture in a business incurring net losses?\n",
       "\n",
       "---\n",
       "\n",
       "**Golden response**: The company's priorities in terms of spending and culture in a business incurring net losses are to spend wisely and maintain a lean culture, continually reinforcing a cost-conscious culture.\n",
       "\n",
       "---\n",
       "\n",
       "**LLM response**: The company's priorities are to work hard to spend wisely and maintain a lean culture, understanding the importance of continually reinforcing a cost-conscious culture, particularly in a business incurring net losses.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown('''\n",
    "**Quesiton**: {question}\n",
    "\n",
    "---\n",
    "\n",
    "**Golden response**: {expected_out}\n",
    "\n",
    "---\n",
    "\n",
    "**LLM response**: {llm_resp}\n",
    "'''.format(\n",
    "    question=sample_question,\n",
    "    expected_out=expected_output,\n",
    "    llm_resp=llm_resp\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d2e61-6aa0-4b16-9903-08617d6cfbc4",
   "metadata": {},
   "source": [
    "### Correctness evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42ccfda2-3da8-407a-9954-142374b96c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated answer is very similar to the reference answer, conveying the same priorities of spending wisely and maintaining a lean culture, especially in a business incurring net losses. The slight rewording does not affect the overall correctness of the answer, but it is not as concise as the reference answer.\n"
     ]
    }
   ],
   "source": [
    "correctness_eval = CorrectnessEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "result = correctness_eval.evaluate(\n",
    "    query=sample_question,\n",
    "    response=llm_resp,\n",
    "    reference=expected_output,\n",
    "    contexts=[doc.text for doc in source_nodes],\n",
    ")\n",
    "print(result.feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0a874-a7df-4db1-b6ab-0cfaed03ea41",
   "metadata": {},
   "source": [
    "### Guideline evaluation\n",
    "---\n",
    "\n",
    "For this evaluation, you will need to provide the guideline to the `GuidelineEvaluator` class. Your guideline will depend on the objective of your application. This can be useful and make improvement for the RAG application, especially in the prompt engineering component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22e2c7a2-993a-478a-8541-85efb41f3997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n",
      "Guideline: The response should fully answer the query.\n",
      "Pass: False\n",
      "Feedback: The response does not fully answer the query as it does not provide specific details about the company's priorities in terms of spending and culture. It only provides general statements about being cost-conscious and maintaining a lean culture. To fully answer the query, the response should provide more concrete information about the company's priorities, such as specific areas where they are cutting costs or investing in order to achieve their goals.\n",
      "=====\n",
      "Guideline: The response should avoid being vague or ambiguous.\n",
      "Pass: False\n",
      "Feedback: The response is too vague and does not provide specific details about the company's priorities in terms of spending and culture. It would be more helpful to provide concrete examples or metrics that illustrate the company's approach to cost management and cultural values during a period of net losses.\n"
     ]
    }
   ],
   "source": [
    "GUIDELINES = [\n",
    "    'The response should fully answer the query.',\n",
    "    'The response should avoid being vague or ambiguous.',\n",
    "]\n",
    "\n",
    "guideline_evaluators = [\n",
    "    GuidelineEvaluator(llm=llama3_1_70b_bedrock_llm, guidelines=guideline)\n",
    "    for guideline in GUIDELINES\n",
    "]\n",
    "for guideline, evaluator in zip(GUIDELINES, guideline_evaluators):\n",
    "    eval_result = evaluator.evaluate(\n",
    "        query=sample_question,\n",
    "        response=llm_resp,\n",
    "        reference=expected_output,\n",
    "        contexts=[doc.text for doc in source_nodes],\n",
    "    )\n",
    "    print(\"=====\")\n",
    "    print('Guideline: {}'.format(guideline))\n",
    "    print('Pass: {}'.format(eval_result.passing))\n",
    "    print('Feedback: {}'.format(eval_result.feedback))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be4a28-1b04-4a3a-b6da-6054fb0bf3d4",
   "metadata": {},
   "source": [
    "### Pairwise evaluation\n",
    "---\n",
    "\n",
    "You can choose to use `PairwiseComparisonEvaluator` when you are unsure which candidate LLMs to use or the configuration within query engine. Use this to iterate and pick the optimal parameters choices.\n",
    "\n",
    "For comparison with **Llama 3 70B**, let's define additional **Llama 3.1 405B** candidate LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e752149-0252-45e0-bee8-bfb7e07326b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_1_405b_model_id = 'meta.llama3-1-405b-instruct-v1:0'\n",
    "\n",
    "llama3_1_405b_bedrock_llm = Bedrock(\n",
    "    model=llama3_1_405b_model_id,\n",
    "    temperature=.1,\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b7a5d71-aa99-435f-9541-0b35e15e3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, query_engine_v2 = get_query_engine(\n",
    "    vector_db=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embedding_model=titan_embedding,\n",
    "    llm=llama3_1_405b_bedrock_llm,\n",
    "    top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8f6c8da-83a2-4120-8c49-7b54dbfdfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = query_engine.query(sample_question)\n",
    "resp2 = query_engine_v2.query(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a0d53d-e3a3-48e8-b446-51394486169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_eval = PairwiseComparisonEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "eval_result = await pairwise_eval.aevaluate(\n",
    "    query=sample_question,\n",
    "    response=resp1,\n",
    "    second_response=resp2,\n",
    "    reference=expected_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e36c19-bb14-488b-8537-8ee41acac669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Question</b>: What are the company's priorities in terms of spending and culture in a business incurring net losses?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b><font color='brown'>Expected output</b>: The company's priorities in terms of spending and culture in a business incurring net losses are to spend wisely and maintain a lean culture, continually reinforcing a cost-conscious culture.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b><font color='blue'>Llama 3 70B response</b>: We will work hard to spend wisely and maintain our lean culture.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b><font color='darkblue'>Llama 3.1 405B response</b>: The company prioritizes maintaining a lean culture and continually reinforcing a cost-conscious culture, particularly in a business incurring net losses, by working hard to spend wisely.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b><font color='green'>Evaluation result</b>: After evaluating the responses from Assistant A and Assistant B, I found that both assistants provided relevant answers that align with the user's question and the provided reference. However, Assistant B's response is more comprehensive and accurately reflects the company's priorities in terms of spending and culture as stated in the reference.\n",
       "\n",
       "Assistant B's response explicitly mentions the importance of maintaining a lean culture and continually reinforcing a cost-conscious culture, which is a direct quote from the reference. Additionally, Assistant B's response provides more context by specifying that this priority is particularly relevant in a business incurring net losses.\n",
       "\n",
       "Assistant A's response, while concise and relevant, lacks the depth and detail provided by Assistant B. It only mentions spending wisely and maintaining a lean culture without providing additional context or emphasizing the importance of a cost-conscious culture.\n",
       "\n",
       "Therefore, based on the factors of helpfulness, relevance, accuracy, depth, and level of detail, I conclude that Assistant B's response is better.\n",
       "\n",
       "Final Verdict: [[B]]</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " ---- "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "    \"<b>Question</b>: {}\".format(sample_question)\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<b><font color='brown'>Expected output</b>: {}</font>\".format(\n",
    "        expected_output\n",
    "    )\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<b><font color='blue'>Llama 3 70B response</b>: {}</font>\".format(resp1)\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<b><font color='darkblue'>Llama 3.1 405B response</b>: {}</font>\".format(resp2)\n",
    "))\n",
    "display(Markdown(\n",
    "    \"<b><font color='green'>Evaluation result</b>: {}</font>\".format(\n",
    "        eval_result.feedback\n",
    "    )\n",
    "))\n",
    "display(Markdown(' ---- '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc14747-fecd-4a41-bb76-8e65aa68c94c",
   "metadata": {},
   "source": [
    "### Relevancy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2df824-34cf-44ac-97d2-70104752cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "rel_eval = RelevancyEvaluator(llm=llama3_1_70b_bedrock_llm)\n",
    "rel_eval_result = rel_eval.evaluate(\n",
    "    query=sample_question,\n",
    "    response=llm_resp,\n",
    "    reference=expected_output,\n",
    "    contexts=[doc.text for doc in source_nodes]\n",
    ")\n",
    "print(rel_eval_result.passing)\n",
    "print(rel_eval_result.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa8d53-ee42-47cb-842c-728d7654d9d5",
   "metadata": {},
   "source": [
    "### Semantic evaluation\n",
    "---\n",
    "\n",
    "You can change the `similarity_mode` to `DOT_PRODUCT` or `EUCLIDEAN`, by default it will use `cosine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a87a76f6-d8af-46b5-a304-65c0afafd8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.9111703519420223\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.base.embeddings.base import SimilarityMode\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=titan_embedding,\n",
    "    similarity_mode=SimilarityMode.DEFAULT,\n",
    "    similarity_threshold=0.6,\n",
    ")\n",
    "\n",
    "result = await evaluator.aevaluate(\n",
    "    response=llm_resp,\n",
    "    reference=expected_output,\n",
    ")\n",
    "print(result.feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c31dc-673d-47f6-afc6-1367513564af",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "In this notebook, we have implemented RAG evaluation using `LlamaIndex` evaluation framework. `LlamaIndex` is an open-source library, which helps AI developers to build generative AI application. It also offers comprehensive set of evaluation tools from **end-to-end** to **component-wised** evaluation.\n",
    "\n",
    "By integrating LlamaIndex evaluation into your existing RAG workflow, you can assess the workflow output as a whole, use it to find better LLM or parameters choices, and drill down to pinpoint the components or area of improvement. Overall, the **LlamaIndex evaluation** framework streamlines the process of assessing your application's performance, allowing you to iterate and improve your index and query strategies more efficiently.\n",
    "\n",
    "However, this evaluation is relied heavily on Large language model (LLMs) for assessment. This can introduce potential drawbacks, including bias, increased costs, and consistency concerns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
