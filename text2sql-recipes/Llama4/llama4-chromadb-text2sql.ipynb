{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4901509a-8316-47a3-879b-544a43b37ef3",
   "metadata": {},
   "source": [
    "# Text-to-SQL with Llama 4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa596a49-ce47-4053-a93d-699bdef52426",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b488b5-d05b-45d2-84fb-6bc80cc30241",
   "metadata": {},
   "source": [
    "This notebook introduces a versatile approach that leverages Llama 4 on Amazon SageMaker JumpStart, including one-shot example prompt engineering, to convert natural language questions into executable SQL queries. Our approach generates SQL queries capable of joining data from tables across multiple databases, enabling information retrieval from complex database structures. This multi-database capability is crucial in real-world scenarios where data is often distributed across various tables with intricate relationships, and queries need to combine information from multiple sources to provide comprehensive insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf152e5-95fe-44fa-836c-5384afd3a1c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Llama 4 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126d29c-4748-4831-8aff-60d71c61462e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**While there are 2 LLama 4 models (Scout and Maverick), today we will focus on the Scout model which can be run on a single-node GPU instance.**\n",
    "\n",
    "#### Llama 4 Scout (text + image input)\n",
    "The lighter model in the Llama 4 collection, perfect for applications requiring efficient processing while maintaining high performance. With 17B active parameters (109B total across experts), Scout can run on a single GPU and handles context lengths up to 10M tokens. This model is ideal for:\n",
    "- Personal information management\n",
    "- Multilingual knowledge retrieval (supports 12 languages)\n",
    "- Mobile AI-powered writing assistants\n",
    "- Customer service applications\n",
    "- Text summarization and classification tasks\n",
    "\n",
    "#### Llama 4 Maverick (text + image input)\n",
    "Meta's most advanced model, designed for enterprise-level applications with 17B active parameters (400B total across experts). With its 128 experts architecture, Maverick excels at:\n",
    "- General knowledge and reasoning\n",
    "- Long-form text generation\n",
    "- Multilingual translation across 12 languages\n",
    "- Coding and mathematical tasks\n",
    "- Advanced reasoning and complex problem-solving\n",
    "- Enterprise applications requiring sophisticated visual reasoning\n",
    "\n",
    "Both models feature:\n",
    "- Multimodal capabilities (text + up to 5 images input)\n",
    "- Support for Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese\n",
    "- Knowledge cutoff of August 2024\n",
    "- Optimization for tool-calling and powering agentic systems\n",
    "\n",
    "For more information, refer to the following link:\n",
    "\n",
    "1. [Llama 4 Model Cards on GitHub](https://github.com/meta-llama/llama)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31897e0-201b-4cc2-ac5b-f517c2d29f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Approach to the Text-to-SQL Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd85684",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook covers the following approaches\n",
    "\n",
    "### Few-shot text-to-SQL powered by ChromaDB (Schema Retrieval vs Enhance Schema Retrieval with Sample Questions)\n",
    "\n",
    "This approach leverages ChromaDB, a vector database, to assist the few-shot text-to-SQL translation process. ChromaDB stores the database schema information, which includes table names, column names, and their descriptions. When a natural language question is provided, the model can retrieve relevant schema information from ChromaDB to aid in generating the SQL query. ChromaDB can be used by leveraging our Embeddings model:\n",
    "\n",
    "In this method, we show the option of using HuggingFace BGE Large EN Embedding model on Amazon SageMaker JumpStart to grab the embeddings from the vector store. \n",
    "\n",
    "By leveraging ChromaDB, the few-shot text-to-SQL translation process can benefit from efficient schema and sample data retrieval, potentially leading to better performance and generalization across different databases and query types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf319c-23af-4108-8372-0e464c1f3127",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dceec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This notebook will provide code snippets to assist with implementing two differents approaches to converting a natural language question into a SQL query. The query will be executed in the database to answer the original question.\n",
    "\n",
    "We will be leveraging SageMaker JumpStart to deploy our Llama 4 and embeddings model to build this text2sql solution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dca637-1d1a-445d-a943-d2448fd39ac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b33e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. [Getting Started](#Getting-Started)\n",
    "    + [Install Dependencies](#Step-0:-Install-Dependencies)\n",
    "    + [Select Model Hosting Service](#Step-1:-Select-Hosting-Model-Service)\n",
    "    + [Get Database and Schema details](Step-2:-Get-Database-and-Schema-details)\n",
    "    + [Create helper functions](Step-3:-Create-helper-functions)\n",
    "1. [Few-shot text-to-SQL powered by ChromaDB](#Few-shot-text-to-SQL-powered-by-ChromaDB)\n",
    "    + [Schema Retrieval](#Schema-Retrieval)\n",
    "    + [Data Preprocessing](#Step-1:-Data-Preprocessing)\n",
    "    + [Ingest docs into ChromaDB](#Step-2:-Ingest-docs-into-ChromaDB)\n",
    "    + [Create a Few-Shot Prompt](#Step-3:-Create-a-Few-Shot-Prompt)\n",
    "    + [Execute Few-Shot Prompts](#Step-4:-Execute-Few-Shot-Prompts)\n",
    "    + [Conclusion](#Step-5-Conclusion)\n",
    "1. [Enhanced Schema Retrieval with ChromaDB and an Embedding Model](#Enhanced-Schema-Retrieval-with-an-Embedding-Model)\n",
    "    + [Ingest docs into ChromaDB](#Step-1:-Ingest-docs-into-ChromaDB)\n",
    "    + [Execute Few-Shot Prompts](#Step-2:-Execute-Few-Shot-Prompts)\n",
    "    + [Conclusion](#Step-3-Conclusion)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a20c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tools\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. In this notebook, LangChain is used to build a prompt template.\n",
    "\n",
    "+ [ChromaDB](https://www.trychroma.com/) is a vector database that enables efficient semantic search, storage, and retrieval of unstructured data like text, images, and audio. It's designed to work well with large language models (LLMs) and provides a simple and scalable way to build applications that can search and retrieve relevant information from vast amounts of data.\n",
    "\n",
    "+ RDS (Relational Database Service) for [MySQL](https://aws.amazon.com/rds/mysql/) is a managed database service provided by Amazon Web Services (AWS). RDS for MySQL simplifies the setup, operation, and scaling of MySQL databases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be5a3a-78a0-4b4f-a0aa-fc0bf0f7676b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a6bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. It is mandatory to have set up the database and sample data prior to using [this notebook](llama4-chromadb-text2sql-DB-Setup.ipynb).\n",
    "2. Use kernel either `conda_python3`, `conda_pytorch_p310` or `conda_tensorflow2_p310`.\n",
    "3. Install the required packages.\n",
    "\n",
    "**Additionally, you should add the following IAM policy to your execution role below:**\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"cloudformation:DescribeStackResources\",\n",
    "\t\t\t\t\"cloudformation:DescribeStacks\",\n",
    "\t\t\t\t\"cloudformation:ListStacks\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": \"*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae4171-4f20-4a68-b6e1-0a40a3854de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465f181-b661-4f59-bdac-92ed0e164203",
   "metadata": {},
   "source": [
    "#### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama 4 Scout: `ml.g6e.48xlarge`, `ml.p5.48xlarge`, `ml.p5en.48xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "**Note:** By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.p5.48xlarge\")`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b2317-d90d-481f-8d51-464a93650978",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd1931-c19d-4c30-bfef-19cbc59724f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 0: Install Dependencies\n",
    "\n",
    "Here, we will install all the required dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d52454-073b-4a5a-b293-924419e1c8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.32 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install mysql-connector-python==8.4.0 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install langchain==0.2.5 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install chromadb==0.5.0 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install numpy==1.26.4 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install psycopg2==2.9.9 -qU --force --quiet --no-warn-conflicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097885d-9818-4015-b273-4fe30415cdfd",
   "metadata": {},
   "source": [
    "**Note:** *When installing libraries using the pip, you may encounter errors or warnings during the installation process. These are generally not critical and can be safely ignored. However, after installing the libraries, it is recommended to restart the kernel or computing environment you are working in. Restarting the kernel ensures that the newly installed libraries are loaded properly and available for use in your code or workflow.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea1457-e5a3-46b1-870f-ba7e73662ec2",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><b>NOTE:</b> Restart the kernel with the updated packages that are installed through the dependencies above</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7518a54-885b-4403-ac4b-d5aab2ebb3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afba42-5061-4464-9ec9-8641992a7d66",
   "metadata": {},
   "source": [
    "#### Import the required modules to run the notbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756999dd-ac11-4b3a-8e43-2c4cdc2ad9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import chromadb\n",
    "from chromadb.api.types import (\n",
    "    Documents,\n",
    "    EmbeddingFunction,\n",
    "    Embeddings,\n",
    ")\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "import mysql.connector as MySQLdb\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1f1bb-966d-4726-804c-b0d1f9108d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Select Hosting Model Service\n",
    "\n",
    "Here, you can select to run this notebook using SageMaker JumpStart or Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904c9df-7245-4c3b-98af-96298a61d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "llama4_scout_id = \"meta-vlm-llama-4-scout-17b-16e-instruct\"\n",
    "DEFULT_LLM_MODEL_ID = llama4_scout_id\n",
    "    \n",
    "model = JumpStartModel(model_id=DEFULT_LLM_MODEL_ID, instance_type=\"ml.p5.48xlarge\")\n",
    "    \n",
    "llm_predictor = model.deploy(accept_eula=True)\n",
    "    \n",
    "print(f\"\\nLLM SageMaker Endpoint Name: [{llm_predictor.endpoint_name}].\\n\")\n",
    "\n",
    "# Deploy BGE Large EN embedding model on Amazon SageMaker JumpStart:\n",
    "# Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "DEFAULT_EMBEDDING_MODEL_ID = \"huggingface-sentencesimilarity-bge-large-en\"\n",
    "\n",
    "text_embedding_model = JumpStartModel(model_id=DEFAULT_EMBEDDING_MODEL_ID, instance_type=\"ml.g5.4xlarge\" )\n",
    "    \n",
    "embedding_predictor = text_embedding_model.deploy()\n",
    "    \n",
    "print(f\"\\nLLM SageMaker Endpoint Name: [{embedding_predictor.endpoint_name}].\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ef33f-7710-4d5b-aa93-19a986002a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook example uses Amazon RDS MySQL DB.\n",
    "llm_selected_db = \"mysql\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12765f-10a8-4aae-9e62-4ee7f4e526cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Get Database and Schema details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2493f9-833f-40ff-a9db-b81f47e37c66",
   "metadata": {},
   "source": [
    "Here, we retrieve the services that are already deployed as a part of the cloudformation template to be used in building the application. The services include,\n",
    "\n",
    "+ Secret ARN with RDS for MySQL Database credentials\n",
    "+ Database Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d355be7-9365-41d5-8f63-153898c7c169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stackname = \"l4-txt2sql\"  # If your stack name differs from \"text2sql\", please modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca002c7-3416-4ac5-b9a1-2b730557fb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "cfn_outputs = cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']\n",
    "\n",
    "# Get rds secret arn and database endpoint from cloudformation outputs\n",
    "for output in cfn_outputs:\n",
    "    if 'SecretArnMySQL' in output['OutputKey']:\n",
    "        mySQL_secret_id = output['OutputValue']\n",
    "\n",
    "    if 'DatabaseEndpointMySQL' in output['OutputKey']:\n",
    "        mySQL_db_host = output['OutputValue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5601b-287b-421e-8711-ef73d5c09cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "secrets_client = boto3.client('secretsmanager')\n",
    "\n",
    "# Get MySQL credentials from Secrets Manager\n",
    "credentials = json.loads(secrets_client.get_secret_value(SecretId=mySQL_secret_id)['SecretString'])\n",
    "\n",
    "# Get password and username from secrets\n",
    "mySQL_db_password = credentials['password']\n",
    "mySQL_db_user = credentials['username']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c970c8c-f566-430f-b12b-e669cc7241eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Establish the database connection (MySQL DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ab665-9b2e-4141-b7fb-df3329ac61c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mySQL_db_conn = MySQLdb.connect(\n",
    "    host=mySQL_db_host,\n",
    "    user=mySQL_db_user,\n",
    "    password=mySQL_db_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cb5ef-41ea-43d9-ace9-8ca9c2dda260",
   "metadata": {},
   "source": [
    "#### Load table schema settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579b20a-eb9c-4146-88e7-d4936d8d3038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_settings(file_path):\n",
    "    \"\"\"\n",
    "    Reads a YAML file and returns its contents as a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        obj: The contents of the YAML file as a Python object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error: Failed to parse the YAML file '{file_path}': {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf569e-6c76-4eee-8be7-cc0ab13a3802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MySQL Table Setup\n",
    "\n",
    "# Load table settings - database: healthcare_db | table_name: patients\n",
    "# Use the confirmed path that exists\n",
    "settings_patients = load_settings('./schemas/patients_ms.yml')  # This path works\n",
    "if settings_patients is not None:\n",
    "    table_patients = settings_patients['table_name']\n",
    "    table_schema_patients = settings_patients['table_schema']\n",
    "    db_patients = settings_patients['database']\n",
    "    print(f\"Successfully loaded patients settings: {db_patients}.{table_patients}\")\n",
    "else:\n",
    "    print(\"Failed to load patients settings\")\n",
    "\n",
    "# Load table settings - database: insurance_db | table_name: providers\n",
    "settings_providers = load_settings('./schemas/providers_ms.yml')  # Use same convention\n",
    "if settings_providers is not None:\n",
    "    table_providers = settings_providers['table_name']\n",
    "    table_schema_providers = settings_providers['table_schema']\n",
    "    db_providers = settings_providers['database']\n",
    "    print(f\"Successfully loaded providers settings: {db_providers}.{table_providers}\")\n",
    "else:\n",
    "    print(\"Failed to load providers settings\")\n",
    "\n",
    "# Load table settings for combined schema\n",
    "settings_patients_providers = load_settings('./schemas/patients-providers_ms.yml')  # Use same convention\n",
    "if settings_patients_providers is not None:\n",
    "    print(\"Successfully loaded combined settings\")\n",
    "else:\n",
    "    print(\"Failed to load combined settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf28a8-0d22-4223-989b-243431149e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Create helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae6aca-18fb-4beb-a6ae-caaeea512d1e",
   "metadata": {},
   "source": [
    "To facilate the usability and readability of the SQL Query Analysis made by Llama 4, let's create a suite of helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85e516-6344-41c7-90c8-0bdbd219ace8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Chat Completion (Invoke LLM and Return response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaedc7-f395-4d92-9809-8f7b71baa42f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `sagemaker_chat_completion` function uses the SageMaker Endpoint to invoke the LLMs. The response from the LLM is extracted and returned as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4863f-ff81-4f02-8c25-7f0d50f99d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sagemaker_chat_completion(\n",
    "    messages: list,\n",
    "    max_gen_len: int = 512,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a chat completion using the OpenAI Chat Completions API format.\n",
    "\n",
    "    Args:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content' keys.\n",
    "        max_gen_len (int, optional): The maximum length of the completion.\n",
    "        temperature (float, optional): Sampling temperature for the model.\n",
    "        top_p (float, optional): Top p sampling ratio for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text completion.\n",
    "    \"\"\"\n",
    "    # Create the request payload using the exact OpenAI Chat Completions format\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_gen_len,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "\n",
    "    # Call the model API to generate the completion\n",
    "    response = llm_predictor.predict(payload)\n",
    "    \n",
    "    # Check for different response formats\n",
    "    if isinstance(response, dict):\n",
    "        if 'choices' in response and len(response['choices']) > 0:\n",
    "            # Standard OpenAI format\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        elif 'generated_text' in response:\n",
    "            # Format we tried before\n",
    "            return response['generated_text'].strip()\n",
    "    \n",
    "    # If we can't extract a valid response\n",
    "    print(f\"Unexpected response format: {response}\")\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ca1db-a4aa-418e-a9aa-998e4a07b0ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Function `sagemaker_chat_completion` generates and executes an SQL query for a given question, and returns a comprehensive analyzes based on the sql query results. This uses Amazon SageMaker to invoke the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698ed84-c952-42f2-8fbc-2f73233725ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Query execution and LLM calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ed73f-1ea7-4a5f-91be-e5217701dc7c",
   "metadata": {},
   "source": [
    "The `execute_query` function will execute SQL queries, typically for retrieving data from a database, and format the results as a string for further processing or display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401ba7a-de43-410d-8015-a38a60f2b6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query(query: str, db_conn) -> str:\n",
    "    \"\"\"Execute an SQL query on the database connection and return the results as a string.\n",
    "\n",
    "    Args:\n",
    "        query (str): SQL query to execute\n",
    "        db_conn (Connection object): Connection object to the database where the query needs to be executed.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the SQL results.\n",
    "    \"\"\"\n",
    "    # Get a cursor from the database connection\n",
    "    mycursor = db_conn.cursor()\n",
    "\n",
    "    # Execute the SQL query\n",
    "    mycursor.execute(query)\n",
    "\n",
    "    # Fetch all result rows\n",
    "    result_rows = mycursor.fetchall()\n",
    "\n",
    "    # Convert result to string with newline between rows\n",
    "    output_text = '\\n'.join([str(x) for x in result_rows])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997f36-a08b-4189-ba98-a410d9063e41",
   "metadata": {},
   "source": [
    "The Function `get_llm_sql_analysis` generates and executes an SQL query for a given question, and returns a comprehensive analyzes based on the sql query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6da888-3725-4ebb-b1c5-994534cf370d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_sql_analysis(question: str, sql_sys_prompt: str, qna_sys_prompt: str, DatabaseType: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an SQL query based on the given question, executes it, and returns an analysis.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question for which an SQL query needs to be generated.\n",
    "        sql_sys_prompt (str): The prompt to be used for generating the SQL query.\n",
    "        qna_sys_prompt (str): The prompt to be used for analyzing the SQL query results.\n",
    "        DatabaseType (str): The type of database to query (mysql/postgresql).\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis of the SQL query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # *****************************************************************************************\n",
    "        # 1. Generate SQL Query\n",
    "        # *****************************************************************************************\n",
    "        print(f\"\\nUsing SageMaker to generate SQL for [{DatabaseType}]\\n\")\n",
    "        \n",
    "        # Create messages for SQL generation using OpenAI format\n",
    "        sql_messages = [\n",
    "            {\"role\": \"system\", \"content\": sql_sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        completion = sagemaker_chat_completion(messages=sql_messages)\n",
    "        print(f\"completion = \\n{completion}\\n\")\n",
    "        \n",
    "        # *****************************************************************************************\n",
    "        # 2. Extract SQL and Execute\n",
    "        # *****************************************************************************************\n",
    "        # Extract SQL query using regex patterns\n",
    "        pattern = r\"<sql>(.*)</sql>\"\n",
    "        sr = re.search(pattern, completion, re.DOTALL)\n",
    "\n",
    "        if sr is None:\n",
    "            pattern = r\"```sql(.*)```\"\n",
    "            sr = re.search(pattern, completion, re.DOTALL)\n",
    "            \n",
    "        if sr is None:\n",
    "            raise ValueError(\"Could not extract SQL query from completion\")\n",
    "\n",
    "        llm_sql_query = sr.group(1).strip()\n",
    "        print(f\"\\nLLM SQL Query: \\n{llm_sql_query}\")\n",
    "    \n",
    "        # Route to appropriate database connection\n",
    "        match DatabaseType:\n",
    "            case \"mysql\":\n",
    "                db_conn = mySQL_db_conn\n",
    "            case \"postgresql\":\n",
    "                db_conn = pg_db_conn\n",
    "\n",
    "        # Execute SQL query\n",
    "        sql_results = execute_query(llm_sql_query, db_conn)\n",
    "        print(f\"\\nsql_results = \\n{sql_results}\")\n",
    "\n",
    "        # *****************************************************************************************\n",
    "        # 3. Evaluate the response\n",
    "        # *****************************************************************************************\n",
    "        print(f\"\\nCalling LLM on SageMaker to analyze the results\\n\")\n",
    "        \n",
    "        # Create messages for analysis using OpenAI format\n",
    "        analysis_prompt = qna_sys_prompt.format(query_results=sql_results, question=question)\n",
    "        analysis_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes SQL query results.\"},\n",
    "            {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "        ]\n",
    "        \n",
    "        llm_sql_analysis = sagemaker_chat_completion(messages=analysis_messages)\n",
    "        print(f\"\\nLLM SQL Analysis: \\n{llm_sql_analysis}\")\n",
    "\n",
    "        return llm_sql_analysis\n",
    "    except Exception as e:\n",
    "        print(f\"\\nException Encountered: \\n{e}\\n\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af419b-9bdd-4899-a187-c5865f71a9f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Embedding Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb825ee-8996-433b-b879-0f2581055ec4",
   "metadata": {},
   "source": [
    "The Class `AmazonSageMakerEmbeddingFunction` initializes an embedding function with `Amazon SageMaker BGE Large` from JumpStart that integrates with ChromaDB . This class can be further extended to add support for other embedding models available on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47479bba-58f5-4906-89d7-9eb688588192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AmazonSageMakerEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Initialize AmazonSageMakerEmbeddingFunction.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional arguments to pass to the sagemaker embedding function.\n",
    "\n",
    "        Example:\n",
    "            >>> sagemaker = AmazonBedrockEmbeddingFunction()\n",
    "            >>> text_inputs = [\"Hello, world!\", \"How are you?\"]\n",
    "            >>> embeddings = sagemaker(texts)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            input_body = {\"text_inputs\": text, \"mode\": \"embedding\"}\n",
    "            body = json.dumps(input_body).encode('utf-8')\n",
    "            response = embedding_predictor.predict(\n",
    "                body,\n",
    "                {\n",
    "                    \"ContentType\": content_type,\n",
    "                    \"Accept\": accept,\n",
    "                }\n",
    "            )\n",
    "            embedding = response.get(\"embedding\")\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366f9d5-f0ca-4008-a8be-04ab47b22779",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Few-shot text-to-SQL powered by ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73cdbb-e062-4d04-981d-ae92ec7e71fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use ChromaDB and the few-shot technique to retrieve table schemas for better performance and generalization across different databases and query types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765c200-7f94-4044-97ca-4681d5058d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schema Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf41750-066b-40f5-a4f2-407acec7da60",
   "metadata": {},
   "source": [
    "In this approach, we will store only the table schemas in ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70751dc4-6b3d-45b8-838d-908cd26684ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Data Preprocessing\n",
    "\n",
    "The first step is to preprocess the data and create a document that will be ingested into ChromaDB. The final doc clearly separates the table schemas by using XML tags such as `<table_schema></table_schema>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8888c-2a37-4a0c-8949-baaa24815ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For MySQL Healthcare Database\n",
    "# The doc includes a structure format for clearly identifying the table schemas\n",
    "doc1 = \"<table_schemas>\\n\"\n",
    "doc1 += f\"<table_schema>\\n {table_schema_patients} \\n</table_schema>\\n\".strip()\n",
    "doc1 += \"\\n</table_schemas>\"\n",
    "\n",
    "# The doc includes a structure format for clearly identifying the table schemas\n",
    "doc2 = \"<table_schemas>\\n\"\n",
    "doc2 += f\"<table_schema>\\n {table_schema_providers} \\n</table_schema>\\n\".strip()\n",
    "doc2 += \"\\n</table_schemas>\"\n",
    "\n",
    "# The doc includes a structure format for clearly identifying the combined table schemas\n",
    "doc3 = \"<table_schemas>\\n\"\n",
    "if settings_patients_providers is not None and 'table_schemas' in settings_patients_providers:\n",
    "    for table_schema in settings_patients_providers['table_schemas']:\n",
    "        doc3 += f\"<table_schema>\\n {table_schema} \\n</table_schema>\\n\"\n",
    "else:\n",
    "    # If the combined schema doesn't have a table_schemas list, include both individual schemas\n",
    "    doc3 += f\"<table_schema>\\n {table_schema_patients} \\n</table_schema>\\n\"\n",
    "    doc3 += f\"<table_schema>\\n {table_schema_providers} \\n</table_schema>\\n\"\n",
    "doc3 += \"\\n</table_schemas>\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad09a7-e39e-48fe-a4af-e4e573aa8d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Ingest docs into ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b088a-846e-4ca2-842f-5ff1c6897b26",
   "metadata": {},
   "source": [
    "After the data is preprocessed, the next step is to ingest all `docs` into ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea1d14-515d-483c-83a5-ac7a5d0f0c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = None\n",
    "\n",
    "# Setup Chroma in-memory, for easy prototyping.\n",
    "chroma_client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7de61-a7f8-4ebb-94a2-9f2df9060309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete collection if exists\n",
    "try:\n",
    "    chroma_client.get_collection(name=\"table-schemas-default-embedding\")\n",
    "except ValueError:\n",
    "    # Collection does not exist\n",
    "    pass\n",
    "else:\n",
    "    chroma_client.delete_collection(name=\"table-schemas-default-embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619eb8a2-dc67-42ec-8156-408117e8253d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For MySQL Healthcare Database\n",
    "# Create collection using ChromaDB's internal embedding function\n",
    "collection1 = chroma_client.get_or_create_collection(name=\"healthcare-table-schemas-default-embedding\", \n",
    "                                                     metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Add docs to the collection.\n",
    "collection1.add(\n",
    "    documents=[\n",
    "        doc1,\n",
    "        doc2,\n",
    "        doc3\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"mysql\", \"database\": db_patients, \"table_name\": table_patients},\n",
    "        {\"source\": \"mysql\", \"database\": db_providers, \"table_name\": table_providers},\n",
    "        {\"source\": \"mysql\", \"database\": f\"{db_patients}-{db_providers}\", \"table_name\": f\"{table_patients}-{table_providers}\" }\n",
    "    ],\n",
    "    ids=[table_patients, table_providers, f\"{table_patients}-{table_providers}\"], # unique for each doc\n",
    ")\n",
    "\n",
    "pg_collection1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff502167-2453-4eb5-9b29-0babf1ed0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RunPrompts(DatabaseType: str, question: str, mySQLCol, pgCol):\n",
    "    \"\"\"\n",
    "    Helper function that does the following:\n",
    "    - Picks up the ChromaDB collection (passed as input parameters) based on the DatabaseType\n",
    "    - Retrieves relevant table schemas from ChromaDB based on the Business Question.\n",
    "    - Invoke SageMaker LLM to return the SQL query for the table schemas retrieved.\n",
    "    - Run the query against the database.\n",
    "    - Invoke SageMaker LLM to analyze results of the query execution against the Business Question.\n",
    "\n",
    "    Args:\n",
    "        DatabaseType (str): mysql / postgresql\n",
    "        question (str): User's business question\n",
    "        mySQLCol (object): ChromadB collection for mySQL Database\n",
    "        pgCol (object): ChromadB collection for PostgreSQL Database\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis of the SQL query results provided by the language model.\n",
    "    \"\"\"\n",
    "    # Route the query according to the database passed\n",
    "    if DatabaseType == \"mysql\":\n",
    "        # For MySQL DB\n",
    "        collection_to_use = mySQLCol\n",
    "        db_conn = mySQL_db_conn\n",
    "    elif DatabaseType == \"postgresql\":\n",
    "        # For PostgreSQL DB\n",
    "        collection_to_use = pgCol\n",
    "        db_conn = pg_db_conn\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported database type: {DatabaseType}\")\n",
    "    \n",
    "    # Query/search 1 most similar results.\n",
    "    docs = collection_to_use.query(\n",
    "        query_texts=[question],\n",
    "        n_results=1\n",
    "    )\n",
    "\n",
    "    pattern = r\"<table_schemas>(.*)</table_schemas>\"\n",
    "    table_schemas = re.search(pattern, docs[\"documents\"][0][0], re.DOTALL).group(1)\n",
    "    print(f\"ChromaDB - Schema Retrieval: \\n{table_schemas.strip()}\")\n",
    "    \n",
    "    # Format the SQL system prompt\n",
    "    SQL_SYS_PROMPT = tmp_sql_sys_prompt.format(\n",
    "        question=question,\n",
    "        table_schemas=table_schemas,\n",
    "        dbtype=DatabaseType\n",
    "    )\n",
    "\n",
    "    # Get the SQL query from the LLM\n",
    "    print(f\"Using SageMaker to generate SQL for [{DatabaseType}]\")\n",
    "    llm_messages = [\n",
    "        {\"role\": \"system\", \"content\": SQL_SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    llm_output = sagemaker_chat_completion(llm_messages)\n",
    "    print(\"\\ncompletion = \")\n",
    "    print(llm_output)\n",
    "    \n",
    "    # Extract SQL query from LLM output\n",
    "    sql_pattern = r\"<sql>(.*?)</sql>\"\n",
    "    sql_matches = re.findall(sql_pattern, llm_output, re.DOTALL)\n",
    "    \n",
    "    if not sql_matches:\n",
    "        # Try alternative formatting with code blocks\n",
    "        sql_pattern = r\"```sql\\s*(.*?)\\s*```\"\n",
    "        sql_matches = re.findall(sql_pattern, llm_output, re.DOTALL)\n",
    "        \n",
    "    if not sql_matches:\n",
    "        print(\"⚠️ Warning: No SQL query found in LLM response. Using full response.\")\n",
    "        llm_sql_query = llm_output\n",
    "    else:\n",
    "        llm_sql_query = sql_matches[0].strip()\n",
    "    \n",
    "    print(\"\\nLLM SQL Query: \")\n",
    "    print(llm_sql_query)\n",
    "    \n",
    "    # Validate the SQL query\n",
    "    filter_keywords = [\"specific\", \"medical\", \"certain\", \"particular\", \"only\", \"with\"]\n",
    "    classification_keywords = [\"classify\", \"categorize\", \"group\", \"type\", \"category\"]\n",
    "    \n",
    "    # Check if the query might be missing filtering\n",
    "    if (\"WHERE\" not in llm_sql_query.upper() and \n",
    "        any(keyword in question.lower() for keyword in filter_keywords) and\n",
    "        not any(keyword in question.lower() for keyword in [\"all\", \"every\", \"total\"])):\n",
    "        print(\"⚠️ Warning: The query may be missing filtering criteria implied in the question.\")\n",
    "    \n",
    "    # Check if multiple statements but only some will execute\n",
    "    if llm_sql_query.count(\";\") > 1:\n",
    "        print(\"⚠️ Note: Multiple SQL statements detected. Ensure all necessary statements are executed.\")\n",
    "    \n",
    "    # Execute the SQL query\n",
    "    try:\n",
    "        sql_results = execute_query(llm_sql_query, db_conn)\n",
    "        print(\"\\nsql_results = \")\n",
    "        print(sql_results)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error executing SQL: {str(e)}\")\n",
    "        sql_results = f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Analyze the results\n",
    "    analysis_prompt = QNA_SYS_PROMPT.format(\n",
    "        query_results=sql_results,\n",
    "        question=question,\n",
    "        sql_query=llm_sql_query  # Include the SQL query for context\n",
    "    )\n",
    "    \n",
    "    analysis_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analyst providing insights from SQL query results.\"},\n",
    "        {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "    ]\n",
    "    \n",
    "    analysis = sagemaker_chat_completion(analysis_messages)\n",
    "    print(\"\\nAnalysis: \")\n",
    "    print(analysis)\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7669-6cd2-485f-8d41-44fb3526c86e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Create a Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca7fb8-1d19-423d-85c2-20967b769a84",
   "metadata": {},
   "source": [
    "Here, we design our prompt template that will account for our question and answer, and formatted correctly for use with Llama 4 models.\n",
    "\n",
    "First, we create a `system prompt` containing two parts:\n",
    "\n",
    "1. `table_schemas`. This is a description of the structure of the database table(s), including the name of the table, the names of the columns within each table, and the data types of each column. This information helps Llama 4 to understand the organization and contents of the table.\n",
    "\n",
    "2. `question`. This is the specific request or information that the user wants to obtain from the table.\n",
    "\n",
    "By including both the table schema and the user's question in the system prompt, we provide Llama 4 model a complete understanding of the table structure and the user's desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588156e2-5b35-4828-a3ca-e077f2c542ac",
   "metadata": {},
   "source": [
    "Now, we'll use a few-shot approach using the retrieved tables from ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbb802-9437-47b5-b728-9e2e7c9a64df",
   "metadata": {},
   "source": [
    "First, we create a `system prompt` containing a placeholder including any number of table schemas for ChromaDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0758c4-7710-43af-ad4f-e1a3123f685a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \n",
    "        \"\"\"You are a {dbtype} query expert whose output is a valid SQL query.\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "It has the following schemas:\n",
    "<table_schemas>\n",
    "{table_schemas}\n",
    "</table_schemas>\n",
    "\n",
    "Important guidelines:\n",
    "1. Always combine the database name and table name to build your queries.\n",
    "2. Only make assumptions based on the schema provided - do not invent columns that aren't in the schema.\n",
    "3. If the query requires filtering (e.g., \"medical coverage\"), ensure you include appropriate WHERE clauses.\n",
    "4. If the question cannot be fully answered with the provided schema, explicitly state what additional information would be needed.\n",
    "5. For multi-part questions, ensure you address all parts in your queries.\n",
    "\n",
    "Please construct a valid SQL statement to answer the following question, return only the {dbtype} query in between <sql></sql>.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{question}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract prompts directly\n",
    "tmp_sql_sys_prompt = instructions[0]['content']\n",
    "sysPt = instructions[0]['content']\n",
    "userPt = instructions[1]['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb0279-e526-436e-92d1-31fb37ffb795",
   "metadata": {},
   "source": [
    "Next, we create a new `system prompt` containing two parts:\n",
    "\n",
    "1. `query_results` represents the SQL query results after executing the prompt `tmp_sql_sys_prompt`. This is the raw data that Llama 4 model will use to generate its analysis.\n",
    "\n",
    "2. `question`. This specifies the type of analysis or insight that the user wants Llama 4 model to provide based on the SQL query results.\n",
    "\n",
    "By combining the SQL query results and the user's question into a single system prompt, we provide Llama 4 model all the information it needs to understand the context and provide a comprehensive analysis tailored to the user's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5decf8-1aa9-4439-b61d-a4b745d66f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Given the following SQL query results:\n",
    "{query_results}\n",
    "\n",
    "And the original question:\n",
    "{question}\n",
    "\n",
    "Please provide an analysis and interpretation of the results to answer the original question.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract the prompt directly\n",
    "QNA_SYS_PROMPT = instructions[0]['content']\n",
    "sysPtI = \"\"  # No system prompt in this instructions list\n",
    "userPtI = instructions[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc134b9c-96de-47b7-a182-664024d0b2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Execute Few Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977f101-cd0b-4ab4-bc6e-85bf3d759f1d",
   "metadata": {},
   "source": [
    "In this example, we expect the table `airplanes` to be used for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8853827-1d81-4ef4-befc-d2f55d52314a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of providers?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a93445-5897-465a-aacc-8e4856dffb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of patients?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf51591-ba4f-43b6-9c63-e70df33b578f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of patients per provider?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a5731-2f4c-46f0-b03a-c417338064ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enhanced Schema Retrieval with an Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c58d4-9330-438a-9aa6-72340fdc4b64",
   "metadata": {},
   "source": [
    "In this approach, we will use an embedding model from AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee69c27-b639-4380-9f32-356f18b55584",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Ingest docs into ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746786d7-8ab0-4b99-b57e-efdba7f1131c",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the data is preprocessed, the next step is to ingest all `docs` into ChromaDB using your selected embedding model (`Amazon SageMaker BGE Large English` or `Amazon Titan Embedding V2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea54fec-5074-44bc-bff5-b5b1e7e4a4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embedding function with AWS\n",
    "aws_ef = AmazonSageMakerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b83b48-c84d-4740-92f0-a552c8026860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete collection if exists\n",
    "try:\n",
    "    chroma_client.get_collection(name=\"table-schemas-aws-embedding-model\")\n",
    "except ValueError:\n",
    "    # Collection does not exist\n",
    "    pass\n",
    "else:\n",
    "    chroma_client.delete_collection(name=\"table-schemas-aws-embedding-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca323c-68c2-41cb-afbc-03fece3db46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a more comprehensive combined schema with clear table relationships\n",
    "    combined_schema = f\"\"\"\n",
    "    /* Database: {db_patients} */\n",
    "    {table_schema_patients}\n",
    "    \n",
    "    /* Database: {db_providers} */\n",
    "    {table_schema_providers}\n",
    "    \n",
    "    /* Relationships */\n",
    "    The {db_patients}.{table_patients} table has a foreign key Insurance_id that references {db_providers}.{table_providers}(Insurance_id).\n",
    "    This relationship indicates which provider covers which patient.\n",
    "    \"\"\"\n",
    "    \n",
    "    collection2.add(\n",
    "        documents=[\n",
    "            table_schema_patients,\n",
    "            table_schema_providers,\n",
    "            combined_schema\n",
    "        ],\n",
    "        metadatas=[\n",
    "            {\"source\": \"mysql\", \"database\": db_patients, \"table_name\": table_patients},\n",
    "            {\"source\": \"mysql\", \"database\": db_providers, \"table_name\": table_providers},\n",
    "            {\"source\": \"mysql\", \"database\": f\"{db_patients}-{db_providers}\", \"table_name\": f\"{table_patients}-{table_providers}\" }\n",
    "        ],\n",
    "        ids=[table_patients, table_providers, f\"{table_patients}-{table_providers}\"], # unique for each doc\n",
    "    )\n",
    "    print(\"Successfully added documents to collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding documents to collection: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec741334-b2ac-4d27-b11e-ee365dadf0fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Execute Few Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265c0a3-40d9-482b-98ff-1077b358cecd",
   "metadata": {},
   "source": [
    "In this example, we expect the table `airplanes` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9a734-4898-4e00-9df1-af448acb5424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of providers?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ca9b6-a2af-4864-9281-9fc88e754452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of patients per provider?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074476e-0484-484d-92ac-e3a041672d42",
   "metadata": {},
   "source": [
    "For this example, we expect the table `airplane` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9fffb-4ebb-4c89-a41f-6e74dc214bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"How many unique providers are represented in the database?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab7a3-bb1b-4f1b-82d1-4e02f8d40848",
   "metadata": {},
   "source": [
    "For this example, we expect the table `flights` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd806b74-efe4-4701-8a3e-480e8a456074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"How many providers provide medical coverage?\"\n",
    "\n",
    "# Call the updated RunPrompts function without ServiceType parameter\n",
    "results = RunPrompts(\n",
    "    DatabaseType=llm_selected_db, \n",
    "    question=question, \n",
    "    mySQLCol=collection1, \n",
    "    pgCol=pg_collection1\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5c40f-5e79-4b0a-9c0a-fa16b8195b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b500a5c-16cc-4d23-9d06-8666928daa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Delete resources\n",
    "try:\n",
    "    llm_predictor.delete_model()\n",
    "    llm_predictor.delete_endpoint()\n",
    "    print(\"LLM resources deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting LLM resources: {e}\")\n",
    "\n",
    "try:\n",
    "    embedding_predictor.delete_model()\n",
    "    embedding_predictor.delete_endpoint()\n",
    "    print(\"Embedding resources deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting embedding resources: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce768358-c208-4c81-acff-ecb39b904b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distributors\n",
    "- AWS\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
