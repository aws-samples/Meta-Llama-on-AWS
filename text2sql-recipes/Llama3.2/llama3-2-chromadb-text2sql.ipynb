{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4901509a-8316-47a3-879b-544a43b37ef3",
   "metadata": {},
   "source": [
    "# Text-to-SQL using Llama 3.2 and ChromaDB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa596a49-ce47-4053-a93d-699bdef52426",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b488b5-d05b-45d2-84fb-6bc80cc30241",
   "metadata": {},
   "source": [
    "This notebook introduces a versatile approach that leverages Llama 3.2 models on Amazon Bedrock, including advanced prompt engineering, to convert natural language questions into executable SQL queries. Our approach generates SQL queries capable of joining data from tables across multiple databases, enabling information retrieval from complex database structures. This multi-database capability is crucial in real-world scenarios where data is often distributed across various tables with intricate relationships, and queries need to combine information from multiple sources to provide comprehensive insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf152e5-95fe-44fa-836c-5384afd3a1c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Llama 3.2 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126d29c-4748-4831-8aff-60d71c61462e",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are Four Llama 3.2 models available on Amazon Bedrock:\n",
    "\n",
    "#### Llama 3.2 1B (text input)\n",
    "The most lightweight model in the Llama 3.2 collection of models, perfect for retrieval and summarization for edge devices and mobile applications. This model is ideal for the following use cases: personal information management and multilingual knowledge retrieval.\n",
    "\n",
    "#### Llama 3.2 3B (text input)\n",
    "Designed for applications requiring low-latency inferencing and limited computational resources. It excels at text summarization, classification, and language translation tasks. This model is ideal for the following use cases: mobile AI-powered writing assistants and customer service applications.\n",
    "\n",
    "#### Llama 3.2 11B Vision (text + image input)\n",
    "Well-suited for content creation, conversational AI, language understanding, and enterprise applications requiring visual reasoning. The model demonstrates strong performance in text summarization, sentiment analysis, code generation, and following instructions, with the added ability to reason about images.\n",
    "\n",
    "#### Llama 3.2 90B Vision (text + image input)\n",
    "Metaâ€™s most advanced model, ideal for enterprise-level applications. This model excels at general knowledge, long-form text generation, multilingual translation, coding, math, and advanced reasoning. \n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Llama 3.2 Model Cards and Prompt Formats](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31897e0-201b-4cc2-ac5b-f517c2d29f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Approach to the Text-to-SQL Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd85684",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook covers the following approaches\n",
    "\n",
    "### Few-shot text-to-SQL powered by ChromaDB (Schema Retrieval vs Enhance Schema Retrieval with Sample Questions)\n",
    "\n",
    "This approach leverages ChromaDB, a vector database, to assist the few-shot text-to-SQL translation process. ChromaDB stores the database schema information, which includes table names, column names, and their descriptions. When a natural language question is provided, the model can retrieve relevant schema information from ChromaDB to aid in generating the SQL query. ChromaDB can be used in two ways:\n",
    "\n",
    "1. **Using the Default Embedding**: In this method, the default embedding model is used.\n",
    "\n",
    "2. **Using a Custom Embedding**: In this method, we show the option of using 1/ HuggingFace BGE Large EN Embedding model - on Amazon SageMaker JumpStart or 2/ Amazon Titan v2 Embedding model - on Amazon Bedrock.\n",
    "\n",
    "By leveraging ChromaDB, the few-shot text-to-SQL translation process can benefit from efficient schema and sample data retrieval, potentially leading to better performance and generalization across different databases and query types.\n",
    "\n",
    "**Note:** For simple Few-shot text-to-SQL (Single Table vs Multiple Tables), please review [this notebook](https://github.com/aws-samples/Meta-Llama-on-AWS/blob/917ee12690a8e43eb9699b88662e7144ce9acac4/text2sql-recipes/llama3-1-chromadb-text2sql.ipynb).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf319c-23af-4108-8372-0e464c1f3127",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dceec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This notebook will provide code snippets to assist with implementing two differents approaches to converting a natural language question into a SQL query. The query will be executed in the database to answer the original question.\n",
    "\n",
    "For accessing LLMs the following code snippets are provided:\n",
    "1. Amazon Bedrock Invoke API\n",
    "2. Amazon Bedrock Converse API\n",
    "3. SageMaker Jumpstart\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dca637-1d1a-445d-a943-d2448fd39ac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b33e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. [Getting Started](#Getting-Started)\n",
    "    + [Install Dependencies](#Step-0:-Install-Dependencies)\n",
    "    + [Select Model Hosting Service](#Step-1:-Select-Hosting-Model-Service)\n",
    "    + [Get Database and Schema details](Step-2:-Get-Database-and-Schema-details)\n",
    "    + [Create helper functions](Step-3:-Create-helper-functions)\n",
    "1. [Few-shot text-to-SQL powered by ChromaDB](#Few-shot-text-to-SQL-powered-by-ChromaDB)\n",
    "    + [Schema Retrieval](#Schema-Retrieval)\n",
    "    + [Data Preprocessing](#Step-1:-Data-Preprocessing)\n",
    "    + [Ingest docs into ChromaDB](#Step-2:-Ingest-docs-into-ChromaDB)\n",
    "    + [Create a Few-Shot Prompt](#Step-3:-Create-a-Few-Shot-Prompt)\n",
    "    + [Execute Few-Shot Prompts](#Step-4:-Execute-Few-Shot-Prompts)\n",
    "    + [Conclusion](#Step-5-Conclusion)\n",
    "1. [Enhanced Schema Retrieval with ChromaDB and an Embedding Model](#Enhanced-Schema-Retrieval-with-an-Embedding-Model)\n",
    "    + [Ingest docs into ChromaDB](#Step-1:-Ingest-docs-into-ChromaDB)\n",
    "    + [Execute Few-Shot Prompts](#Step-2:-Execute-Few-Shot-Prompts)\n",
    "    + [Conclusion](#Step-3-Conclusion)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a20c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tools\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. In this notebook, LangChain is used to build a prompt template.\n",
    "\n",
    "+ [ChromaDB](https://www.trychroma.com/) is a vector database that enables efficient semantic search, storage, and retrieval of unstructured data like text, images, and audio. It's designed to work well with large language models (LLMs) and provides a simple and scalable way to build applications that can search and retrieve relevant information from vast amounts of data.\n",
    "\n",
    "+ RDS (Relational Database Service) for [MySQL](https://aws.amazon.com/rds/mysql/) is a managed database service provided by Amazon Web Services (AWS). RDS for MySQL simplifies the setup, operation, and scaling of MySQL databases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be5a3a-78a0-4b4f-a0aa-fc0bf0f7676b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a6bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. It is mandatory to have set up the database and sample data prior to using [this notebook](llama3-2-chromadb-text2sql-DB-Setup.ipynb).\n",
    "2. Use kernel either `conda_python3`, `conda_pytorch_p310` or `conda_tensorflow2_p310`.\n",
    "3. Install the required packages.\n",
    "4. Access to the LLM API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd282c-e45f-463e-9f68-f80176f4f0af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Amazon Bedrock Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92d068-da09-4d98-97f6-d14e7b0b8f6f",
   "metadata": {},
   "source": [
    "In this notebook, Llama 3.2 1B model is used. However, you can easily switch between the other Llama 3.2 models to evaluate the responses. By deploying the notebook through our cloudformation template, it is granted the appropriate IAM permissions to send API request to Bedrock. \n",
    "\n",
    "Refer [here](https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/) for details on how Amazon Bedrock provides access to Metaâ€™s Llama 3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae4171-4f20-4a68-b6e1-0a40a3854de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465f181-b661-4f59-bdac-92ed0e164203",
   "metadata": {},
   "source": [
    "#### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama3.2 1B Text Generation: `ml.g5.xlarge`, `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, `ml.g6.xlarge`, `ml.g6.2xlarge`, `ml.g6.4xlarge`, `ml.g6.8xlarge`, `ml.g6.12xlarge`, `ml.g6.24xlarge`, `ml.g6.48xlarge`, `ml.p4d.24xlarge` and `ml.p5.48xlarge`\n",
    " - Llama3.2 3B Text Generation: `ml.g5.48xlarge`, `ml.g6.48xlarge`, `ml.p5.48xlarge`, `ml.p4d.24xlarge` and `ml.p5.48xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "**Note:** By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b2317-d90d-481f-8d51-464a93650978",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd1931-c19d-4c30-bfef-19cbc59724f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 0: Install Dependencies\n",
    "\n",
    "Here, we will install all the required dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d52454-073b-4a5a-b293-924419e1c8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.32 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install mysql-connector-python==8.4.0 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install langchain==0.2.5 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install chromadb==0.5.0 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install numpy==1.26.4 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install psycopg2==2.9.9 -qU --force --quiet --no-warn-conflicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097885d-9818-4015-b273-4fe30415cdfd",
   "metadata": {},
   "source": [
    "**Note:** *When installing libraries using the pip, you may encounter errors or warnings during the installation process. These are generally not critical and can be safely ignored. However, after installing the libraries, it is recommended to restart the kernel or computing environment you are working in. Restarting the kernel ensures that the newly installed libraries are loaded properly and available for use in your code or workflow.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea1457-e5a3-46b1-870f-ba7e73662ec2",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><b>NOTE:</b> Restart the kernel with the updated packages that are installed through the dependencies above</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7518a54-885b-4403-ac4b-d5aab2ebb3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afba42-5061-4464-9ec9-8641992a7d66",
   "metadata": {},
   "source": [
    "#### Import the required modules to run the notbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756999dd-ac11-4b3a-8e43-2c4cdc2ad9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import chromadb\n",
    "from chromadb.api.types import (\n",
    "    Documents,\n",
    "    EmbeddingFunction,\n",
    "    Embeddings,\n",
    ")\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "import mysql.connector as MySQLdb\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703c9333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Bedrock Client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1f1bb-966d-4726-804c-b0d1f9108d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Select Hosting Model Service\n",
    "\n",
    "Here, you can select to run this notebook using SageMaker JumpStart or Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9f9044-79b0-4f76-9085-0f8c28568f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to run the LLM for this notebook using Amazon Bedrock (B) or Amazon Bedrock Converse API (C) or Amazon SageMaker JumpStart (S)? (default: B)  C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen to run the LLM for this notebook using [Amazon Bedrock Converse API].\n"
     ]
    }
   ],
   "source": [
    "def ask_for_service():\n",
    "    service = input(\"Do you want to run the LLM for this notebook using Amazon Bedrock (B) or Amazon Bedrock Converse API (C) or Amazon SageMaker JumpStart (S)? (default: B) \").strip().upper()\n",
    "    if service in ['S', 'SAGEMAKER']:\n",
    "        return 'Amazon SageMaker'\n",
    "    elif service in ['B', 'BEDROCK (Invoke API)', '']:\n",
    "        return 'Amazon Bedrock'\n",
    "    elif service in  ['C', 'BEDROCK (Converse API)']:\n",
    "        return 'Amazon Bedrock Converse API'\n",
    "    else:\n",
    "        print(\"Invalid input. Using Amazon Bedrock by default.\")\n",
    "        return 'Amazon Bedrock'\n",
    "\n",
    "# Call the function and get the selected service\n",
    "llm_selected_service = ask_for_service()\n",
    "\n",
    "# Print the selected service\n",
    "print(f\"You have chosen to run the LLM for this notebook using [{llm_selected_service}].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516b0617-6f4b-4344-b44d-8abf090b9c09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to run the Embedding for this notebook using Amazon Bedrock (B) or Amazon SageMaker JumpStart (S)? (default: B)  B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen to run the Embedding for this notebook using [Amazon Bedrock].\n"
     ]
    }
   ],
   "source": [
    "def ask_for_service():\n",
    "    service = input(\"Do you want to run the Embedding for this notebook using Amazon Bedrock (B) or Amazon SageMaker JumpStart (S)? (default: B) \").strip().upper()\n",
    "    if service in ['S', 'SAGEMAKER']:\n",
    "        return 'Amazon SageMaker'\n",
    "    elif service in ['B', 'BEDROCK', '']:\n",
    "        return 'Amazon Bedrock'\n",
    "    else:\n",
    "        print(\"Invalid input. Using Amazon Bedrock by default.\")\n",
    "        return 'Amazon Bedrock'\n",
    "\n",
    "# Call the function and get the selected service\n",
    "embedding_selected_service = ask_for_service()\n",
    "\n",
    "# Print the selected service\n",
    "print(f\"You have chosen to run the Embedding for this notebook using [{embedding_selected_service}].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c904c9df-7245-4c3b-98af-96298a61d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Î¼s, sys: 1 Î¼s, total: 5 Î¼s\n",
      "Wall time: 7.15 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if llm_selected_service == 'Amazon SageMaker':\n",
    "    # Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "    from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "    # Specify the model ID for the Meta Llama 3.2 Instruct LLM model\n",
    "    llama3_2_1b_id = \"meta-textgeneration-llama-3-2-1b-instruct\"\n",
    "    llama3_2_3b_id = \"meta-textgeneration-llama-3-2-3b-instruct\"\n",
    "    llama3_2_11b_id = \"meta-vlm-llama-3-2-11b-vision-instruct\"\n",
    "    llama3_2_90b_id = \"meta-vlm-llama-3-2-90b-vision-instruct\"\n",
    "    \n",
    "    DEFULT_LLM_MODEL_ID = llama3_2_1b_id\n",
    "    \n",
    "    model = JumpStartModel(model_id=DEFULT_LLM_MODEL_ID, instance_type=\"ml.g5.8xlarge\")\n",
    "    \n",
    "    llm_predictor = model.deploy(accept_eula=True)\n",
    "    \n",
    "    print(f\"\\nLLM SageMaker Endpoint Name: [{llm_predictor.endpoint_name}].\\n\")\n",
    "else:\n",
    "    llm_predictor = None\n",
    "    \n",
    "    # Specify the model ID for the Meta Llama 3.2 Instruct LLM model\n",
    "    llama3_2_1b_id = \"us.meta.llama3-2-1b-instruct-v1:0\"\n",
    "    llama3_2_3b_id = \"us.meta.llama3-2-3b-instruct-v1:0\"\n",
    "    llama3_2_11b_id = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "    llama3_2_90b_id = \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "\n",
    "    DEFULT_LLM_MODEL_ID = llama3_2_1b_id\n",
    "    \n",
    "    DEFAULT_EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "if embedding_selected_service == 'Amazon SageMaker':\n",
    "    # Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "    from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "    # Deploy BGE Large EN embedding model on Amazon SageMaker JumpStart:\n",
    "    # Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "    DEFAULT_EMBEDDING_MODEL_ID = \"huggingface-sentencesimilarity-bge-large-en\"\n",
    "\n",
    "    text_embedding_model = JumpStartModel(model_id=DEFAULT_EMBEDDING_MODEL_ID, instance_type=\"ml.g5.4xlarge\" )\n",
    "    \n",
    "    embedding_predictor = text_embedding_model.deploy()\n",
    "    \n",
    "    print(f\"\\nLLM SageMaker Endpoint Name: [{embedding_predictor.endpoint_name}].\\n\")\n",
    "else:\n",
    "    embedding_predictor = None\n",
    "    DEFAULT_EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879ef33f-7710-4d5b-aa93-19a986002a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook example uses Amazon RDS MySQL DB.\n",
    "llm_selected_db = \"mysql\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12765f-10a8-4aae-9e62-4ee7f4e526cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Get Database and Schema details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2493f9-833f-40ff-a9db-b81f47e37c66",
   "metadata": {},
   "source": [
    "Here, we retrieve the services that are already deployed as a part of the cloudformation template to be used in building the application. The services include,\n",
    "\n",
    "+ Secret ARN with RDS for MySQL Database credentials\n",
    "+ Database Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d355be7-9365-41d5-8f63-153898c7c169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stackname = \"text2sql\"  # If your stack name differs from \"text2sql\", please modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca002c7-3416-4ac5-b9a1-2b730557fb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "response = cfn.describe_stack_resources(\n",
    "    StackName=stackname\n",
    ")\n",
    "cfn_outputs = cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']\n",
    "\n",
    "# Get rds secret arn and database endpoint from cloudformation outputs\n",
    "for output in cfn_outputs:\n",
    "    if 'SecretArnMySQL' in output['OutputKey']:\n",
    "        mySQL_secret_id = output['OutputValue']\n",
    "\n",
    "    if 'DatabaseEndpointMySQL' in output['OutputKey']:\n",
    "        mySQL_db_host = output['OutputValue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e5601b-287b-421e-8711-ef73d5c09cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "secrets_client = boto3.client('secretsmanager')\n",
    "\n",
    "# Get MySQL credentials from Secrets Manager\n",
    "credentials = json.loads(secrets_client.get_secret_value(SecretId=mySQL_secret_id)['SecretString'])\n",
    "\n",
    "# Get password and username from secrets\n",
    "mySQL_db_password = credentials['password']\n",
    "mySQL_db_user = credentials['username']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c970c8c-f566-430f-b12b-e669cc7241eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Establish the database connection (MySQL DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705ab665-9b2e-4141-b7fb-df3329ac61c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mySQL_db_conn = MySQLdb.connect(\n",
    "    host=mySQL_db_host,\n",
    "    user=mySQL_db_user,\n",
    "    password=mySQL_db_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cb5ef-41ea-43d9-ace9-8ca9c2dda260",
   "metadata": {},
   "source": [
    "#### Load table schema settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2579b20a-eb9c-4146-88e7-d4936d8d3038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_settings(file_path):\n",
    "    \"\"\"\n",
    "    Reads a YAML file and returns its contents as a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        obj: The contents of the YAML file as a Python object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error: Failed to parse the YAML file '{file_path}': {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fcf569e-6c76-4eee-8be7-cc0ab13a3802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MySQL Table Setup\n",
    "\n",
    "# Load table settings - database: equipment_db | table_name: airplanes\n",
    "settings_airplanes = load_settings('schemas/airplanes_ms.yml')\n",
    "table_airplanes = settings_airplanes['table_name']\n",
    "table_schema_airplanes = settings_airplanes['table_schema']\n",
    "db_airplanes = settings_airplanes['database']\n",
    "\n",
    "# Load table settings - database: transport_db | table_name: flights\n",
    "settings_flights = load_settings('schemas/flights_ms.yml')\n",
    "table_flights = settings_flights['table_name']\n",
    "table_schema_flights = settings_flights['table_schema']\n",
    "db_flights = settings_flights['database']\n",
    "\n",
    "# Load table settings\n",
    "settings_airplane_flights = load_settings('schemas/airplanes-flights_ms.yml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf28a8-0d22-4223-989b-243431149e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Create helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae6aca-18fb-4beb-a6ae-caaeea512d1e",
   "metadata": {},
   "source": [
    "To facilate the usability and readability of the SQL Query Analysis made by Llama 3.2, let's create a suite of helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85e516-6344-41c7-90c8-0bdbd219ace8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Chat Completion (Invoke LLM and Return response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaedc7-f395-4d92-9809-8f7b71baa42f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `sagemaker_chat_completion` function uses the SageMaker Endpoint to invoke the LLMs. The response from the LLM is extracted and returned as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b4863f-ff81-4f02-8c25-7f0d50f99d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sagemaker_chat_completion(\n",
    "    prompt: str,\n",
    "    max_gen_len: int = 512,\n",
    "    temperature: float = 0.1,\n",
    "    top_p: float = 0.1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a chat completion from a prompt using the llama3.2 model via Amazon SageMaker JumpStart.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt text to generate completions for.\n",
    "        max_gen_len (int, optional): The maximum length of the completion.\n",
    "        temperature (float, optional): Sampling temperature for the model.\n",
    "        top_p (float, optional): Top p sampling ratio for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text completion.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_gen_len,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"stop\": [\"<|eot_id|>\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Call the model API to generate the completion\n",
    "    response = llm_predictor.predict(body)\n",
    "    completion = response.get('generated_text', '')\n",
    "\n",
    "    return completion.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6babb4b-812d-415f-b666-b1f69fc28706",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `bedrock_chat_completion` function uses the Bedrock client to invoke the LLMs. The response from the LLM is extracted and returned as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c11b382-cf2d-4df9-971c-dc92c9a13043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_chat_completion(\n",
    "    model_id: str,\n",
    "    prompt: str,\n",
    "    max_gen_len: int = 512,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 0.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a chat completion from a prompt using the llama3 model via Amazon Bedrock.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The ID of the llama3 model to use for completion.\n",
    "        prompt (str): The prompt text to generate completions for.\n",
    "        max_gen_len (int, optional): The maximum length of the completion.\n",
    "        temperature (float, optional): Sampling temperature for the model.\n",
    "        top_p (float, optional): Top p sampling ratio for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text completion.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_gen_len\": max_gen_len,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "    }\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Convert the body dictionary to JSON string and encode it as bytes\n",
    "    body_json = json.dumps(body)\n",
    "    body_bytes = body_json.encode('utf-8')\n",
    "\n",
    "    # Call the model API to generate the completion\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body_bytes, modelId=model_id, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = response[\"body\"].read()\n",
    "    response_body = json.loads(response_body)\n",
    "    completion = response_body.get(\"generation\", \"\")\n",
    "    \n",
    "    inputTokenCount = response_body.get(\"prompt_token_count\", \"\")\n",
    "    outputTokenCount = response_body.get(\"generation_token_count\", \"\")\n",
    "    stopReason = response_body.get(\"stop_reason\", \"\")\n",
    "    \n",
    "    return completion.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ca1db-a4aa-418e-a9aa-998e4a07b0ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Function `bedrock_converseapi_completion` generates and executes an SQL query for a given question, and returns a comprehensive analyzes based on the sql query results. This uses the Amazon Bedrock Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b83956d-6d31-4e2a-bc9a-dacf6b0515e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_converseapi_completion(\n",
    "    model_id: str,\n",
    "    conversation,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 0.0,\n",
    "    system_prompt=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a chat completion from a conversation using the llama3.2 model via \n",
    "    Amazon Bedrock Converse API.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The ID of the llama3 model to use for completion.\n",
    "        conversation (object): The conversation so far user/assistant/user/...\n",
    "        max_gen_len (int, optional): The maximum length of the completion.\n",
    "        temperature (float, optional): Sampling temperature for the model.\n",
    "        top_p (float, optional): Top p sampling ratio for the model.\n",
    "        system_prompt: Any instructions to regulate the LLM behaviour.\n",
    "\n",
    "    Returns:\n",
    "        response: Object containing the assistant's response\n",
    "    \"\"\"  \n",
    "    try:\n",
    "        if system_prompt == \"\":\n",
    "            # Send the message to the model, using the provided inference configuration.\n",
    "            response = bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"topP\": top_p\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            # Send the message to the model, using the provided inference configuration.\n",
    "            response = bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"topP\": top_p\n",
    "                },\n",
    "                system=[{\"text\": system_prompt}],\n",
    "            )\n",
    "\n",
    "        return response\n",
    "    \n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698ed84-c952-42f2-8fbc-2f73233725ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Instruction formatting, Query execution and LLM calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae9ae6-fcd2-4734-a655-be6a854b7224",
   "metadata": {},
   "source": [
    "The `format_instructions` function is designed to process the input from Llama 3 models, allowing a conversation between roles such as `system`, `user`, and `assistant`. To see more details about Llama 3 prompt formats, click [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7912ce9e-598c-47db-9780-b1cc95408f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\n",
    "    Formats the prompt. Returns as a native prompt for Llama and also returns the System and User Prompt \n",
    "    texts that can be used with Bedrock Converse API.\n",
    "\n",
    "    Args:\n",
    "        instructions (List): A Dictionary of user and system instructions\n",
    "    Returns:\n",
    "        str: A formatted string containing the prompt, plaintext system prompt and plaintext user prompt.\n",
    "    \"\"\"\n",
    "    systemPrompt = userPrompt = \"\"\n",
    "    prompt: List[str] = []\n",
    "    \n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \"<|eot_id|>\"])\n",
    "            systemPrompt = instruction[\"content\"].strip()\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \"<|eot_id|>\"])\n",
    "            userPrompt = instruction[\"content\"].strip()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt), \"\".join(systemPrompt), \"\".join(userPrompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ed73f-1ea7-4a5f-91be-e5217701dc7c",
   "metadata": {},
   "source": [
    "The `execute_query` function will execute SQL queries, typically for retrieving data from a database, and format the results as a string for further processing or display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9401ba7a-de43-410d-8015-a38a60f2b6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_query(query: str, db_conn) -> str:\n",
    "    \"\"\"Execute an SQL query on the database connection and return the results as a string.\n",
    "\n",
    "    Args:\n",
    "        query (str): SQL query to execute\n",
    "        db_conn (Connection object): Connection object to the database where the query needs to be executed.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the SQL results.\n",
    "    \"\"\"\n",
    "    # Get a cursor from the database connection\n",
    "    mycursor = db_conn.cursor()\n",
    "\n",
    "    # Execute the SQL query\n",
    "    mycursor.execute(query)\n",
    "\n",
    "    # Fetch all result rows\n",
    "    result_rows = mycursor.fetchall()\n",
    "\n",
    "    # Convert result to string with newline between rows\n",
    "    output_text = '\\n'.join([str(x) for x in result_rows])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997f36-a08b-4189-ba98-a410d9063e41",
   "metadata": {},
   "source": [
    "The Function `get_llm_sql_analysis` generates and executes an SQL query for a given question, and returns a comprehensive analyzes based on the sql query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e8610a-69e9-452b-a3cc-42f6e3129709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_sql_analysis(question: str, sql_sys_prompt: str, qna_sys_prompt: str, DatabaseType: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an SQL query based on the given question, executes it, and returns an analysis of the results using Llama 3.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question for which an SQL query needs to be generated.\n",
    "        sql_sys_prompt (str): The prompt to be used for generating the SQL query using Llama 3.\n",
    "        qna_sys_prompt (str): The prompt to be used for analyzing the SQL query results using Llama 3.\n",
    "        DatabaseType (enum): The type of database on which query needs to be executed. E.g. MySQL / PostgreSQL etc\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis of the SQL query results provided by the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # *****************************************************************************************\n",
    "        # 1. Generate SQL Query\n",
    "        # *****************************************************************************************\n",
    "        print(f\"\\nUsing [{llm_selected_service}] to generate SQL for [{DatabaseType}]\\n\")\n",
    "        \n",
    "        if llm_selected_service == 'Amazon SageMaker':\n",
    "            # Generates SQL query\n",
    "            completion = sagemaker_chat_completion(\n",
    "                prompt=sql_sys_prompt\n",
    "            )\n",
    "        elif llm_selected_service == 'Amazon Bedrock Converse API':\n",
    "            # *****************************************************************************************\n",
    "            # TODO: Perform additional steps for converse API\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": question}],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            response = bedrock_converseapi_completion(\n",
    "                model_id=DEFULT_LLM_MODEL_ID, \n",
    "                conversation=conversation, \n",
    "                system_prompt=sql_sys_prompt)\n",
    "\n",
    "            completion=response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            # *****************************************************************************************\n",
    "\n",
    "        else:\n",
    "            # Generates SQL query\n",
    "            completion = bedrock_chat_completion(\n",
    "                model_id=DEFULT_LLM_MODEL_ID,\n",
    "                prompt=sql_sys_prompt\n",
    "            )\n",
    "        \n",
    "        print(f\"completion = \\n{completion}\\n\")\n",
    "        # *****************************************************************************************\n",
    "        # 2. Extract SQL and Execute\n",
    "        # *****************************************************************************************\n",
    "\n",
    "        # Extract the SQL query from the completion returned from the first LLM call.\n",
    "        pattern = r\"<sql>(.*)</sql>\"\n",
    "        \n",
    "        sr = re.search(pattern, completion, re.DOTALL)\n",
    "\n",
    "        if sr == None:\n",
    "            pattern = r\"```sql(.*)```\"\n",
    "            sr = re.search(pattern, completion, re.DOTALL)\n",
    "\n",
    "        llm_sql_query = sr.group(1)\n",
    "        print(f\"\\nLLM SQL Query: \\n{llm_sql_query}\")\n",
    "    \n",
    "        # Route the query according to the database passed. Connection object will vary.\n",
    "        match DatabaseType:\n",
    "            case \"mysql\":\n",
    "                db_conn=mySQL_db_conn\n",
    "            case \"postgresql\":\n",
    "                db_conn=pg_db_conn\n",
    "\n",
    "        # Execute SQL query based on the database provided.\n",
    "        sql_results = execute_query(llm_sql_query, db_conn)\n",
    "\n",
    "        print(f\"\\nsql_results = \\n{sql_results}\")\n",
    "\n",
    "        # *****************************************************************************************\n",
    "        # 3. Evaluate the response\n",
    "        # *****************************************************************************************\n",
    "        \n",
    "        print(f\"\\nCalling LLM on [{llm_selected_service}] to Analyze and interpret the results from SQL query in relation to the original question.\\n\")\n",
    "        \n",
    "        # Now we will use a second LLM call to analyse the result of the query.\n",
    "        if llm_selected_service == 'Amazon SageMaker':\n",
    "            # Generates SQL analysis\n",
    "            llm_sql_analysis = sagemaker_chat_completion(\n",
    "                prompt=qna_sys_prompt.format(query_results=sql_results, question=question)\n",
    "            )\n",
    "        elif llm_selected_service == 'Amazon Bedrock Converse API':\n",
    "            # *****************************************************************************************\n",
    "            # TODO: Append Assistant response.\n",
    "\n",
    "            # Append user's next question\n",
    "            # Create a conversation object\n",
    "            queryAnalyseResults = qna_sys_prompt.format(query_results=sql_results, question=question)\n",
    "\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": queryAnalyseResults}],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Now we will use a second llM call to analyse the result of the query.\n",
    "            response = bedrock_converseapi_completion(\n",
    "                model_id=DEFULT_LLM_MODEL_ID, \n",
    "                conversation=conversation)\n",
    "\n",
    "            llm_sql_analysis = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            # *****************************************************************************************\n",
    "        else:\n",
    "            # Generates SQL analysis\n",
    "            llm_sql_analysis = bedrock_chat_completion(\n",
    "                model_id=DEFULT_LLM_MODEL_ID,\n",
    "                prompt=qna_sys_prompt.format(query_results=sql_results, question=question)\n",
    "            )\n",
    "\n",
    "        print(f\"\\nLLM SQL Analysis: \\n{llm_sql_analysis}\")\n",
    "\n",
    "        return llm_sql_analysis\n",
    "    except Exception as e:\n",
    "        print(f\"\\nException Encountered: \\n{e}\\n\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af419b-9bdd-4899-a187-c5865f71a9f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Embedding Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb825ee-8996-433b-b879-0f2581055ec4",
   "metadata": {},
   "source": [
    "The Class `AmazonBedrockEmbeddingFunction` initializes an embedding function with `Amazon Titan Embedding Model V2` that integrates with ChromaDB . This class can be further extended to add support for other embedding models available on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df8dfb0a-0b02-444b-b8a1-67ec98772a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AmazonBedrockEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        session: \"boto3.Session\",\n",
    "        model_name: str = \"amazon.titan-embed-text-v2:0\",\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Initialize AmazonBedrockEmbeddingFunction.\n",
    "\n",
    "        Args:\n",
    "            session (boto3.Session): The boto3 session to use.\n",
    "            model_name (str, optional): Identifier of the model, defaults to \"amazon.titan-embed-text-v1\"\n",
    "            **kwargs: Additional arguments to pass to the boto3 client.\n",
    "\n",
    "        Example:\n",
    "            >>> import boto3\n",
    "            >>> session = boto3.Session(profile_name=\"profile\", region_name=\"us-east-1\")\n",
    "            >>> bedrock = AmazonBedrockEmbeddingFunction(session=session)\n",
    "            >>> texts = [\"Hello, world!\", \"How are you?\"]\n",
    "            >>> embeddings = bedrock(texts)\n",
    "        \"\"\"\n",
    "\n",
    "        self._model_name = model_name\n",
    "\n",
    "        self._client = session.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        accept = \"*/*\"\n",
    "        content_type = \"application/json\"\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            input_body = {\"inputText\": text, \"dimensions\": 512, \"normalize\": True}\n",
    "            body = json.dumps(input_body)\n",
    "            response = self._client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=self._model_name,\n",
    "                accept=accept,\n",
    "                contentType=content_type,\n",
    "            )\n",
    "            embedding = json.load(response.get(\"body\")).get(\"embedding\")\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47479bba-58f5-4906-89d7-9eb688588192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AmazonSageMakerEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Initialize AmazonSageMakerEmbeddingFunction.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional arguments to pass to the sagemaker embedding function.\n",
    "\n",
    "        Example:\n",
    "            >>> sagemaker = AmazonBedrockEmbeddingFunction()\n",
    "            >>> text_inputs = [\"Hello, world!\", \"How are you?\"]\n",
    "            >>> embeddings = sagemaker(texts)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            input_body = {\"text_inputs\": text, \"mode\": \"embedding\"}\n",
    "            body = json.dumps(input_body).encode('utf-8')\n",
    "            response = embedding_predictor.predict(\n",
    "                body,\n",
    "                {\n",
    "                    \"ContentType\": content_type,\n",
    "                    \"Accept\": accept,\n",
    "                }\n",
    "            )\n",
    "            embedding = response.get(\"embedding\")\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366f9d5-f0ca-4008-a8be-04ab47b22779",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Few-shot text-to-SQL powered by ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73cdbb-e062-4d04-981d-ae92ec7e71fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use ChromaDB and the few-shot technique to retrieve table schemas for better performance and generalization across different databases and query types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765c200-7f94-4044-97ca-4681d5058d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schema Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf41750-066b-40f5-a4f2-407acec7da60",
   "metadata": {},
   "source": [
    "In this approach, we will store only the table schemas in ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70751dc4-6b3d-45b8-838d-908cd26684ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Data Preprocessing\n",
    "\n",
    "The first step is to preprocess the data and create a document that will be ingested into ChromaDB. The final doc clearly separates the table schemas by using XML tags such as `<table_schema></table_schema>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f8888c-2a37-4a0c-8949-baaa24815ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For MySQL\n",
    "# The doc includes a structure format for clearly identifying the table schemas\n",
    "doc1 = \"<table_schemas>\\n\"\n",
    "doc1 += f\"<table_schema>\\n {settings_airplanes['table_schema']} \\n</table_schema>\\n\".strip()\n",
    "doc1 += \"\\n</table_schemas>\"\n",
    "\n",
    "# The doc includes a structure format for clearly identifying the table schemas\n",
    "doc2 = \"<table_schemas>\\n\"\n",
    "doc2 += f\"<table_schema>\\n {settings_flights['table_schema']} \\n</table_schema>\\n\".strip()\n",
    "doc2 += \"\\n</table_schemas>\"\n",
    "\n",
    "# The doc includes a structure format for clearly identifying the table schemas\n",
    "doc3 = \"<table_schemas>\\n\"\n",
    "for table_schema in settings_airplane_flights['table_schemas']:\n",
    "    doc3 += f\"<table_schema>\\n {table_schema} \\n</table_schema>\\n\"\n",
    "doc3 += \"\\n</table_schemas>\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad09a7-e39e-48fe-a4af-e4e573aa8d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Ingest docs into ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b088a-846e-4ca2-842f-5ff1c6897b26",
   "metadata": {},
   "source": [
    "After the data is preprocessed, the next step is to ingest all `docs` into ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cea1d14-515d-483c-83a5-ac7a5d0f0c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = None\n",
    "\n",
    "# Setup Chroma in-memory, for easy prototyping.\n",
    "chroma_client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83c7de61-a7f8-4ebb-94a2-9f2df9060309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete collection if exists\n",
    "try:\n",
    "    chroma_client.get_collection(name=\"table-schemas-default-embedding\")\n",
    "except ValueError:\n",
    "    # Collection does not exist\n",
    "    pass\n",
    "else:\n",
    "    chroma_client.delete_collection(name=\"table-schemas-default-embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "619eb8a2-dc67-42ec-8156-408117e8253d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [00:02<00:00, 31.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# For MySQL\n",
    "# Create collection using ChromaDB's internal embedding function\n",
    "collection1 = chroma_client.get_or_create_collection(name=\"table-schemas-default-embedding\", \n",
    "                                                     metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Add docs to the collection.\n",
    "collection1.add(\n",
    "    documents=[\n",
    "        doc1,\n",
    "        doc2,\n",
    "        doc3\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"mysql\", \"database\": db_airplanes, \"table_name\": table_airplanes},\n",
    "        {\"source\": \"mysql\", \"database\": db_flights, \"table_name\": table_flights},\n",
    "        {\"source\": \"mysql\", \"database\": f\"{db_airplanes}-{db_flights}\", \"table_name\": f\"{table_airplanes}-{table_flights}\" }\n",
    "    ],\n",
    "    ids=[table_airplanes, table_flights, f\"{table_airplanes}-{table_flights}\"], # unique for each doc\n",
    ")\n",
    "\n",
    "pg_collection1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff502167-2453-4eb5-9b29-0babf1ed0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RunPrompts(DatabaseType: str, ServiceType: str, question: str, mySQLCol, pgCol):\n",
    "    \"\"\"\n",
    "    Helper function that does the following:\n",
    "    - Picks up the ChromaDB collection (passed as input parameters) based on the DatabaseType\n",
    "    - Retrieves relevant table schemas from ChromaDB based on the Business Question.\n",
    "    - Invoke the LLM to return the SQL query for the table schemas retrieved.\n",
    "    - Run the query against the database.\n",
    "    - Invoke the LLM to analyse results of the query execution against the Business Question.\n",
    "\n",
    "    Args:\n",
    "        DatabaseType (str): mysql / postgresql\n",
    "        ServiceType (str): 'Amazon Bedrock' / 'Amazon Bedrock Converse API'\n",
    "        question (str): User's business question\n",
    "        mySQLCol (object): ChromadB collection for mySQL Database\n",
    "        pgCol (object): ChromadB collection for mySQL Database\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis of the SQL query results provided by the language model.\n",
    "    \"\"\"\n",
    "    # Route the query according to the database passed\n",
    "    if DatabaseType == \"mysql\":\n",
    "        # For MySQL DB\n",
    "        collection_to_use = mySQLCol\n",
    "    elif DatabaseType == \"postgresql\":\n",
    "        # For PostgreSQL DB\n",
    "        collection_to_use = pgCol\n",
    "    \n",
    "    # Query/search 1 most similar results.\n",
    "    docs = collection_to_use.query(\n",
    "        query_texts=[question],\n",
    "        n_results=1\n",
    "    )\n",
    "\n",
    "    pattern = r\"<table_schemas>(.*)</table_schemas>\"\n",
    "    table_schemas = re.search(pattern, docs[\"documents\"][0][0], re.DOTALL).group(1)\n",
    "    print(f\"ChromaDB - Schema Retrieval: \\n{table_schemas.strip()}\")\n",
    "    \n",
    "    # Amazon Bedrock Invoke API expects a fully formatted prompt.\n",
    "    # Amazon Bedrock Converse API expects regular text\n",
    "    if (llm_selected_service == 'Amazon Bedrock') or (llm_selected_service == 'Amazon SageMaker'):\n",
    "        tmp_sql_sys_prompt1 = tmp_sql_sys_prompt\n",
    "        qna_sys_prompt1 = QNA_SYS_PROMPT\n",
    "    elif ServiceType == 'Amazon Bedrock Converse API':\n",
    "        tmp_sql_sys_prompt1 = sysPt\n",
    "        qna_sys_prompt1 = userPtI\n",
    "\n",
    "    SQL_SYS_PROMPT_s1 = PromptTemplate.from_template(tmp_sql_sys_prompt1).format(\n",
    "        question=question,\n",
    "        table_schemas=table_schemas,\n",
    "        dbtype=DatabaseType\n",
    "    )\n",
    "\n",
    "    results = get_llm_sql_analysis(\n",
    "        question=question,\n",
    "        sql_sys_prompt=SQL_SYS_PROMPT_s1,\n",
    "        qna_sys_prompt=qna_sys_prompt1,\n",
    "        DatabaseType=DatabaseType\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d7669-6cd2-485f-8d41-44fb3526c86e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Create a Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca7fb8-1d19-423d-85c2-20967b769a84",
   "metadata": {},
   "source": [
    "Here, we design our prompt template that will account for our question and answer, and formatted correctly for use with Llama 3.2 models.\n",
    "\n",
    "First, we create a `system prompt` containing two parts:\n",
    "\n",
    "1. `table_schemas`. This is a description of the structure of the database table(s), including the name of the table, the names of the columns within each table, and the data types of each column. This information helps Llama 3.2 to understand the organization and contents of the table.\n",
    "\n",
    "2. `question`. This is the specific request or information that the user wants to obtain from the table.\n",
    "\n",
    "By including both the table schema and the user's question in the system prompt, we provide Llama 3.2 model a complete understanding of the table structure and the user's desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588156e2-5b35-4828-a3ca-e077f2c542ac",
   "metadata": {},
   "source": [
    "Now, we'll use a few-shot approach using the retrieved tables from ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbb802-9437-47b5-b728-9e2e7c9a64df",
   "metadata": {},
   "source": [
    "First, we create a `system prompt` containing a placeholder including any number of table schemas for ChromaDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc0758c4-7710-43af-ad4f-e1a3123f685a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \n",
    "        \"\"\"You are a {dbtype} query expert whose output is a valid sql query.\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "It has the following schemas:\n",
    "<table_schemas>\n",
    "{table_schemas}\n",
    "</table_schemas>\n",
    "\n",
    "Always combine the database name and table name to build your queries. You must identify these two values before proving a valid SQL query.\n",
    "\n",
    "Please construct a valid SQL statement to answer the following the question, return only the {dbtype} query in between <sql></sql>.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{question}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tmp_sql_sys_prompt, sysPt, userPt = format_instructions(instructions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb0279-e526-436e-92d1-31fb37ffb795",
   "metadata": {},
   "source": [
    "Next, we create a new `system prompt` containing two parts:\n",
    "\n",
    "1. `query_results` represents the SQL query results after executing the prompt `tmp_sql_sys_prompt`. This is the raw data that Llama 3 model will use to generate its analysis.\n",
    "\n",
    "2. `question`. This specifies the type of analysis or insight that the user wants Llama 3 model to provide based on the SQL query results.\n",
    "\n",
    "By combining the SQL query results and the user's question into a single system prompt, we provide Llama 3 model all the information it needs to understand the context and provide a comprehensive analysis tailored to the user's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf5decf8-1aa9-4439-b61d-a4b745d66f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Given the following SQL query results:\n",
    "{query_results}\n",
    "\n",
    "And the original question:\n",
    "{question}\n",
    "\n",
    "Please provide an analysis and interpretation of the results to answer the original question.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "QNA_SYS_PROMPT, sysPtI, userPtI = format_instructions(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc134b9c-96de-47b7-a182-664024d0b2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Execute Few Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977f101-cd0b-4ab4-bc6e-85bf3d759f1d",
   "metadata": {},
   "source": [
    "In this example, we expect the table `airplanes` to be used for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8853827-1d81-4ef4-befc-d2f55d52314a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To answer the question, we need to identify the database name and table name. Based on the given schemas, we can assume the database name is 'transport_db' and the table name is 'flights'. \n",
      "\n",
      "Here's the SQL query to get the total count of airplanes:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights\n",
      "```\n",
      "\n",
      "This query will return the total count of rows in the 'flights' table, which corresponds to the total count of airplanes.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(20,)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "Analysis and Interpretation:\n",
      "\n",
      "The given SQL query results are:\n",
      "\n",
      "(20,)\n",
      "\n",
      "This result is likely a single row in a table, containing a single value. In this case, the value is 20.\n",
      "\n",
      "The original question asks for the total count of airplanes.\n",
      "\n",
      "Interpretation:\n",
      "\n",
      "Based on the given result, it appears that there is a total count of 20 airplanes. This suggests that the table contains data about airplanes, and the count of 20 represents the total number of airplanes in the dataset.\n",
      "\n",
      "However, without more context or information about the table structure and the data, it's difficult to provide a more detailed interpretation. It's possible that the table contains additional columns or rows that provide more information about the airplanes, but the count of 20 is the only relevant information provided in the query results.\n",
      "\n",
      "Answer to the original question:\n",
      "\n",
      "The total count of airplanes is 20.\n",
      "CPU times: user 114 ms, sys: 0 ns, total: 114 ms\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of airplanes?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection1, pg_collection1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28a93445-5897-465a-aacc-8e4856dffb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To get the total count of flights, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights;\n",
      "```\n",
      "\n",
      "This query will return the total count of flights in the `transport_db` database.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights;\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(20,)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "**Analysis and Interpretation**\n",
      "\n",
      "The given SQL query results are:\n",
      "\n",
      "(20,)\n",
      "\n",
      "This result set contains a single row with two columns: `flight_id` and `count`.\n",
      "\n",
      "**Interpretation**\n",
      "\n",
      "The `flight_id` column is likely a unique identifier for each flight, and the `count` column represents the number of flights that have taken off from that airport.\n",
      "\n",
      "**Analysis**\n",
      "\n",
      "The result set indicates that there is only one flight that has taken off from the given airport. This suggests that the airport has only one flight scheduled for the given day.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the analysis, the original question \"What is the total count of flights?\" is answered by the single row in the result set, which contains a single flight with a count of 1.\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "Suppose we have a database with a table called `flights` that stores information about each flight, including the flight_id, departure_airport, arrival_airport, and departure_time. We can use the result set to answer the original question by joining the `flights` table with the `flights` table on the departure_airport column and counting the number of rows in the result set.\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_flights\n",
      "FROM flights\n",
      "WHERE departure_airport = 'ABC';\n",
      "```\n",
      "\n",
      "This query will return the total count of flights that depart from airport 'ABC'.\n",
      "CPU times: user 97.9 ms, sys: 7.28 ms, total: 105 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of flights?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection1, pg_collection1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bf51591-ba4f-43b6-9c63-e70df33b578f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "```sql\n",
      "SELECT \n",
      "    Producer, \n",
      "    COUNT(*) as Total_count\n",
      "FROM \n",
      "    transport_db.flights\n",
      "GROUP BY \n",
      "    Producer\n",
      "ORDER BY \n",
      "    Total_count DESC;\n",
      "```\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT \n",
      "    Producer, \n",
      "    COUNT(*) as Total_count\n",
      "FROM \n",
      "    transport_db.flights\n",
      "GROUP BY \n",
      "    Producer\n",
      "ORDER BY \n",
      "    Total_count DESC;\n",
      "\n",
      "\n",
      "Exception Encountered: \n",
      "1054 (42S22): Unknown column 'Producer' in 'field list'\n",
      "\n",
      "CPU times: user 107 ms, sys: 0 ns, total: 107 ms\n",
      "Wall time: 403 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of flights per producer?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection1, pg_collection1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d524f72-c687-49a0-8dbc-21476fbcedcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 5: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2970425-61a7-489d-b133-3cbf0d6e85b9",
   "metadata": {},
   "source": [
    "We may observe that ChromaDB is unable to retrieve the correct table schema for the \"airplane\" table. The issue arose due to a confusion caused by a foreign key reference. Specifically, ChromaDB retrieved the \"flights\" table instead of the \"airplanes\" table because the \"flights\" table contains a field called \"Airplane_id\" which references the \"airplanes\" table as a foreign key. This foreign key reference led to the confusion, resulting in ChromaDB retrieving the wrong table.\n",
    "\n",
    "To mitigate this issue, we will use a more robust embedding model like the `Amazon Titan Embedding Model`.\n",
    "\n",
    "We will see this in action in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a5731-2f4c-46f0-b03a-c417338064ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enhanced Schema Retrieval with an Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c58d4-9330-438a-9aa6-72340fdc4b64",
   "metadata": {},
   "source": [
    "In this approach, we will use an embedding model from AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee69c27-b639-4380-9f32-356f18b55584",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Ingest docs into ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746786d7-8ab0-4b99-b57e-efdba7f1131c",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the data is preprocessed, the next step is to ingest all `docs` into ChromaDB using your selected embedding model (`Amazon SageMaker BGE Large English` or `Amazon Titan Embedding V2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ea54fec-5074-44bc-bff5-b5b1e7e4a4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embedding function with AWS\n",
    "if embedding_selected_service == \"Amazon SageMaker\":\n",
    "    aws_ef = AmazonSageMakerEmbeddingFunction()\n",
    "else:\n",
    "    session = boto3.Session()\n",
    "    aws_ef = AmazonBedrockEmbeddingFunction(\n",
    "        session=session,\n",
    "        model_name=DEFAULT_EMBEDDING_MODEL_ID\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52b83b48-c84d-4740-92f0-a552c8026860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete collection if exists\n",
    "try:\n",
    "    chroma_client.get_collection(name=\"table-schemas-aws-embedding-model\")\n",
    "except ValueError:\n",
    "    # Collection does not exist\n",
    "    pass\n",
    "else:\n",
    "    chroma_client.delete_collection(name=\"table-schemas-aws-embedding-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3ca323c-68c2-41cb-afbc-03fece3db46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create collection using Amazon Titan Embedding model\n",
    "collection2 = chroma_client.get_or_create_collection(name=\"table-schemas-aws-embedding-model\", \n",
    "                                                     embedding_function=aws_ef, \n",
    "                                                     metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "collection2.add(\n",
    "    documents=[\n",
    "        doc1,\n",
    "        doc2,\n",
    "        doc3\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"mysql\", \"database\": db_airplanes, \"table_name\": table_airplanes},\n",
    "        {\"source\": \"mysql\", \"database\": db_flights, \"table_name\": table_flights},\n",
    "        {\"source\": \"mysql\", \"database\": f\"{db_airplanes}-{db_flights}\", \"table_name\": f\"{table_airplanes}-{table_flights}\" }\n",
    "    ],\n",
    "    ids=[table_airplanes, table_flights, f\"{table_airplanes}-{table_flights}\"], # unique for each doc\n",
    ")\n",
    "\n",
    "pg_collection2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec741334-b2ac-4d27-b11e-ee365dadf0fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Execute Few Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265c0a3-40d9-482b-98ff-1077b358cecd",
   "metadata": {},
   "source": [
    "In this example, we expect the table `airplanes` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2b9a734-4898-4e00-9df1-af448acb5424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To get the total count of airplanes, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) \n",
      "FROM equipment_db.airplanes;\n",
      "```\n",
      "\n",
      "This query will return the total count of airplanes in the `airplanes` table.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM equipment_db.airplanes;\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(20,)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "**Analysis and Interpretation**\n",
      "\n",
      "The given SQL query results are:\n",
      "\n",
      "(20,)\n",
      "\n",
      "This result set contains a single row with two columns: `id` and `count`. The `id` column is not provided, but based on the context, it's likely a column representing the airplane ID. The `count` column represents the total count of airplanes.\n",
      "\n",
      "**Interpretation**\n",
      "\n",
      "The result set indicates that there is only one airplane, and the total count of airplanes is 1.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the provided query results, the original question \"What is the total count of airplanes?\" is answered as follows:\n",
      "\n",
      "The total count of airplanes is 1.\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "This result can be used in various scenarios, such as:\n",
      "\n",
      "* Displaying the total count of airplanes in a dashboard or report.\n",
      "* Using the result as a starting point for further analysis or calculations.\n",
      "* Providing a clear and concise answer to the original question.\n",
      "\n",
      "**Code Example**\n",
      "\n",
      "Here's an example of how you can use this result in a SQL query to display the total count of airplanes:\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_airplanes\n",
      "FROM airplanes;\n",
      "```\n",
      "This query will return the total count of airplanes, which is 1 in this case.\n",
      "CPU times: user 18.1 ms, sys: 543 Î¼s, total: 18.6 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of airplanes?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "969ca9b6-a2af-4864-9281-9fc88e754452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To get the total count of flights, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights;\n",
      "```\n",
      "\n",
      "This query will return the total count of flights in the `transport_db` database.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM transport_db.flights;\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(20,)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "**Analysis and Interpretation**\n",
      "\n",
      "The given SQL query results are:\n",
      "\n",
      "(20,)\n",
      "\n",
      "This result set contains a single row with two columns: `flight_id` and `count`.\n",
      "\n",
      "**Interpretation**\n",
      "\n",
      "The `flight_id` column is likely a unique identifier for each flight, and the `count` column represents the number of flights that have taken off from that airport.\n",
      "\n",
      "**Analysis**\n",
      "\n",
      "The result set indicates that there is only one flight that has taken off from the given airport. This suggests that the airport has only one flight scheduled for the given day.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the analysis, the original question \"What is the total count of flights?\" is answered by the single row in the result set, which contains a single flight with a count of 1.\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "Suppose we have a database with a table called `flights` that stores information about each flight, including the flight_id, departure_airport, arrival_airport, and departure_time. We can use the result set to answer the original question by joining the `flights` table with the `flights` table on the departure_airport column and counting the number of rows in the result set.\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_flights\n",
      "FROM flights\n",
      "WHERE departure_airport = 'ABC';\n",
      "```\n",
      "\n",
      "This query will return the total count of flights that depart from airport 'ABC'.\n",
      "CPU times: user 10.1 ms, sys: 3.43 ms, total: 13.6 ms\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of flights?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98128cd2-4164-4c8e-a534-d41521a9390a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To answer the question, we need to join the `airplanes` table with the `flights` table based on the `Airplane_id` column. We will then group the results by the `Producer` column and count the number of flights for each producer.\n",
      "\n",
      "Here is the SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    e.Producer, \n",
      "    COUNT(f.Flight_number) AS Total_flights\n",
      "FROM \n",
      "    equipment_db.airplanes e\n",
      "JOIN \n",
      "    transport_db.flights f ON e.Airplane_id = f.Airplane_id\n",
      "GROUP BY \n",
      "    e.Producer\n",
      "```\n",
      "\n",
      "This query will return a result set with the producer name and the total count of flights for each producer.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT \n",
      "    e.Producer, \n",
      "    COUNT(f.Flight_number) AS Total_flights\n",
      "FROM \n",
      "    equipment_db.airplanes e\n",
      "JOIN \n",
      "    transport_db.flights f ON e.Airplane_id = f.Airplane_id\n",
      "GROUP BY \n",
      "    e.Producer\n",
      "\n",
      "\n",
      "sql_results = \n",
      "('Boeing', 4)\n",
      "('Airbus', 8)\n",
      "('Embraer', 5)\n",
      "('Bombardier', 3)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "Analysis and Interpretation:\n",
      "\n",
      "The given SQL query results represent the count of flights for each aircraft producer. The results are:\n",
      "\n",
      "('Boeing', 4)\n",
      "('Airbus', 8)\n",
      "('Embraer', 5)\n",
      "('Bombardier', 3)\n",
      "\n",
      "To answer the original question, \"What is the total count of flights per producer?\", we need to calculate the total count of flights for each producer.\n",
      "\n",
      "Here's the calculation:\n",
      "\n",
      "- Boeing: 4 flights\n",
      "- Airbus: 8 flights\n",
      "- Embraer: 5 flights\n",
      "- Bombardier: 3 flights\n",
      "\n",
      "Total count of flights per producer is the sum of the individual counts:\n",
      "\n",
      "4 (Boeing) + 8 (Airbus) + 5 (Embraer) + 3 (Bombardier) = 20 flights\n",
      "\n",
      "Therefore, the total count of flights per producer is 20.\n",
      "CPU times: user 15.8 ms, sys: 0 ns, total: 15.8 ms\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"What is the total count of flights per producer?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074476e-0484-484d-92ac-e3a041672d42",
   "metadata": {},
   "source": [
    "For this example, we expect the table `airplane` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0a9fffb-4ebb-4c89-a41f-6e74dc214bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "```sql\n",
      "SELECT COUNT(DISTINCT Producer) AS unique_producers\n",
      "FROM equipment_db.airplanes;\n",
      "```\n",
      "\n",
      "This SQL query will return the number of unique airplane producers represented in the database.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT COUNT(DISTINCT Producer) AS unique_producers\n",
      "FROM equipment_db.airplanes;\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(4,)\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "**Analysis and Interpretation**\n",
      "\n",
      "The given SQL query results are:\n",
      "\n",
      "(4,)\n",
      "\n",
      "This result set contains a single row with two columns: `id` and `name`. The `id` column is an integer, and the `name` column is a string.\n",
      "\n",
      "**Interpretation**\n",
      "\n",
      "The `id` column is an integer, which means it represents a unique identifier for each airplane producer. The `name` column is a string, which represents the name of the airplane producer.\n",
      "\n",
      "**Question Answer**\n",
      "\n",
      "The original question asks how many unique airplane producers are represented in the database. Based on the query results, we can conclude that there is only one unique airplane producer represented in the database.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The query results indicate that there is only one airplane producer in the database, represented by the single row with `id` = 4 and `name` = (no value). Therefore, the answer to the original question is:\n",
      "\n",
      "There is only one unique airplane producer represented in the database.\n",
      "CPU times: user 10.9 ms, sys: 3.33 ms, total: 14.3 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"How many unique airplane producers are represented in the database?\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab7a3-bb1b-4f1b-82d1-4e02f8d40848",
   "metadata": {},
   "source": [
    "For this example, we expect the table `flights` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd806b74-efe4-4701-8a3e-480e8a456074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "```sql\n",
      "SELECT \n",
      "    Destination, \n",
      "    COUNT(*) as Total_Flights, \n",
      "    SUM(CASE WHEN Arrival_date = Arrival_time THEN 1 ELSE 0 END) as Total_Scheduled_Flights\n",
      "FROM \n",
      "    transport_db.flights\n",
      "GROUP BY \n",
      "    Destination, \n",
      "    Arrival_date\n",
      "ORDER BY \n",
      "    Total_Scheduled_Flights DESC;\n",
      "```\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT \n",
      "    Destination, \n",
      "    COUNT(*) as Total_Flights, \n",
      "    SUM(CASE WHEN Arrival_date = Arrival_time THEN 1 ELSE 0 END) as Total_Scheduled_Flights\n",
      "FROM \n",
      "    transport_db.flights\n",
      "GROUP BY \n",
      "    Destination, \n",
      "    Arrival_date\n",
      "ORDER BY \n",
      "    Total_Scheduled_Flights DESC;\n",
      "\n",
      "\n",
      "sql_results = \n",
      "('Orlando', 1, Decimal('0'))\n",
      "('Salt Lake City', 1, Decimal('0'))\n",
      "('Denver', 1, Decimal('0'))\n",
      "('Chicago', 1, Decimal('0'))\n",
      "('Minneapolis', 1, Decimal('0'))\n",
      "('Houston', 1, Decimal('0'))\n",
      "('New York', 1, Decimal('0'))\n",
      "('Detroit', 1, Decimal('0'))\n",
      "('Dallas', 1, Decimal('0'))\n",
      "('San Francisco', 1, Decimal('0'))\n",
      "('Los Angeles', 1, Decimal('0'))\n",
      "('Boston', 1, Decimal('0'))\n",
      "('Miami', 1, Decimal('0'))\n",
      "('San Diego', 1, Decimal('0'))\n",
      "('Philadelphia', 1, Decimal('0'))\n",
      "('Las Vegas', 1, Decimal('0'))\n",
      "('Phoenix', 1, Decimal('0'))\n",
      "('Seattle', 1, Decimal('0'))\n",
      "('Atlanta', 1, Decimal('0'))\n",
      "('Tampa', 1, Decimal('0'))\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "**Analysis and Interpretation of SQL Query Results**\n",
      "\n",
      "The provided SQL query results are a list of flight schedules for various destinations, grouped by arrival date. The results are in the following format:\n",
      "\n",
      "| Destination | Arrival Date | Number of Flights |\n",
      "| --- | --- | --- |\n",
      "| Orlando | 2022-01-01 | 1 |\n",
      "| Orlando | 2022-01-02 | 1 |\n",
      "| Orlando | 2022-01-03 | 1 |\n",
      "| ... | ... | ... |\n",
      "| Orlando | 2022-12-31 | 1 |\n",
      "| Salt Lake City | 2022-01-01 | 1 |\n",
      "| Salt Lake City | 2022-01-02 | 1 |\n",
      "| Salt Lake City | 2022-01-03 | 1 |\n",
      "| ... | ... | ... |\n",
      "| Houston | 2022-01-01 | 1 |\n",
      "| Houston | 2022-01-02 | 1 |\n",
      "| Houston | 2022-01-03 | 1 |\n",
      "| ... | ... | ... |\n",
      "| New York | 2022-01-01 | 1 |\n",
      "| New York | 2022-01-02 | 1 |\n",
      "| New York | 2022-01-03 | 1 |\n",
      "| ... | ... | ... |\n",
      "\n",
      "**Interpretation of Results**\n",
      "\n",
      "The results indicate that there are 12 destinations with 1 flight scheduled for each destination on January 1st, 2022. This suggests that these destinations are likely to be popular tourist spots or have a high demand for flights.\n",
      "\n",
      "The results also show that the number of flights scheduled for each destination decreases by 1 on January 2nd, 2022, and continues to decrease by 1 on each subsequent day. This indicates that the demand for flights decreases over time, likely due to the fact that the initial flights have already taken place.\n",
      "\n",
      "**Analysis of Flight Schedules**\n",
      "\n",
      "The results can be analyzed further to gain insights into the flight schedules. For example:\n",
      "\n",
      "* The destinations with the highest number of flights on January 1st, 2022, are likely to be popular tourist spots, such as Orlando and Salt Lake City.\n",
      "* The destinations with the highest number of flights on January 2nd, 2022, are likely to be destinations with a high demand for flights, such as New York and Houston.\n",
      "* The destinations with the highest number of flights on January 3rd, 2022, are likely to\n",
      "CPU times: user 12.1 ms, sys: 2.15 ms, total: 14.3 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"Get the total number of flights scheduled for each destination, grouped by arrival date\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199367e-7117-4553-b2f6-47073cb17769",
   "metadata": {},
   "source": [
    "For this example, we expect the table `airplanes` and `flights` to be included for the SQL llm analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a62f3823-367d-47a1-a0de-e4babe42ef08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB - Schema Retrieval: \n",
      "<table_schema>\n",
      " CREATE TABLE equipment_db.airplanes -- Table name\n",
      "(\n",
      "Airplane_id INT(10), -- airplane id\n",
      "Producer VARCHAR(20), -- name of the producer\n",
      "Type VARCHAR(10), -- airplane type\n",
      "PRIMARY KEY (Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "<table_schema>\n",
      " CREATE TABLE transport_db.flights -- Table name\n",
      "(\n",
      "Flight_number VARCHAR(10), -- flight id\n",
      "Arrival_time VARCHAR(20), -- arrival time (YYYY-MM-DDTH:M:S)\n",
      "Arrival_date VARCHAR(20), -- arrival date (YYYY-MM-DD)\n",
      "Departure_time VARCHAR(20), -- departure time (YYYY-MM-DDTH:M:S)\n",
      "Departure_date VARCHAR(20), -- departure date (YYYY-MM-DD)\n",
      "Destination VARCHAR(20), -- destination\n",
      "Airplane_id INT(10), -- airplane id\n",
      "PRIMARY KEY (Flight_number),\n",
      "FOREIGN KEY (Airplane_id) REFERENCES equipment_db.airplanes(Airplane_id)\n",
      ")\n",
      " \n",
      "</table_schema>\n",
      "\n",
      "Using [Amazon Bedrock Converse API] to generate SQL for [mysql]\n",
      "\n",
      "completion = \n",
      "To answer this question, we need to join the `airplanes` table with the `flights` table based on the `Airplane_id` column. We also need to filter the results to include only flights that have flown to New York.\n",
      "\n",
      "Here's the SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    e.Airplane_id, \n",
      "    e.Producer\n",
      "FROM \n",
      "    equipment_db.airplanes e\n",
      "INNER JOIN \n",
      "    transport_db.flights f ON e.Airplane_id = f.Airplane_id\n",
      "WHERE \n",
      "    f.Destination = 'New York'\n",
      "```\n",
      "\n",
      "This query will return the `Airplane_id` and `Producer` for all airplanes that have flown to New York.\n",
      "\n",
      "\n",
      "LLM SQL Query: \n",
      "\n",
      "SELECT \n",
      "    e.Airplane_id, \n",
      "    e.Producer\n",
      "FROM \n",
      "    equipment_db.airplanes e\n",
      "INNER JOIN \n",
      "    transport_db.flights f ON e.Airplane_id = f.Airplane_id\n",
      "WHERE \n",
      "    f.Destination = 'New York'\n",
      "\n",
      "\n",
      "sql_results = \n",
      "(6, 'Airbus')\n",
      "\n",
      "Calling LLM on [Amazon Bedrock Converse API] to Analyze and interpret the results from SQL query in relation to the original question.\n",
      "\n",
      "\n",
      "LLM SQL Analysis: \n",
      "Based on the provided SQL query results, we can analyze and interpret the data to answer the original question.\n",
      "\n",
      "The results show that there is only one airplane with ID 6, which is produced by Airbus.\n",
      "\n",
      "However, the original question asks for airplanes that have flown to New York. Unfortunately, the provided results do not contain any information about the flight destinations of the airplanes.\n",
      "\n",
      "To answer the original question, we would need additional data, such as a table that contains flight information, including the destination of each flight. Without this data, we cannot determine which airplanes have flown to New York.\n",
      "\n",
      "If we were to assume that the provided results are part of a larger dataset that includes flight information, we could potentially join the two tables to answer the original question. However, based solely on the provided results, we cannot provide a definitive answer.\n",
      "CPU times: user 13.5 ms, sys: 1.7 ms, total: 15.2 ms\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Business question\n",
    "question = \"Find the airplane IDs and producers for airplanes that have flown to New York\"\n",
    "\n",
    "RunPrompts(llm_selected_db, llm_selected_service, question, collection2, pg_collection2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5c40f-5e79-4b0a-9c0a-fa16b8195b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b500a5c-16cc-4d23-9d06-8666928daa6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 Î¼s, total: 4 Î¼s\n",
      "Wall time: 5.48 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Delete resources\n",
    "if llm_selected_service == 'Amazon SageMaker':\n",
    "    llm_predictor.delete_model()\n",
    "    llm_predictor.delete_endpoint()\n",
    "\n",
    "if embedding_selected_service == 'Amazon SageMaker':\n",
    "    embedding_predictor.delete_model()\n",
    "    embedding_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce768358-c208-4c81-acff-ecb39b904b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "1. We can observe that ChromaDB and `Amazon Titan Embedding` model were able to retrieve the correct table schemas for the previous examples based on a natural language query.\n",
    "2. We can see that by using `Amazon Titan Embedding` model, we can get a more specific number of tables pertaining to our natural language query.\n",
    "2. We may also observe that in some cases the LLM is not able to provide a proper query. This could be potentially due to the 1b varient being used. You can experiment with changing the LLM to one that has a higher number of parameters e.g., 3b or 11b or 90b and notice that such concerns are alleviated.\n",
    "3. We may also observe that at times, the running the sequence once may not yield an expected results, but if we run the same sequence again, we get a proper result. Consider incorporating a retry mechanism.\n",
    "4. Consider incorporating an SQL query error correction mechanism as well.\n",
    "5. Experiment with including additional metadata within the Chroma DB metadata embeddings store to define relationships between database objects. This will provide flexibility for further complex scenarios such as: Stored Procedures, Triggers, User Defined Functions etc.\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
