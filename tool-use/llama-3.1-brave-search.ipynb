{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function Calling with Llama 3.1 Models**\n",
    "Welcome to this notebook exploring function calling capabilities with Meta's Llama 3.1 models. Llama 3.1, the latest iteration in the Llama family, introduces native support for function calling, marking a significant advancement in the model's capabilities and potential applications.\n",
    "\n",
    "![Llamas](imgs/llama-pic.jpeg)\n",
    "\n",
    "### **What's New in Llama 3.1?**\n",
    "Llama 3.1 brings several exciting improvements:\n",
    "- Native Function Calling: Built-in support for generating structured JSON outputs that can be used with various APIs.\n",
    "- Multilingual Support: Expanded language understanding across 8 languages, including English, French, German, Hindi, Italian Portuguese, Spanish, and Thai.\n",
    "- Improved Performance: Benchmarking in the GPT-4+ class, competitive with both GPT-4 and Claude 3.5 Sonnet.\n",
    "- Increased Context Window: The 8B and 70B models now support a 128,000 token context.\n",
    "\n",
    "### **Function Calling Overview**\n",
    "Function calling, also known as tool use, allows Llama 3.1 to interact with external tools or APIs by generating structured outputs. This capability enables more complex, multi-step interactions between the model and available tools, enhancing its problem-solving abilities and practical applications.\n",
    "\n",
    "### **Amazon Bedrock**\n",
    "![bedrock](imgs/bedrock-img.png)\n",
    "\n",
    "Amazon Bedrock is a fully managed service that provides access to a wide range of powerful foundation models (FMs) through a unified API. It offers models from leading AI companies like Mistral, Anthropic, AI21 Labs, Cohere, Stability AI, and Amazon's own Titan models.\n",
    "\n",
    "- **Unified API**: Bedrock provides a single API endpoint to access different models, simplifying integration and allowing developers to experiment with or switch between models with minimal code changes.\n",
    "- **Serverless and Fully Managed**: As a fully managed service, Bedrock eliminates the need for users to handle infrastructure management, making it easier to build and deploy generative AI applications.\n",
    "- **Model Customization**: Users can customize models with their own data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG).\n",
    "- **Security and Privacy**: Bedrock offers built-in security features, ensuring data privacy and compliance with various standards. It follows best practices like encrypting all data and preventing third parties from accessing user data.\n",
    "- **Agents for Complex Tasks**: Bedrock allows the creation of agents that can plan and execute multi-step tasks using enterprise systems and data sources.\n",
    "- **Integration with AWS Services**: Bedrock can be easily integrated with other AWS services and existing applications.\n",
    "- **Model Evaluation**: A new capability that helps customers assess, compare, and select the best model for their application.\n",
    "- **Guardrails**: Bedrock provides tools to implement safeguards tailored to application needs and aligned with responsible AI policies.\n",
    "- **Custom Model Import**: A new feature that allows customers to import and access their own custom models as a fully managed API in Bedrock.\n",
    "- **Playground Environment**: Bedrock offers Playgrounds (text, chat, and image) to compare models and experiment with different options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview in the Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Amazon Bedrock chat playground with Llama 3.1 70b Instruct model as a quick way to demonstrate function calling.  There are two ways of invoking function calling: one is the ```Python``` way and the other is the ```JSON``` way.  The essential difference between the two is that in the ```Python``` method, the Llama 3.1 models have been trained to recognize a small number of functions natively.  These functions are: ```wolfram_alpha```, ```brave_search``` and ```code_interpreter```.  In other words the models have been trained to recognize when they should call and how to call functions for computation, internet search and code.  They also have been trained to respond in a **zero shot** fashion to tool definitions they have not seen before.  This is the ```JSON``` method.  Both methods will be demonstrated in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "![Python function calling](imgs/python-tool-calling-chat-playground.png)\n",
    "Note the use of the system prompt and the parsimonious nature of the declaration to make the model aware of the tools it can use.  Note also the format of the response, including the new special tag ```<|python_tag|>``` which signifies a tool call.\n",
    "\n",
    "### JSON\n",
    "\n",
    "![JSON function calling](imgs/json-tool-calling-chat-playground.png)\n",
    "Note the use of the user prompt to make the model aware of the tools it can use.  It can be placed in the system prompt too and developers will have to test which is most effective.  Note also the format of the response.\n",
    "\n",
    "At the time of developing this notebook the chat playgrounds used the ```ConverseStream``` API's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brave Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brave Search is an independent browser which does not track user activity.  Brave Search API's have free tiers, although you must provide payment details. In order to send request to the API, we need to create an account and get the API token.\n",
    "\n",
    "1) To get API key we will first register for an account [Register for an Account](https://api.search.brave.com/register). \n",
    "2) After the account is created, you can navigate to the [Subscriptions page](https://api.search.brave.com/app/subscriptions/subscribe?tab=ai) where you should select the **Free AI** subscription.  You are required to enter payment details.\n",
    "3) Having created a subscription then navigate to the [API Keys](https://api.search.brave.com/app/keys) and generate an API Key, selecting the **Free AI** subscription you just created. After that you will have access to a token, which you will use for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To begin with, let's check that we can call the Brave Search API and familiarise ourselves with the structure of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3==1.35.11 colorama==0.4.6 brave-search==0.1.8 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import boto3\n",
    "import getpass\n",
    "import requests\n",
    "import urllib.parse\n",
    "from PIL import Image\n",
    "from colorama import Fore\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from brave import Brave\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before you continue, please add your Brave Search API token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = getpass.getpass('Please enter your brave-search-api-token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's ask a question where the knowledge cannot be in the model's training data, in other words, something that has changed recently\n",
    "question = \"What are the names of the candidates who will contest the US Presidential Election on Tuesday November 5 2024?\"\n",
    "\n",
    "print(f\"{Fore.YELLOW}Question: {question}\")\n",
    "\n",
    "query = urllib.parse.quote_plus(question)\n",
    "safesearch=urllib.parse.quote_plus('strict')\n",
    "limit_results = 2\n",
    "query_url = f\"https://api.search.brave.com/res/v1/web/search?q={query}&count={limit_results}&safesearch={safesearch}\"\n",
    "headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Accept-Encoding\": \"gzip\",\n",
    "        \"X-Subscription-Token\": api_token\n",
    "    }\n",
    "\n",
    "request = requests.get(query_url, headers=headers)\n",
    "content = request.content\n",
    "\n",
    "# Check if the content starts with the gzip magic number\n",
    "if content.startswith(b'\\x1f\\x8b'):\n",
    "    try:\n",
    "        decompressed_response = gzip.decompress(content)\n",
    "    except gzip.BadGzipFile:\n",
    "        # If decompression fails, use the original content\n",
    "        decompressed_response = content\n",
    "else:\n",
    "    # If it's not gzip, use the original content\n",
    "    decompressed_response = content\n",
    "\n",
    "# Parse the JSON response\n",
    "try:\n",
    "    search_results = json.loads(decompressed_response)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON response\")\n",
    "    search_results = None\n",
    "\n",
    "# Check if we have valid results\n",
    "if search_results and 'web' in search_results and 'results' in search_results['web']:\n",
    "    # Iterate through the search results\n",
    "    for result in search_results['web']['results']:\n",
    "        # Access different fields of each result\n",
    "        title = result.get('title', 'No title')\n",
    "        url = result.get('url', 'No URL')\n",
    "        description = result.get('description', 'No description')\n",
    "\n",
    "        # Print or process the data as needed\n",
    "        print(f\"{Fore.LIGHTBLUE_EX}==================\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Description: {description}\")\n",
    "\n",
    "else:\n",
    "    print(f\"{Fore.LIGHTRED_EX}===============\")\n",
    "    print(\"No valid search results found in the response\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a Python package for the Brave Search API which helps to make the request and parse the results.  The following code snippet shows the use of that library to display the same information as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brave = Brave(api_token)\n",
    "\n",
    "question = \"What are the names of the candidates who will contest the US Presidential Election on Tuesday November 5 2024?\"\n",
    "\n",
    "print(f\"{Fore.YELLOW}Question: {question}\")\n",
    "\n",
    "limit_results = 2\n",
    "\n",
    "search_results = brave.search(q=question, count=limit_results)\n",
    "\n",
    "for result in search_results.web_results:\n",
    "    # Access different fields of each result\n",
    "    title = result[\"title\"]\n",
    "    url = result[\"url\"]\n",
    "    description = result[\"description\"]\n",
    "\n",
    "    # Print or process the data as needed\n",
    "    print(f\"{Fore.LIGHTBLUE_EX}==================\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Description: {description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the Web Search can be found here: https://api.search.brave.com/app/documentation/web-search/get-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating into Bedrock\n",
    "\n",
    "Now that we know how to use the Brave Search API we can integrate it into Amazon Bedrock using the [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html). We will do this in 2 ways. Firstly, we will use the ```function``` calling capability inherent in the Llama 3.1 models.  We've previously referred to this as the ```Python``` method.  Secondly, we will use the Bedrock ```tool``` calling method and rely on the **zero shot** capability of the model.  We've previously referred to this as the ```JSON``` method.\n",
    "\n",
    "The essential difference is how the LLM notifies the executor that it would like to call a function and then how the executor threads the function call completion back into the conversation.\n",
    "\n",
    "Let's go ahead and set up the conversation with the model in Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = 'meta.llama3-1-70b-instruct-v1:0'\n",
    "# modelId = 'meta.llama3-1-405b-instruct-v1:0'\n",
    "# modelId = 'meta.llama3-1-8b-instruct-v1:0'\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Function Calling - Python Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the names of the candidates who will contest the US Presidential Election on Tuesday November 5 2024?\"\n",
    "system_prompt = \"Environment: ipython /r/n \" \\\n",
    "                \"Tools: brave_search /r/n\" \\\n",
    "                \"Cutting Knowledge Date: December 2023 /r/n\" \\\n",
    "                \"Today Date: 23 July 2024 /r/n\" \\\n",
    "                \"You are a helpful assistant.\" \n",
    "    \n",
    "system = [{\"text\": system_prompt}]\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": question}]}]\n",
    "\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"system\": system,\n",
    "    \"messages\": messages,\n",
    "    \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 2048},\n",
    "}\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "print(f\"{Fore.GREEN}Response: {response['output']['message']['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can parse the text of the response, recognize the ```<|python_tag|>``` and forward the call to the ```executor```.  Let's write a simple ```executor``` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Writing the executor\n",
    "The ```executor``` will parse the string response from the LLM which represents a function call and map it to the internal executable representation which will make the call and return the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BraveSearchPython:\n",
    "    api_token = None\n",
    "    args = {}\n",
    "    \n",
    "    def __init__(self, token, function_call):\n",
    "        self.api_token = token\n",
    "        string = function_call.split(\"(\")[1].split(\")\")[0]\n",
    "        for match in re.findall(r\"(\\w+)=(?:'((?:[^'\\\\]|\\\\.)*)'\"\n",
    "                                r\"|\\\"((?:[^\\\"\\\\]|\\\\.)*)\\\"|(\\d+))\", string):\n",
    "            key = match[0]\n",
    "            value = match[1] if match[1] else match[2] if match[2] else int(match[3])\n",
    "            if value and \"'\" in value:\n",
    "                value = value.replace(\"\\\\'\", \"'\")\n",
    "            if value and '\"' in value:\n",
    "                value = value.replace('\\\\\"', '\"')\n",
    "            self.args[key] = value\n",
    "        self.args[\"count\"] = 2\n",
    "        \n",
    "    def call(self, query, count) -> []:\n",
    "\n",
    "        brave = Brave(self.api_token)\n",
    "        search_results = brave.search(q=query, count=count)\n",
    "        results = []\n",
    "        \n",
    "        for result in search_results.web_results:\n",
    "            results.append({'title': result[\"title\"], 'url': result[\"url\"], 'description': result[\"description\"][:200]})\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialising the executor and calling Amazon Bedrock\n",
    "If the response begins with ```<|python_tag|>``` then gather the function call and pass it to the ```executor```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function_invocation_response = (response['output']['message']['content'][0]['text'])\n",
    "messages.append(response['output']['message'])\n",
    "\n",
    "if function_invocation_response.find(\"<|python_tag|>\") > 0:\n",
    "    function_call = function_invocation_response[len(\"<|python_tag|>\")+2:].strip()\n",
    "else:\n",
    "    function_call = None\n",
    "\n",
    "if function_call:\n",
    "    pythonsearch = BraveSearchPython(api_token, function_call)\n",
    "    tool_response = getattr(pythonsearch, 'call')(**pythonsearch.args)\n",
    "    # note the use of the user role here when it is actually the assistant\n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"text\": str(tool_response)}]})\n",
    "\n",
    "\n",
    "print(f\"{Fore.BLUE}Messages:\\n\")\n",
    "for m in messages:\n",
    "    print(f\"{m}\")\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "print(f\"{Fore.GREEN}Response: {response['output']['message']['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see there are some implementation details that are sub-optimal using this method.  Firstly, parsing the ```<|python_tag|>``` is inelegant and the code is difficult to understand.  Secondly, the turn choreography forces us to either mis-represent the tool results as having been provided by the ```user``` or not append the message with the ```function_invocation_response``` to the messages collection, mis-representing the flow of the conversation.  Neither of which are ideal.  We can try again by injecting Llama 3.1 special tokens into the messages.  In particular we will use the ```<eom_id>``` [end of message](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#built-in-python-based-tool-calling) tag which indicates the ```assistant``` is engaged in multi-step reasoning and therefore can support multiple contiguous ```assistant``` messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Threading the Llama 3.1 Special Tokens\n",
    "We will clear out all but the original ```user``` message from the message collection and add an edited first ```assistant``` message: the message invoking the tool.  We'll edit that by adding the ```<eom_id>``` tag and then play the conversation forward but, faithfully adding the result of calling the tool as an ```assistant``` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = messages[:1]\n",
    "special_message = function_invocation_response + '<|eom_id|>'\n",
    "messages.append({\"role\": \"assistant\", \"content\": [{\"text\": special_message}]})\n",
    "\n",
    "function_call = function_invocation_response[len(\"<|python_tag|>\")+2:].strip()\n",
    "pythonsearch = BraveSearchPython(api_token, function_call)\n",
    "tool_response = getattr(pythonsearch, 'call')(**pythonsearch.args)\n",
    "messages.append({\"role\": \"assistant\", \"content\": [{\"text\": str(tool_response)}]})\n",
    "\n",
    "print(f\"{Fore.BLUE}Messages:\\n\")\n",
    "for m in messages:\n",
    "    print(f\"{m}\")\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "print(f\"{Fore.GREEN}Response: {response['output']['message']['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Llama Function Calling - JSON Method\n",
    "\n",
    "The other way to use function calling in Llama 3.1 is the JSON method.  When we previewed in the console we provided a JSON structure in the user message which described the tool use.  The Amazon Bedrock native way of doing this is to provide the JSON definition in the ```toolConfig``` and make use of the [Amazon Bedrock Tool Use capability](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html) in the Amazon Bedrock Converse API.  That is what we will do now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Writing the tool definition\n",
    "Amazon Bedrock tool use requires a JSON Schema file which describes the function which can be called and its arguments.  In this definition there is a single required argument and a single optional argument.  Note that there are a lot of similarities between the [JSON that Amazon Bedrock requires](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html) to recognize the tool and the [JSON that Llama 3.1 models require](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#json-based-tool-calling) but, they are not exactly the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toolConfig = {\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"brave_search\",\n",
    "        \"description\": \"search the internet using the brave web search api\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"User's question\"\n",
    "              },\n",
    "                \"count\" : {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"the maximum number of results to return\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Passing the tool definition to Amazon Bedrock\n",
    "Passing the tool to Amazon Bedrock is a matter of including the ```toolConfig``` in the parameters for the Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = 'What are the names of the candidates who will contest the US Presidential Election on Tuesday November 5 2024?'\n",
    "\n",
    "system_prompt = \"Environment: ipython /r/n \" \\\n",
    "                \"Cutting Knowledge Date: December 2023 /r/n\" \\\n",
    "                \"Today Date: 22 September 2024 /r/n\" \\\n",
    "                \"You are a helpful assistant.\" \\\n",
    "                \"If you need to consult external sources please limit yourself to 5\"\n",
    "    \n",
    "system = [{\"text\": system_prompt}]\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": question}]}]\n",
    "\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"system\": system,\n",
    "    \"messages\": messages,\n",
    "    \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 2048},\n",
    "    \"toolConfig\": toolConfig\n",
    "}\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "print(f\"{Fore.GREEN}Response: {response['output']['message']['content'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Writing the executor\n",
    "The ```executor``` will parse the JSON response from the LLM which represents a function call and map it to the internal executable representation which will make the call and return the results. We've already written one for the ```Python``` method so we are familiar with the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BraveSearchJson:\n",
    "    api_token = None\n",
    "    toolUseId = None\n",
    "    args = {}\n",
    "    \n",
    "    def __init__(self, token, function_call):\n",
    "        self.api_token = token\n",
    "        self.toolUseId = function_call['toolUseId']\n",
    "        self.args = function_call['input']\n",
    "        \n",
    "    def call(self, query, count) -> []:\n",
    "\n",
    "        brave = Brave(self.api_token)\n",
    "        search_results = brave.search(q=query, count=int(count))\n",
    "        results = []\n",
    "        \n",
    "        for result in search_results.web_results:\n",
    "            results.append({'title': result[\"title\"], 'url': result[\"url\"], 'description': result[\"description\"][:200]})\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialising the executor and calling Amazon Bedrock\n",
    "If the response is of type ```toolUse``` then gather the function call and pass it to the ```executor```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_invocation_response = (response['output']['message']['content'][0]['toolUse'])\n",
    "messages.append(response['output']['message'])\n",
    "\n",
    "print(tool_invocation_response)\n",
    "if tool_invocation_response:\n",
    "    jsonsearch = BraveSearchJson(api_token, tool_invocation_response)\n",
    "else:\n",
    "    jsonsearch = None\n",
    "\n",
    "if jsonsearch:    \n",
    "    tool_response = getattr(jsonsearch, 'call')(**jsonsearch.args)\n",
    "    #note the use of the user role to return the tool use result\n",
    "    messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                'toolResult': {\n",
    "                                    'toolUseId': jsonsearch.toolUseId,\n",
    "                                    'content': [\n",
    "                                        {\n",
    "                                            \"text\": str(tool_response)\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "\n",
    "print(f\"{Fore.BLUE}Messages:\\n\")\n",
    "for m in messages:\n",
    "    print(f\"{m}\")\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "print(f\"{Fore.GREEN}Response: {response['output']['message']['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of things to note here.  Firstly, there appears to be the same problem with turn choreography as we encountered before.  Unfortunately it is not possible to add the ```<|eom_id|>``` tag in the ```toolUse``` response and so we must live with the mis-representation of the ```toolResult``` coming from the user.  What's also interesting to note is that we seem to be able to specify the number of results we would like back from Brave Search by using the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
