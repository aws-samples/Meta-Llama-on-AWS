{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model in SageMaker JumpStart for Amazon Bedrock Custom Model Import with Knowledge Distillation\n",
    "\n",
    "## Knowledge Distillation from Large to Small Language Models  (Llama 3.1 405B → Llama 3.2 1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Amazon SageMaker JumpStart to train a smaller language model through knowledge distillation from a larger foundation model, and then deploy it through Amazon Bedrock's Custom Model Import feature. The process involves distilling knowledge from a large language model (405B parameters) to a smaller model (1B parameters) while maintaining performance on specialized QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### What is Knowledge Distillation?\n",
    "Knowledge distillation is a model compression technique where a smaller model (student) is trained to mimic the behavior of a larger model (teacher). This process transfers knowledge from the larger model to the smaller one, creating a more efficient model that maintains much of the performance of the original.\n",
    "### Why Use Knowledge Distillation?\n",
    "- Cost Efficiency: Smaller models have lower inference costs\n",
    "- Reduced Latency: Faster response times for real-time applications\n",
    "- Lower Resource Requirements: Less memory and compute needed\n",
    "- Specialized Knowledge: Focus on domain-specific capabilities\n",
    "- Deployment Flexibility: Enables deployment on resource-constrained environments\n",
    "### Architecture Overview\n",
    "This implementation uses:\n",
    "- Teacher Model: Llama 3.1 405B parameters (via Amazon Bedrock)\n",
    "- Student Model: Llama 3.2 1B parameters (via SageMaker JumpStart)\n",
    "- Workflow: Generate high-quality responses with teacher → Train student to replicate behavior → Deploy student via Bedrock Custom Model Import → Evaluate models\n",
    "\n",
    "![Simplified workflow](knowledge_distillation_workflow_simplified.jpg)\n",
    "*Simplified workflow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker jmespath datasets transformers jinja2 ipywidgets boto3 boto3 matplotlib numpy jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Account Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section configures the necessary AWS resources including:\n",
    "\n",
    "- SageMaker session and default bucket\n",
    "- IAM roles and permissions\n",
    "- Region-specific settings\n",
    "- Required SDK versions and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "\n",
    "# AWS SDK imports\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker import hyperparameters, metric_definitions\n",
    "from sagemaker.parameter import ContinuousParameter, CategoricalParameter, IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "\n",
    "\n",
    "# Data processing and ML imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Custom modules (assuming these exist in your environment)\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"utils\"] = utils\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "# Import custom functions\n",
    "from utils import (\n",
    "    download_artifacts, \n",
    "    remove_field_from_json, \n",
    "    upload_artifacts, \n",
    "    cleanup_local_files, \n",
    "    wait_for_model_availability, \n",
    "    test_image_processing\n",
    ")\n",
    "from iam_role_helper import create_or_update_role\n",
    "\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"evaluations_help_functions\", \"evaluation_help_functions.py\")\n",
    "evaluations_help_functions = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"evaluations_help_functions\"] = evaluations_help_functions\n",
    "spec.loader.exec_module(evaluations_help_functions)\n",
    "\n",
    "# Import custom functions\n",
    "from evaluations_help_functions import (\n",
    "    run_model_comparison,\n",
    "    setup_logging,\n",
    "    analyze_errors,\n",
    "    create_dual_radar_plots,\n",
    "    prepare_and_upload_evaluation_files,\n",
    "    create_llm_judge_evaluation,\n",
    "    get_evaluation_files,\n",
    "    construct_evaluation_key,\n",
    "    generate_model_comparison_report,\n",
    "    analyze_and_plot_metrics,\n",
    "    generate_model_comparison_report_knowledge\n",
    "   \n",
    ")\n",
    "\n",
    "# Set default configurations\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100,  # More reasonable number than 100\n",
    "        'max_attempts': 3,         # Maximum retry attempts\n",
    "        'mode': 'adaptive',        # Uses adaptive retry mode with client-side throttling\n",
    "    },\n",
    "    connect_timeout=5,    # Reduce connection timeout from default 60s\n",
    "    read_timeout=30,      # Reduce read timeout from default 60s\n",
    "    max_pool_connections=50,  # Increase from default 10\n",
    "    tcp_keepalive=True    # Enable TCP keepalive\n",
    ")\n",
    "\n",
    "# Initialize key AWS clients\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker',region_name='us-west-2')\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-west-2',\n",
    "    config=config)\n",
    "brt = boto3.client(service_name='bedrock-runtime',region_name='us-west-2',\n",
    "    config=config)\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #change the name of the role if you are running locally\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-name')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "region=sess.boto_region_name\n",
    "\n",
    "prefix = \"llama-qa-distillation\"\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Configuration\n",
    "Configures IAM roles with required permissions for:\n",
    "- Amazon Bedrock model access\n",
    "- S3 bucket operations for model artifacts\n",
    "- CloudWatch logging capabilities\n",
    "- Cross-service permissions for SageMaker\n",
    "\n",
    "Key components:\n",
    "1. Trust relationships for service principals\n",
    "2. Permission policies for resource access\n",
    "3. Cross-account access configurations\n",
    "4. Logging and monitoring permissions\n",
    "\n",
    "Ensure that your SageMaker execution role's trust policy (Trusted Entities) allows Bedrock to assume the role. This is required so that Bedrock can submit and manage batch inference jobs on your behalf.\n",
    "\n",
    "To manually edit the trust policy, navigate to the SageMaker execution role you are using. Go to the Trust relationships tab and click Edit trust policy. Allow the bedrock.amazonaws.com service to assume the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the SageMaker excution role Trusted relationships\n",
    "\n",
    "# 1. Get the SageMaker execution role ARN\n",
    "try:\n",
    "    import sagemaker\n",
    "    execution_role_arn = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    # Fallback: list roles to find one with 'AmazonSageMaker-ExecutionRole' in the name\n",
    "    iam = boto3.client('iam')\n",
    "    roles = iam.list_roles()['Roles']\n",
    "    execution_role_arn = next(\n",
    "        (role['Arn'] for role in roles if 'AmazonSageMaker-ExecutionRole' in role['RoleName']),\n",
    "        None\n",
    "    )\n",
    "    if not execution_role_arn:\n",
    "        raise Exception(\"Could not find a SageMaker execution role.\")\n",
    "\n",
    "# 2. Extract the role name from the ARN\n",
    "role_name = execution_role_arn.split('/')[-1]\n",
    "\n",
    "# 3. Define the new trust policy\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\",\n",
    "                    \"sagemaker.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Update the trust policy for the SageMaker execution role\n",
    "iam = boto3.client('iam')\n",
    "try:\n",
    "    iam.update_assume_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    print(f\"Updated trust policy for SageMaker execution role: {role_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating trust policy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM Role Configuration for Amazon Bedrock Custom Model Import\n",
    "\n",
    "# 1. Setup Basic Variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']  # Get current AWS account ID\n",
    "region = \"us-west-2\"  # Note: Custom Model Import (CMI) only works in us-west-2 and us-east-1\n",
    "training_bucket = sagemaker_session_bucket  # S3 bucket where training artifacts are stored\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"  # Name for the IAM role we'll create\n",
    "\n",
    "# 2. Define Trust Relationship Policy\n",
    "# This policy defines which AWS services can assume this role\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        # Allow Bedrock service to assume this role\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                # Ensure requests only come from our account\n",
    "                \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                # Limit to specific Bedrock model import jobs\n",
    "                \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "            }\n",
    "        },\n",
    "        # Allow Lambda service to assume this role (if needed for auxiliary functions)\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Define Permission Policy\n",
    "# This policy defines what AWS resources the role can access\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        # Allow S3 access for model artifacts\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],  # Read-only access to S3\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{training_bucket}\",  # Access to bucket\n",
    "                f\"arn:aws:s3:::{training_bucket}/*\"  # Access to objects in bucket\n",
    "            ],\n",
    "            \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}  # Restrict to our account\n",
    "        },\n",
    "        # Allow CloudWatch Logs access for monitoring\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"  # Access to CloudWatch Logs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Create or Update the IAM Role\n",
    "bedrock_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=permission_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model Selection in Amazon Bedrock\n",
    "\n",
    "### Model Selection Criteria\n",
    "When choosing a foundation model in Amazon Bedrock for knowledge distillation, several key factors should be considered:\n",
    "\n",
    "#### 1. Model Architecture and Size\n",
    "The Meta Llama 3.1 405B model offers several advantages as a teacher model:\n",
    "- Larger parameter count provides richer knowledge representation\n",
    "- Enhanced ability to capture complex patterns and relationships\n",
    "- Superior performance on specialized tasks like medical QA\n",
    "- Better few-shot learning capabilities\n",
    "#### 2. Cost-Performance Trade-offs\n",
    "Amazon Bedrock's pay-per-use pricing model enables:\n",
    "- No upfront infrastructure costs\n",
    "- Payment only for actual inference time\n",
    "- Flexible scaling based on demand\n",
    "- Cost optimization through batch processing\n",
    "\n",
    "Reference: [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "#### 3. Specialized Knowledge Transfer\n",
    "The 405B model is particularly suitable for knowledge distillation because:\n",
    "- Higher accuracy on complex medical terminology\n",
    "- Better understanding of scientific context\n",
    "- More nuanced response generation\n",
    "- Improved zero-shot performance on domain-specific tasks\n",
    "#### 4. Operational Considerations\n",
    "Benefits of using Bedrock for the teacher model:\n",
    "- Serverless architecture eliminates infrastructure management\n",
    "- Built-in auto-scaling\n",
    "- High availability across AWS regions\n",
    "- Simplified API integration\n",
    "### Model Configuration\n",
    "The Llama 3.1 405B model in Bedrock can be configured with:\n",
    "- Temperature settings for response diversity\n",
    "- Maximum token length for comprehensive answers\n",
    "- Top-p and top-k sampling parameters\n",
    "- Custom prompt templates for specialized tasks\n",
    "\n",
    "Reference: [Amazon Bedrock Llama Model Configuration](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html)\n",
    "\n",
    "### Integration with Knowledge Distillation\n",
    "The workflow leverages Bedrock's advantages:\n",
    "1. Generate high-quality training data through batch inference\n",
    "2. Create specialized QA pairs for student model training\n",
    "3. Maintain quality while reducing computational requirements\n",
    "4. Enable seamless deployment through Custom Model Import\n",
    "\n",
    "Reference: \n",
    "- [Amazon Bedrock Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)\n",
    "- [Amazon Bedrock Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html)\n",
    "\n",
    "### Best Practices\n",
    "When using the teacher model:\n",
    "1. Implement proper error handling and retry mechanisms\n",
    "2. Use batch processing for dataset generation\n",
    "3. Monitor usage and costs through AWS CloudWatch\n",
    "4. Implement appropriate security controls and encryption\n",
    "\n",
    "For more information on model selection and configuration, see:\n",
    "- [Choose the best foundational model for your AI applications](https://community.aws/content/2fKJW0z9PEIKec94DZwtYigCF7i/choose-the-best-foundational-model-for-your-ai-applications?lang=en)\n",
    "- [Llama Technical Documentation](https://www.llama.com/docs/overview/)\n",
    "- [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple models available on Bedrock depending the region. In our case we would focus on llama 3.1 405b instruct that is available in us-west-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock', region_name=\"us-west-2\")\n",
    "model_id='meta.llama3-1-405b-instruct-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference with Bedrock Runtime\n",
    "\n",
    "This section demonstrates how to perform inference using the Bedrock Runtime client with the Llama model.\n",
    "\n",
    "**Note**: The Bedrock runtime client is specifically for model inference, separate from the main Bedrock client used for model management.\n",
    "\n",
    "\n",
    "Inference Helper Function.\n",
    "\n",
    "This function handles the core interaction with the Bedrock Runtime API, including error handling and response formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    try:\n",
    "        response = brt.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Setup and Model Parameters.\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "- temperature: Lower values make output more focused and deterministic\n",
    "- top_p: Controls diversity of token selection\n",
    "- max_gen_len: Limits response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "\n",
    "question = \"\"\"Is a mandatory general surgery rotation necessary in the surgical clerkship?\"\"\"\n",
    "user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\": user_message,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_gen_len\": 512,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Configuration and Invocation\n",
    "- Uses Llama 3.1 405B parameter model\n",
    "- Expects and returns JSON formatted data\n",
    "- Response includes generated text in the \"generation\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Generation\n",
    "\n",
    "This section explains how to prepare and process the PubMedQA dataset for knowledge distillation using AWS services.\n",
    "\n",
    "### Overview\n",
    "The PubMedQA dataset is a large-scale question-answering dataset focused on biomedical research literature. We'll use Amazon S3 for storage and SageMaker Processing Jobs for data preparation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Details\n",
    "**PubMedQA Dataset**\n",
    "- Source: [PubMedQA GitHub Repository](https://github.com/pubmedqa/pubmedqa/tree/master)\n",
    "- Citation: \n",
    ">\n",
    "> Jin, Q., Dhingra, B., Liu, Z., Cohen, W., & Lu, X. (2019). In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 2567-2577.\n",
    "- Format: JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Steps\n",
    "\n",
    "#### 1. Dataset Download and Validation\n",
    "This module handles downloading and processing PubMedQA dataset from GitHub for use with \n",
    "Amazon SageMaker and Amazon Bedrock knowledge distillation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_json(url):\n",
    "    try:\n",
    "        # Convert regular GitHub URL to raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        return requests.get(raw_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://github.com/pubmedqa/pubmedqa/blob/master/data/ori_pqal.json\"\n",
    "data = get_github_json(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Processing and JSONL Conversion\n",
    "This section demonstrates how to process the PubMedQA dataset in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "qa_index=list(data.keys())\n",
    "for i in qa_index:\n",
    "    keys_to_get = ['QUESTION', 'CONTEXTS','LONG_ANSWER']\n",
    "    result = {k: data[i].get(k) for k in keys_to_get}\n",
    "    dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_dataset='dataset.jsonl'\n",
    "with open(output_file_dataset, 'w') as outfile:\n",
    "    for sample in dataset:\n",
    "        # Create the complete record for batch inference\n",
    "        batch_record = {\n",
    "            \"question\": sample['QUESTION'],\n",
    "            \"answers\": sample['LONG_ANSWER']\n",
    "        }\n",
    "        \n",
    "        outfile.write(json.dumps(batch_record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Teacher Model for QA Generation\n",
    "\n",
    "Explains the process of:\n",
    "\n",
    "- Generating synthetic QA pairs\n",
    "- Batch processing with Bedrock\n",
    "- Data augmentation strategies\n",
    "- Quality control measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Processing vs Real-Time Inference\n",
    "\n",
    "Based on performance testing and cost analysis, Amazon Bedrock's batch processing capabilities offer significant advantages over real-time inference:\n",
    "\n",
    "1. **Performance Benefits**\n",
    "   - Higher throughput for large-scale processing\n",
    "   - Reduced risk of API throttling\n",
    "   - More efficient resource utilization\n",
    "\n",
    "2. **Cost Optimization**\n",
    "   - Lower per-request costs compared to real-time inference\n",
    "   - Better resource allocation and scheduling\n",
    "   - Reduced overhead from connection management\n",
    "\n",
    "3. **Operational Advantages**\n",
    "   - Built-in retry mechanisms\n",
    "   - Simplified monitoring and logging\n",
    "   - Better handling of large datasets\n",
    "\n",
    "For this implementation, we leverage Bedrock's batch processing to optimize both performance and cost efficiency while maintaining processing quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset for Bedrock Batch Processing\n",
    "\n",
    "This code creates a JSONL file formatted specifically for Amazon Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Converts QA dataset into Bedrock's required batch processing format\n",
    "- **Key Operations**:\n",
    "  - Formats prompts using Llama 3's instruction template\n",
    "  - Assigns unique IDs to each record\n",
    "  - Sets inference parameters (temperature, max length, etc.)\n",
    "  - Creates JSONL output with required Bedrock structure\n",
    "\n",
    "The resulting file enables efficient batch processing of multiple questions through Bedrock's batch inference API, optimizing for throughput and cost efficiency.\n",
    "\n",
    "> **Note**: The template uses Llama 3's specific tokens (`<|begin_of_text|>`, `<|eot_id|>`) for proper model instruction formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bedrock_batch_dataset(dataset, output_file='bedrock_batch_dataset.jsonl'):\n",
    "    # Simplified prompt template for Llama 3 instruction format\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Generate a unique record ID (11 characters)\n",
    "            record_id = str(uuid.uuid4())[:11]\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = prompt_template.format(\n",
    "                question=sample[\"QUESTION\"]\n",
    "            )\n",
    "\n",
    "            # Create the model input body for Llama 3\n",
    "            body = {\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 1024,\n",
    "                \"temperature\": 0.0,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "\n",
    "            # Create the complete record for batch inference\n",
    "            batch_record = {\n",
    "                \"recordId\": record_id,\n",
    "                \"modelInput\": body\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(batch_record) + '\\n')\n",
    "\n",
    "# Usage\n",
    "create_bedrock_batch_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Batch Dataset to Amazon S3\n",
    "\n",
    "This code handles the upload of the prepared batch dataset to Amazon S3, a necessary step before running Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Transfers the local JSONL file to S3 for Bedrock access\n",
    "- **Components**:\n",
    "  - Uses SageMaker's `S3Uploader` utility for simplified file transfer\n",
    "  - Organizes files under a structured prefix (`distillation/batch/data`)\n",
    "  - Automatically handles S3 path formatting and permissions\n",
    "\n",
    "> **Note**: The S3 location will be referenced in subsequent Bedrock batch inference job configurations. Ensure the Bedrock role has appropriate S3 read permissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination paths\n",
    "local_path_batch_file = 'bedrock_batch_dataset.jsonl'\n",
    "s3_prefix_batch = 'distillation/batch/data'  # This will be the folder in S3\n",
    "\n",
    "# Upload the file\n",
    "s3_path_batch = S3Uploader.upload(\n",
    "    local_path=local_path_batch_file,\n",
    "    desired_s3_uri=f's3://{bucket}/{s3_prefix_batch}',\n",
    ")\n",
    "\n",
    "print(f\"File uploaded successfully to: {s3_path_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Batch Inference Configuration\n",
    "\n",
    "This section configures and launches a batch inference job using Amazon Bedrock for large-scale QA processing:\n",
    "\n",
    "#### Configuration Components\n",
    "- **Input Configuration**: Points to the JSONL dataset in S3\n",
    "- **Output Configuration**: Specifies where Bedrock will store inference results\n",
    "- **Job Settings**: \n",
    "  - Unique job name using timestamp\n",
    "  - Model ARN for Llama 3.1 405B\n",
    "  - IAM role for execution permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix=\"output\"\n",
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": s3_path_batch\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket}/{s3_prefix_batch}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime  # This is the correct import\n",
    "jobName = 'batch-job-ga' + str(int(datetime.now().timestamp()))\n",
    "response=bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role,\n",
    "    modelId='arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-405b-instruct-v1:0',\n",
    "    #modelId='meta.llama3-1-405b-instruct-v1:0',\n",
    "    \n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see Amazon Bedrock Batch Inference documentation.\n",
    "Reference: [Amazon Bedrock Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Bedrock Batch Job Status\n",
    "\n",
    "This code implements a job status monitoring loop for the Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Tracks batch job progress until completion or failure\n",
    "- **Key Operations**:\n",
    "  - Extracts job ARN and ID for tracking\n",
    "  - Polls job status every 5 minutes\n",
    "  - Provides real-time status updates\n",
    "  - Handles completion and failure scenarios\n",
    "\n",
    "> **Note**: Consider implementing this monitoring pattern in AWS Lambda or Step Functions for production workloads.\n",
    "\n",
    "Consider using the sample Bedrock batch job results if you do not want to wait for your own job to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobArn)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Bedrock Batch Results for Training\n",
    "\n",
    "This section handles the retrieval and processing of batch inference results from S3 for model training:\n",
    "#### Data Flow\n",
    "1. **Retrieval**: Fetches batch results from S3\n",
    "2. **Processing**: Extracts model generations from JSON responses\n",
    "3. **Formatting**: Prepares data for JumpStart/Bedrock training format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve batch results from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve batch results from S3\n",
    "#job_id='wyg9q4pvli86'#Use the current bedrock job is if you are continuing from another step\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "prefix = f\"{s3_prefix_batch}/{output_prefix}/{job_id}/\"\n",
    "print(f\"prefix: {bucket}/{prefix}\")\n",
    "object_key = f\"{prefix}{local_path_batch_file}.out\"\n",
    "response = s3.get_object(Bucket=bucket, Key=object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and extract teacher model responses\n",
    "json_data = response['Body'].read().decode('utf-8')\n",
    "teacher_answer=[]\n",
    "for line in json_data.splitlines():\n",
    "        data = json.loads(line)\n",
    "        print(data['modelOutput']['generation'])\n",
    "        teacher_answer.append(data['modelOutput']['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines the original dataset with teacher model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_item, teacher in zip(dataset, teacher_answer):\n",
    "    data_item['TEACHER_ANSWER'] = teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: This paired dataset forms the foundation for training the student model to mimic the teacher's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data for SageMaker JumpStart\n",
    "\n",
    "This section formats the QA dataset for fine-tuning using SageMaker JumpStart's specific requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formatting Process\n",
    "1. **Template Creation**\n",
    "   - Defines Llama 3's instruction format\n",
    "   - Includes system message and conversation structure\n",
    "   - Maintains special tokens for model context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dataset Transformation**\n",
    "   - Converts QA pairs to instruction format\n",
    "   - Structures teacher responses as completions\n",
    "   - Creates JSONL format required by JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_jumpstart_dataset(dataset, output_file='train.jsonl', template_file='template.json'):\n",
    "    # Create the template file required by JumpStart for Q&A format\n",
    "    template = {\n",
    "        \"prompt\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"completion\": \"{response}\"\n",
    "    }\n",
    "    \n",
    "    # Save the template file\n",
    "    with open(template_file, 'w') as f:\n",
    "        json.dump(template, f)\n",
    "\n",
    "    # Process the dataset and create the training file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Format the data in the same structure as the synthetic data\n",
    "            training_entry = {\n",
    "                \"instruction\": sample[\"QUESTION\"],\n",
    "                \"response\": sample[\"LONG_ANSWER\"].strip()\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(training_entry) + '\\n')\n",
    "            \n",
    "def verify_jsonl(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if i == 0:  # Print first example\n",
    "                    print(\"Sample entry:\")\n",
    "                    print(json.dumps(data, indent=2))\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset files for JumpStart fine-tuning\n",
    "# print(dataset)\n",
    "\n",
    "create_jumpstart_dataset(dataset)\n",
    "verify_jsonl('train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Format\n",
    "- **Input**: Question-answer pairs with teacher model responses\n",
    "- **Output**: JSONL file containing:\n",
    "  - Instruction prompt with special tokens (`<|begin_of_text|>`)\n",
    "  - Question text\n",
    "  - Teacher model response\n",
    "  - End of text markers (`<|eot_id|>`)\n",
    "\n",
    "> **Important**: Follows [JumpStart Data Format Guidelines](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning-instruction-based.html).\n",
    "\n",
    "#### Validation Process\n",
    "The `verify_jsonl()` function checks:\n",
    "- JSONL format validity\n",
    "- Special token placement\n",
    "- Complete instruction/response pairs\n",
    "\n",
    "Example format:\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"What is the role of antibiotics in treating viral infections?\",\n",
    "  \"response\": \"Antibiotics are not effective against viral infections...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model Configuration (LLAMA 3.2 1B)\n",
    "\n",
    "This section covers the setup and configuration of the student model using Amazon SageMaker JumpStart:\n",
    "\n",
    "### Model Selection Criteria\n",
    "- Base model: LLAMA 3.2 1B\n",
    "- Optimized for knowledge distillation\n",
    "- Suitable for QA tasks\n",
    "- Efficient inference characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Upload\n",
    "\n",
    "The following code uploads the prepared training dataset and template to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "# Configure S3 paths with SageMaker defaults\n",
    "default_bucket_prefix = sagemaker.Session().default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "# Upload training files to S3\n",
    "local_data_file = \"train.jsonl\"\n",
    "template_file=\"template.json\"\n",
    "train_data_location = f\"s3://{bucket}{default_bucket_prefix_path}/oasst_top1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(template_file,train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")\n",
    "print(f\"template saved on:{train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Model Selection in SageMaker JumpStart\n",
    "\n",
    "This section implements an interactive model selection interface and configures training metrics:\n",
    "\n",
    "#### Model Selection Process\n",
    "- **Purpose**: Enables selection of appropriate student model from JumpStart's catalog\n",
    "- **Focus**: Text generation models suitable for knowledge distillation\n",
    "- **Default**: LLAMA 3.2 1B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create interactive model selector\n",
    "try:\n",
    "    dropdown = Dropdown(\n",
    "        options=list_jumpstart_models(\"search_keywords includes Text Generation\"),\n",
    "        value=\"meta-textgeneration-llama-3-2-1b-instruct\",\n",
    "        description=\"Select a JumpStart text generation model:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "    display(dropdown)\n",
    "except:\n",
    "    dropdown = None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dropdown:\n",
    "    student_model_id = dropdown.value\n",
    "else:\n",
    "    # Provide model id as meta-textgeneration-llama-3-1-405b-instruct-fp8 for the instruct variant\n",
    "    model_id = \"meta-textgeneration-llama-3-2-1b-instruct\"\n",
    "model_version_student = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric Setup\n",
    "- **Purpose**: Establishes standardized metrics for training evaluation\n",
    "- **Implementation**: Leverages SageMaker's built-in metric definitions\n",
    "- **Scope**: Covers training, validation, and system metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import metric_definitions\n",
    "print(metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Job Hyperparameter Configuration\n",
    "\n",
    "This section retrieves and configures the default hyperparameters for the student model training:\n",
    "\n",
    "#### Hyperparameter Setup\n",
    "- **Purpose**: Initializes model training configuration\n",
    "- **Source**: Uses JumpStart's optimized defaults\n",
    "- **Scope**: Includes learning rates, batch sizes, and model-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters_student = hyperparameters.retrieve_default(\n",
    "    model_id=student_model_id, model_version=model_version_student,\n",
    ")\n",
    "\n",
    "print(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Customization\n",
    "This section modifies default hyperparameters for knowledge distillation training:\n",
    "\n",
    "#### Parameter Adjustments\n",
    "- **Purpose**: Customizes training configuration for instruction-based learning\n",
    "- **Key Modifications**:\n",
    "  - Sets single epoch for initial testing\n",
    "  - Configures for instruction tuning\n",
    "  - Establishes fixed random seed for reproducibility\n",
    "  - Defines maximum input length constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters_student[\"epoch\"] = \"1\"\n",
    "my_hyperparameters_student['chat_dataset']=\"False\"\n",
    "my_hyperparameters_student['instruction_tuned']=\"True\"\n",
    "my_hyperparameters_student['seed']=\"10\"# this could help us to have the same results\n",
    "my_hyperparameters_student['max_input_length']=\"1024\"# this could help us to have the same results\n",
    "\n",
    "\n",
    "hyperparameters.validate(\n",
    "    model_id=student_model_id, model_version=model_version_student, hyperparameters=my_hyperparameters_student\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Configuration\n",
    "\n",
    "This section configures and executes automated hyperparameter optimization using SageMaker's Hyperparameter Tuning Jobs:\n",
    "\n",
    "#### Parameter Search Space Configuration\n",
    "- **Purpose**: Defines ranges for key training parameters\n",
    "- **Implementation**: Uses SageMaker's parameter types for optimization\n",
    "- **Scope**: Covers learning dynamics and LoRA-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges without as_json_range\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.0005, scaling_type=\"Logarithmic\"),\n",
    "    'lora_r': CategoricalParameter(['4', '8', '12', '16']),\n",
    "    'lora_alpha': CategoricalParameter(['16', '32', '48', '64']),\n",
    "    'lora_dropout': ContinuousParameter(0.01, 0.2),\n",
    "    'per_device_train_batch_size': CategoricalParameter(['2', '4', '6', '8','16']),\n",
    "    'gradient_accumulation_steps': CategoricalParameter(['1', '2', '3', '4']),\n",
    "    'max_steps': CategoricalParameter(['50', '75', '100']),\n",
    "    'warmup_steps': CategoricalParameter(['5', '7', '10']),\n",
    "    'num_train_epochs': CategoricalParameter(['1', '2'])\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_defs=metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b\", model_version='1.1.1',)\n",
    "print(metric_defs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced Metric Tracking\n",
    "- **Purpose**: Monitors both training and resource utilization metrics\n",
    "- **Implementation**: Combines default and custom GPU memory metrics\n",
    "- **Scope**: Enables comprehensive performance monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_metrics = [\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'train:loss', 'Regex': 'train_loss = ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = metric_defs + memory_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Job Configuration\n",
    "\n",
    "This section configures the hyperparameter optimization job using SageMaker's tuning capabilities:\n",
    "\n",
    "#### Configuration Components\n",
    "- **Purpose**: Automates hyperparameter optimization for model training\n",
    "- **Strategy**: Uses Bayesian optimization for efficient parameter search\n",
    "- **Scale**: Manages multiple training jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=my_hyperparameters_student,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    # instance_type='ml.g5.xlarge',\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    metric_definitions=combined_metrics,  # Add metric definitions here\n",
    "    enable_sagemaker_metrics=True  # Enable SageMaker metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='huggingface-textgeneration:train-loss',\n",
    "    metric_definitions=combined_metrics,\n",
    "    objective_type='Minimize',\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=1,#Adjust depending the available instances\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy='Bayesian',\n",
    "    base_tuning_job_name='llm-llama-3-2-1b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the hyperparameter tuning job\n",
    "tuner.fit({\"training\": train_data_location}, wait=True)\n",
    "# First, wait for the tuning job to complete\n",
    "tuner.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices for Hyperparameter Tuning**\n",
    ">\n",
    "> 1. **Resource Management**\n",
    ">    - Set `max_parallel_jobs` based on quota limits\n",
    ">    - Choose appropriate instance types (`ml.g5.2xlarge`)\n",
    ">    - Monitor GPU memory utilization\n",
    ">    - Consider cost optimization with spot instances\n",
    ">\n",
    "> 2. **Job Configuration**\n",
    ">    - Use descriptive `base_tuning_job_name`\n",
    ">    - Enable SageMaker metrics for monitoring\n",
    ">    - Set appropriate stopping conditions\n",
    ">    - Configure proper objective metrics\n",
    ">\n",
    "> 3. **Optimization Strategy**\n",
    ">    - Start with Bayesian optimization\n",
    ">    - Define meaningful parameter ranges\n",
    ">    - Balance exploration vs exploitation\n",
    ">    - Monitor convergence patterns\n",
    ">\n",
    "> See [Hyperparameter Tuning Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Best Training Results\n",
    "\n",
    "This section explains how to access and analyze the best performing model from the hyperparameter tuning job:\n",
    "#### Accessing Best Model\n",
    "- **Purpose**: Retrieves optimal hyperparameters and model artifacts\n",
    "- **Implementation**: Uses SageMaker's tuning job APIs\n",
    "- **Output**: Best performing model configuration and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Overview\n",
    "1. Get best training job name from tuner\n",
    "2. Retrieve detailed job information using SageMaker client\n",
    "3. Extract optimized hyperparameters\n",
    "4. Access performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job)['HyperParameters']\n",
    "print(\"Best hyperparameters: \\n\")\n",
    "pprint.pprint(best_hyperparameters_student_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job_1 = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")\n",
    "\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_1)['HyperParameters']\n",
    "print(f\"Best hyperparameters: {best_hyperparameters_student_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices**:\n",
    "> 1. **Result Analysis**\n",
    ">    - Review convergence patterns\n",
    ">    - Compare against baseline metrics\n",
    ">    - Document optimal parameters\n",
    ">\n",
    "> 2. **Model Management**\n",
    ">    - Save best configuration\n",
    ">    - Track experiment metadata\n",
    ">    - Document performance characteristics\n",
    ">\n",
    "> For more information, see [Analyzing Hyperparameter Tuning Results](https://sagemaker-examples.readthedocs.io/en/latest/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Optimized Hyperparameters\n",
    "\n",
    "This section configures and launches a training job using the best hyperparameters from tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Components\n",
    "1. **Hyperparameter Setup**\n",
    "   - Uses optimized parameters from tuning\n",
    "   - Extends training epochs for full model convergence\n",
    "   - Configures training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)\n",
    "best_hyperparameters_student_1['num_train_epochs']=10\n",
    "best_hyperparameters_student_1['epoch']=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **TensorBoard Integration**\n",
    "   - **Purpose**: Enables real-time training visualization\n",
    "   - **Storage**: Configures S3 location for logs\n",
    "   - **Access**: Enables Sagemaker Studio Tensorboard integration\n",
    "   > For more information, see [TensorBoard in Amazon SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create proper TensorBoard output configuration\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f's3://{bucket}/tensorboard-logs/llama3-model-distillation',\n",
    "    container_local_output_path='/opt/ml/output/tensorboard'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Metric Tracking Configuration**\n",
    "   - **Training Metrics**: Loss, perplexity, epoch statistics\n",
    "   - **Resource Metrics**: GPU/CPU memory utilization\n",
    "   - **Performance Metrics**: Throughput and timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "student_model_id = \"meta-textgeneration-llama-3-2-1b\"\n",
    "model_version_student = \"*\"\n",
    "\n",
    "estimator_student = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=best_hyperparameters_student_1,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\",\n",
    "        \"TENSORBOARD_LOGGING\": \"true\",\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    "    tensorboard_output_config=tensorboard_output_config  # Use the proper config object\n",
    ")\n",
    "# Define metrics to track\n",
    "metric_definitions = [\n",
    "    # Training Metrics\n",
    "    {'Name': 'train:loss', 'Regex': 'step .* is completed and loss is ([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:perplexity', 'Regex': 'train_perplexity=([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:epoch_loss', 'Regex': 'train_epoch_loss=([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    {'Name': 'eval:loss', 'Regex': 'eval_epoch_loss=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    {'Name': 'eval:perplexity', 'Regex': 'eval_ppl=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Performance Metrics\n",
    "    {'Name': 'epoch_time', 'Regex': 'epcoh time ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_throughput', 'Regex': '([0-9\\\\.]+)it/s'},\n",
    "    \n",
    "    # Memory Usage\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'cpu:peak_memory', 'Regex': 'CPU Total Peak Memory consumed during the train \\\\(max\\\\): ([0-9\\\\.]+) GB'}\n",
    "]\n",
    "# Add metrics to estimator\n",
    "estimator_student.metric_definitions = metric_definitions\n",
    "# Launch TensorBoard in SageMaker Studio\n",
    "tensorboard_callback = {\n",
    "    'Config': {\n",
    "        'TrainingJobName': 'llama-3-2-1b-model-distilation'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Training Launch**\n",
    "   - **Implementation**: Uses JumpStart estimator\n",
    "   - **Monitoring**: Enables comprehensive logging\n",
    "   - **Visualization**: Integrates with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_student.fit({\"training\": train_data_location},\n",
    "    wait=True,\n",
    "    logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices**:\n",
    "> 1. **Training Monitoring**\n",
    ">    - Track all defined metrics\n",
    ">    - Monitor resource utilization\n",
    ">    - Review TensorBoard visualizations\n",
    ">\n",
    "> 2. **Resource Management**\n",
    ">    - Configure appropriate instance types\n",
    ">    - Monitor memory usage\n",
    ">    - Track training progress\n",
    ">\n",
    "> 3. **Output Management**\n",
    ">    - Organize TensorBoard logs\n",
    ">    - Maintain training artifacts\n",
    ">    - Document training results\n",
    "\n",
    "For more information, see:\n",
    "- [SageMaker Training Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html)\n",
    "- [TensorBoard Integration](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Deployment\n",
    "\n",
    "This section covers the deployment and testing of the trained student model using Amazon Bedrock Custom Model Import (CMI):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Import Process\n",
    "- **Purpose**: Deploys trained model to Bedrock for serverless inference\n",
    "- **Implementation**: Automates model import and configuration\n",
    "- **Benefits**: Enables seamless integration with AWS AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training job name and model URI\n",
    "\n",
    "training_job_name = estimator_student._current_job_name\n",
    "# training_job_name='meta-textgeneration-llama-3-2-1b-2025-04-29-18-27-52-998'\n",
    "\n",
    "training_job_response=sagemaker_client.describe_training_job(\n",
    "    TrainingJobName=training_job_name\n",
    ")\n",
    "model_uri_1=training_job_response['ModelArtifacts']['S3ModelArtifacts']\n",
    "#model_uri_1 = estimator_student.model_data['S3DataSource']['S3Uri']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Configuration\n",
    "1. **Model Preparation**\n",
    "   - Retrieves training artifacts\n",
    "   - Validates model format\n",
    "   - Configures deployment parameters\n",
    "2. **Import Job Setup**\n",
    "   - Creates unique identifiers\n",
    "   - Sets up IAM permissions\n",
    "   - Configures resource allocation\n",
    "\n",
    "Ensure that your S3 bucket policy allows GetObject and ListObject for the Sagemaker_Bedrock_import_role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME = 'us-west-2'\n",
    "bedrock = boto3.client(service_name='bedrock',\n",
    "                       region_name=REGION_NAME)\n",
    "# Generate a unique job name\n",
    "timestamp = int(time.time())\n",
    "random_number = random.randint(1000, 9999)\n",
    "JOB_NAME = f\"meta3-import-model-{timestamp}-{random_number}\"\n",
    "\n",
    "ROLE_ARN = bedrock_role_arn\n",
    "IMPORTED_MODEL_NAME = f\"llama3_1_student_1_llama_1b_{timestamp}-{random_number}\"\n",
    "S3_URI = model_uri_1\n",
    "\n",
    "# createModelImportJob API\n",
    "create_job_response = bedrock.create_model_import_job(\n",
    "    jobName=JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=ROLE_ARN,\n",
    "    modelDataSource={\n",
    "        \"s3DataSource\": {\n",
    "            \"s3Uri\": model_uri_1\n",
    "        }\n",
    "    },\n",
    ")\n",
    "job_arn = create_job_response.get(\"jobArn\")\n",
    "print(f\"Model import job created with ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Monitoring\n",
    "- Tracks import job progress\n",
    "- Validates model availability\n",
    "- Monitors resource utilization\n",
    "- Handles deployment errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_filter = IMPORTED_MODEL_NAME  # Replace with your model name\n",
    "model_info = wait_for_model_availability(model_name_filter,max_attempts=30,delay=60)\n",
    "#\n",
    "if model_info:\n",
    "    model_arn_1=model_info[\"modelArn\"]\n",
    "    print(\"Model is now available in Bedrock.\")\n",
    "else:\n",
    "    print(\"Failed to find the model in Bedrock within the specified attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Configuration\n",
    "- Sets up runtime client\n",
    "- Configures retry policies\n",
    "- Implements error handling\n",
    "- Optimizes performance settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import json\n",
    "\n",
    "REGION_NAME = 'us-west-2'\n",
    "MODEL_ID= model_arn_1\n",
    "#MODEL_ID='arn:aws:bedrock:us-west-2:786045444066:imported-model/u1y9gohfgrm8'\n",
    "\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100,  # More reasonable number than 100\n",
    "        'max_attempts': 3,         # Maximum retry attempts\n",
    "        'mode': 'adaptive',        # Uses adaptive retry mode with client-side throttling\n",
    "    },\n",
    "    connect_timeout=5,    # Reduce connection timeout from default 60s\n",
    "    read_timeout=30,      # Reduce read timeout from default 60s\n",
    "    max_pool_connections=50,  # Increase from default 10\n",
    "    tcp_keepalive=True    # Enable TCP keepalive\n",
    ")\n",
    "message = \"Hello, what it is the weather in seattle?\"\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "br_runtime = session.client(service_name = 'bedrock-runtime', \n",
    "                                 region_name=REGION_NAME, \n",
    "                                 config=config)\n",
    "    \n",
    "try:\n",
    "    invoke_response = brt.invoke_model(modelId=MODEL_ID, \n",
    "                                            body=json.dumps({'prompt': message}), \n",
    "                                            accept=\"application/json\", \n",
    "                                            contentType=\"application/json\")\n",
    "    invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    print(json.dumps(invoke_response, indent=4))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(e.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Bedrock Model Deployment with Custom Model Import\n",
    "\n",
    "> **1. Import Configuration**\n",
    "> - Use descriptive, unique model names with timestamps\n",
    "> - Configure appropriate IAM roles and permissions\n",
    "> - Implement robust error handling mechanisms\n",
    "> - Set appropriate timeout values\n",
    "> - Validate model artifacts before import\n",
    "\n",
    "> **2. Deployment Monitoring**\n",
    "> - Track import job status regularly\n",
    "> - Implement automated status checks\n",
    "> - Set up CloudWatch alerts\n",
    "> - Monitor resource utilization\n",
    "> - Track deployment metrics\n",
    "\n",
    "> **3. Testing Strategy**\n",
    "> - Implement comprehensive test cases\n",
    "> - Validate model responses\n",
    "> - Monitor inference latency\n",
    "> - Track error rates and types\n",
    "> - Test with various input formats\n",
    "\n",
    "### Key Benefits of Bedrock Deployment\n",
    "\n",
    "#### Operational Benefits\n",
    "- **Serverless Infrastructure**\n",
    "  - No server management required\n",
    "  - Automatic scaling capabilities\n",
    "  - Pay-per-use pricing model\n",
    "\n",
    "- **Management Simplification**\n",
    "  - Automated deployments\n",
    "  - Built-in monitoring\n",
    "  - Simplified updates\n",
    "\n",
    "#### Technical Benefits\n",
    "- **Performance**\n",
    "  - Optimized inference\n",
    "  - Low-latency responses\n",
    "  - Automatic resource scaling\n",
    "\n",
    "- **Integration**\n",
    "  - Seamless AWS service connectivity\n",
    "  - Built-in security features\n",
    "  - Standardized APIs\n",
    "\n",
    "### Cost Benefits\n",
    "- **Import Costs:**\n",
    "  - No fees for importing custom model weights\n",
    "  - No control plane action costs\n",
    "  - Supported architectures include Meta Llama 2, Llama 3, Flan and Mistral\n",
    "\n",
    "- **Operational Costs:**\n",
    "  - Pay-per-use pricing based on Custom Model Units (CMUs)\n",
    "  - Billed in 5-minute increments\n",
    "  - Costs scale with number of active model copies\n",
    "  - Data transfer costs apply for out-of-network traffic\n",
    "\n",
    "- **Cost Management:**\n",
    "  - Costs determined by concurrent model copies needed\n",
    "  - Active duration of each model copy\n",
    "  - Consider concurrency requirements for accurate cost estimation\n",
    "#### Additional Resources\n",
    "- [Bedrock Custom Model Import Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)\n",
    "- [Bedrock Custom Model Import Pricing Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/import-model-calculate-cost.html)\n",
    "- [Model Monitoring Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Environment Setup\n",
    "\n",
    "#### Configuration Requirements\n",
    "**Model Configuration**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can provide the model arn\n",
    "#model_arn_1=\"model_arn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we create a persistent configuration dictionary that stores model and artifact information across potential session breaks. This approach follows AWS notebook best practices for long-running workflows that might span multiple sessions.\n",
    "\n",
    "#### Why This Matters\n",
    "Amazon Bedrock evaluation jobs can take several hours to start and complete. By storing configuration in a reusable dictionary (and optionally persisting to disk), we can:\n",
    "\n",
    "1. Resume workflow execution after session timeouts\n",
    "2. Maintain consistent configuration references across multiple notebook sessions\n",
    "3. Simplify troubleshooting by preserving job identifiers and artifact locations\n",
    "4. Implement checkpoint recovery for multi-stage workflows\n",
    "\n",
    "> **Best Practice:** For production implementations, consider using Step Functions to orchestrate these long-running processes or implementing checkpoint mechanisms that store state in Amazon S3.\n",
    "\n",
    "This pattern is particularly useful during development and testing of complex Bedrock Custom Model Import workflows, where you might need to inspect intermediate results before proceeding to subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'student': {\n",
    "        'model_id': f'{model_arn_1}',#This needs to be updated with your CMI arfcn, this can be integrated on the other notebook\n",
    "        'output_prefix': 'student_model',\n",
    "        'model_name': 'llama3-2-1b-fine-tuned-pubmed'  # This will be used as modelIdentifier\n",
    "    },\n",
    "    'base': {\n",
    "        'model_id': 'us.meta.llama3-2-1b-instruct-v1:0',\n",
    "        'output_prefix': 'base_model',\n",
    "        'model_name': 'meta-llama3-2-1b-base'  # This will be used as modelIdentifier\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    " # Generate timestamp once and use it consistently\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Benchmarking with Ground Truth Validation\n",
    "\n",
    "This section executes a comprehensive performance benchmark comparing the student and base models using standardized metrics such as latency, throughput, and reliability.\n",
    "\n",
    "##### Performance Metrics Collection\n",
    "The benchmarking process captures critical operational metrics including:\n",
    "- Average and peak response latency\n",
    "- Token processing rates (input and output TPM)\n",
    "- Request success rates and error patterns\n",
    "- Resource utilization patterns\n",
    "##### Ground Truth Integration\n",
    "We leverage the original training dataset questions and corresponding answers as ground truth for these measurements. This approach provides several benefits:\n",
    "\n",
    "1. **Consistency**: Using the same dataset ensures fair comparison between models\n",
    "2. **Reproducibility**: Enables repeatable benchmarking across model versions\n",
    "3. **Preparation for Bedrock Evaluation**: The formatted output will serve as input for Amazon Bedrock's evaluation jobs in later steps\n",
    "\n",
    "> **Best Practice:** When benchmarking models for production deployment, always measure both functional accuracy (correctness of answers) and non-functional characteristics (latency, throughput) using representative datasets that match expected production workloads.\n",
    "\n",
    "The results from this benchmark will help quantify the performance improvements gained through knowledge distillation while preparing the necessary artifacts for more detailed qualitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "log_file = setup_logging(timestamp)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "input_file = \"dataset.jsonl\"\n",
    "sample_size = 100  # Start with a small number to verify everything works\n",
    "\n",
    "logger.info(f\"Starting model comparison with sample size: {sample_size}\")\n",
    "logger.info(f\"Log file: {log_file}\")\n",
    "\n",
    "try:\n",
    "    # Run comparison with sample size\n",
    "    comparison_results = run_model_comparison(\n",
    "        input_file=input_file, \n",
    "        model_configs=MODEL_CONFIGS, \n",
    "        bedrock_runtime=brt ,\n",
    "        sample_size=sample_size,\n",
    "        timestamp=timestamp\n",
    "    )\n",
    "    print(comparison_results)\n",
    "\n",
    "    # Print results\n",
    "    logger.info(\"\\nComparison Results:\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    for model_name, result in comparison_results.items():\n",
    "        logger.info(f\"\\nModel: {model_name}\")\n",
    "        logger.info(f\"Output file: {result['output_file']}\")\n",
    "        logger.info(\"\\nMetrics:\")\n",
    "        logger.info(json.dumps(result['metrics'], indent=2))\n",
    "\n",
    "    # Save comparison results\n",
    "    comparison_file = f\"model_comparison_{timestamp}.json\"\n",
    "    with open(comparison_file, 'w') as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "    logger.info(f\"\\nDetailed comparison saved to: {comparison_file}\")\n",
    "    \n",
    "    # Analyze errors\n",
    "    logger.info(\"\\nAnalyzing errors from log file...\")\n",
    "    error_counts, error_examples = analyze_errors(log_file)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Fatal error in main execution:\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Create visualizations\n",
    "df, fig = create_dual_radar_plots(comparison_results)\n",
    "\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nComparative Metrics Table:\")\n",
    "display(df)\n",
    "\n",
    "# Save results\n",
    "df.to_csv('model_metrics_comparison.csv')\n",
    "fig.savefig('model_comparison_radar_plots.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Knowledge Evaluation (Optional)\n",
    "\n",
    "This section demonstrates how to configure and execute qualitative knowledge evaluations using Amazon Bedrock's LLM-as-judge capability. This step is optional and can be skipped based on your situation.\n",
    "\n",
    "> **Note:** If you prefer to skip the evaluation setup and proceed directly to examining results, you can [jump ahead to the Performance Comparison Report section](#model-performance-comparison-report-generation).\n",
    "\n",
    "##### Amazon Bedrock Evaluation Setup\n",
    "\n",
    "Amazon Bedrock evaluations provide automated, standardized assessments of model knowledge and capabilities across multiple dimensions including correctness, helpfulness, and coherence.\n",
    "\n",
    "> **Important:** Bedrock evaluation jobs using the LLM-as-judge feature may experience extended queue times (potentially several hours). You can either continue to subsequent sections, or execute the job and periodically check its status using the provided monitoring code. This behavior is expected during periods of high service demand, see:\n",
    "- [Evaluate model performance using another LLM as a judge](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-judge.html)\n",
    "##### S3 Bucket CORS Configuration Requirements\n",
    "\n",
    "Bedrock evaluation jobs require Cross-Origin Resource Sharing (CORS) configuration on your S3 bucket. The following setup steps will:\n",
    "\n",
    "1. Configure CORS policies to allow Bedrock services to access your evaluation files\n",
    "2. Upload properly formatted evaluation artifacts to your S3 bucket\n",
    "3. Prepare bucket locations for evaluation results\n",
    "\n",
    "> **Security Note:** If your organization has restrictions on enabling CORS for security reasons, consider skipping this section and using the performance metrics from the previous evaluation step. The upcoming code will attempt to modify your S3 bucket's CORS configuration, which may violate organizational security policies.\n",
    "\n",
    "##### Alternative Approaches\n",
    "\n",
    "If you cannot proceed with Bedrock evaluations due to CORS restrictions or queue times, you can:\n",
    "\n",
    "1. Continue to the next section where we examine evaluation results from pre-executed jobs\n",
    "2. Use the built-in performance metrics captured in previous steps\n",
    "3. Implement your own evaluation logic using the performance benchmark results\n",
    "\n",
    "**Best Practice:** For production workloads, schedule Bedrock evaluation jobs during off-peak hours and implement asynchronous notification mechanisms (such as SNS topics) to alert you when evaluations complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **S3 Bucket Configuration and File Upload**\n",
    "This code section configures your Amazon S3 bucket and uploads the evaluation files needed for Bedrock model assessment. The process follows AWS best practices for organizing evaluation artifacts.\n",
    "\n",
    "##### Key Operations\n",
    "\n",
    "The code performs several essential steps for Bedrock evaluation preparation:\n",
    "\n",
    "1. **CORS Configuration**: Configures Cross-Origin Resource Sharing (CORS) policies on your S3 bucket, enabling Bedrock services to access your evaluation files securely\n",
    "2. **Structured Uploading**: Organizes model comparison results into a standardized directory structure with timestamped prefixes for tracking and version control\n",
    "3. **File Validation**: Verifies that uploaded artifacts meet Bedrock's format requirements and are accessible with appropriate permissions\n",
    "\n",
    "> **Best Practice:** When preparing evaluation datasets for Bedrock, maintain consistent naming conventions and utilize timestamp-based versioning to track evolution of job status over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload and verify results\n",
    "# Prepare bucket and upload results\n",
    "s3_locations = prepare_and_upload_evaluation_files(MODEL_CONFIGS, bucket,sess,timestamp)\n",
    "\n",
    "# Print final locations\n",
    "print(\"\\nFinal S3 Locations:\")\n",
    "print(\"=\" * 50)\n",
    "for model_name, location in s3_locations.items():\n",
    "    if(model_name=='student'):\n",
    "        MODEL_CONFIGS['student']['s3_location']=location\n",
    "    elif(model_name=='base'):\n",
    "        MODEL_CONFIGS['base']['s3_location']=location\n",
    "    print(f\"{model_name}: {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IAM Role and Policy Configuration for Amazon Bedrock Evaluation\n",
    "\n",
    "This code segment creates and configures a dedicated IAM role with precise permissions following the principle of least privilege for Bedrock model evaluation. The role configuration implements AWS security best practices for service-to-service interactions.\n",
    "\n",
    "###### Permission Structure\n",
    "\n",
    "The code establishes a comprehensive security model through:\n",
    "\n",
    "1. **Trust Relationship**: Defines a targeted trust policy allowing only Amazon Bedrock and SageMaker services to assume this role\n",
    "2. **Fine-grained Permissions**: Creates specific permissions for Bedrock evaluation operations including:\n",
    "   - Creating and managing evaluation jobs\n",
    "   - Accessing model resources\n",
    "   - Invoking models for comparison\n",
    "3. **S3 Access Controls**: Implements scoped permissions for S3 operations, limiting access to only the bucket paths required for evaluation artifacts\n",
    "4. **Resource-level Permissions**: Uses ARN patterns to restrict actions to specific resources within your account\n",
    "\n",
    "> **Security Best Practice:** This implementation follows AWS's recommended approach of creating purpose-specific roles with narrowly scoped permissions rather than using broader administrator roles, enhancing your security posture while enabling the necessary functionality.\n",
    "\n",
    "The resulting IAM role provides Bedrock evaluation jobs with precisely the permissions needed to access models and evaluation data without excessive privileges, conforming to AWS Well-Architected security principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# 1. Setup Basic Variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']  # Get current AWS account ID\n",
    "region = \"us-west-2\"  # Note: Custom Model Import (CMI) only works in us-west-2 and us-east-1\n",
    "role_name = \"Bedrock_Evaluation_Role\"  # Name for the new IAM role we'll create\n",
    "\n",
    "# 2. Define Trust Relationship Policy\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\",\n",
    "                    \"sagemaker.amazonaws.com\"  # Added SageMaker service\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Define Permission Policy\n",
    "# Bedrock resources access policy\n",
    "bedrock_access_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"BedrockConsole\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateEvaluationJob\",\n",
    "                \"bedrock:GetEvaluationJob\",\n",
    "                \"bedrock:ListEvaluationJobs\",\n",
    "                \"bedrock:StopEvaluationJob\",\n",
    "                \"bedrock:GetCustomModel\",\n",
    "                \"bedrock:ListCustomModels\",\n",
    "                \"bedrock:CreateProvisionedModelThroughput\",\n",
    "                \"bedrock:UpdateProvisionedModelThroughput\",\n",
    "                \"bedrock:GetProvisionedModelThroughput\",\n",
    "                \"bedrock:ListProvisionedModelThroughputs\",\n",
    "                \"bedrock:GetImportedModel\",\n",
    "                \"bedrock:ListImportedModels\",\n",
    "                \"bedrock:ListTagsForResource\",\n",
    "                \"bedrock:UntagResource\",\n",
    "                \"bedrock:TagResource\",\n",
    "                \"bedrock:InvokeModel\",\n",
    "                \"bedrock:InvokeModelWithResponseStream\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:bedrock:{region}::foundation-model/*\",\n",
    "                f\"arn:aws:bedrock:{region}:{account_id}:*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# S3 access policy\n",
    "s3_access_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"S3AccessForModelEvaluation\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:GetBucketCORS\",\n",
    "                \"s3:PutBucketCORS\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:ListBucketVersions\",\n",
    "                \"s3:GetBucketLocation\",\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:ListMultipartUploadParts\",\n",
    "                \"s3:ListBucketMultipartUploads\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{bucket}\",\n",
    "                f\"arn:aws:s3:::{bucket}/*\",\n",
    "                f\"arn:aws:s3:::{bucket}/model-evaluation/*\"  # Specific path\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Combine policies\n",
    "combined_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": (\n",
    "        bedrock_access_policy[\"Statement\"] +\n",
    "        s3_access_policy[\"Statement\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "# 4. Create or Update the IAM Role\n",
    "\n",
    "bedrock_evaluation_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=combined_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_evaluation_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Storing Configuration State***\n",
    "This step saves the model configuration dictionary to a local JSON file, implementing AWS notebook best practices for state management during long-running workflows.\n",
    "\n",
    "> **Best Practice:** Persisting configuration state to disk creates checkpoints that allow you to resume work after notebook kernel restarts or session timeouts—particularly valuable with Bedrock evaluation jobs that may take hours to complete.\n",
    "\n",
    "By serializing the complete configuration, including model identifiers, S3 locations, and job references, you can reliably restart your workflow from intermediate points without recalculating or recreating previous steps. This technique is especially useful during iterative development when troubleshooting Bedrock evaluation jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MODEL_CONFIGS dictionary\n",
    "with open('model_configs.json', 'w') as f:\n",
    "    json.dump(MODEL_CONFIGS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Amazon Bedrock Evaluation Job Configuration - Base Model\n",
    "\n",
    "This section prepares the configuration parameters needed to evaluate the base model using Amazon Bedrock's model evaluation framework. Following AWS best practices, we create a structured configuration that enables controlled, reproducible model assessments.\n",
    "\n",
    "**Configuration Components**\n",
    "\n",
    "The code establishes several crucial parameters:\n",
    "\n",
    "1. **Unique Job Identification**: Creates a UUID-based job name for tracking and debugging\n",
    "2. **IAM Authorization**: References the previously created role ARN with appropriate permissions\n",
    "3. **Data Locations**: Configures input and output S3 URIs following AWS's recommended path structure\n",
    "4. **Model Selection**: Specifies which foundation model will be used as the evaluation baseline\n",
    "5. **Inference Source Naming**: Sets a descriptive identifier for the model in evaluation reports\n",
    "\n",
    "> **Best Practice:** Using UUID-based identifiers with descriptive prefixes creates uniquely identifiable resources while maintaining human-readable naming patterns. This approach facilitates both programmatic tracking and manual review of evaluation jobs in the AWS Console.\n",
    "\n",
    "This configuration establishes the foundation for a standardized evaluation process that can be consistently applied across multiple model versions as your distillation workflow evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your variables\n",
    "# Define your variable with a unique UUID\n",
    "job_name_base = f\"model-eval-base-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "#base configuration for bedrock\n",
    "role_arn = bedrock_evaluation_role_arn\n",
    "dataset_s3_uri_base = MODEL_CONFIGS['base']['s3_location']\n",
    "output_s3_uri_base = MODEL_CONFIGS['base']['output_location']\n",
    "model_identifier = \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "inference_source_name_base = MODEL_CONFIGS['base']['model_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing Amazon Bedrock Evaluation Jobs\n",
    "\n",
    "This section demonstrates how to programmatically create and launch model evaluation jobs using Amazon Bedrock's evaluation capability. The implementation follows AWS best practices for automating evaluation workflows through the boto3 SDK.\n",
    "\n",
    "##### Evaluation Job Execution\n",
    "\n",
    "The code creates separate evaluation jobs for each model configuration:\n",
    "\n",
    "1. **Orchestrated Creation**: Iterates through model configurations to create parallel evaluation jobs\n",
    "2. **Consistent Parameters**: Applies the same LLM-as-judge evaluator model across all evaluations\n",
    "3. **Unique Identification**: Assigns distinct job names with UUID suffixes for tracking\n",
    "4. **Error Handling**: Implements AWS-recommended exception handling patterns for API operations\n",
    "\n",
    "> **⚠️ Important:** Evaluation jobs using the \"Bring Your Own Inference\" (BYOI) approach may currently experience extended processing times due to service optimizations in progress. Jobs might remain in queue for several hours before execution begins.\n",
    "\n",
    "##### Monitoring and Recovery\n",
    "\n",
    "The configuration dictionary is updated with job ARNs and names, enabling:\n",
    "- Status tracking across notebook sessions\n",
    "- Resumability if your notebook environment disconnects\n",
    "- Correlation between jobs and their results\n",
    "> **Best Practice:** For production implementations, consider using EventBridge to monitor job state changes or implement a Step Functions workflow that can manage the entire evaluation process asynchronously while providing status updates.\n",
    "\n",
    "This approach balances the need for automation with practical considerations around Bedrock service limitations, providing a robust pattern for launching evaluation jobs that can scale to multiple models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "# Job Configuration\n",
    "evaluator_model = \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "MODEL_CONFIGS['base']['output_location']='/'.join(MODEL_CONFIGS['base']['s3_location'].rsplit('/', 1)[0].split('/')[:-1] + ['evaluation'])+\"/base/\"\n",
    "MODEL_CONFIGS['student']['output_location']='/'.join(MODEL_CONFIGS['student']['s3_location'].rsplit('/', 1)[0].split('/')[:-1] + ['evaluation'])+\"/student/\"\n",
    "role_arn = bedrock_evaluation_role_arn\n",
    "\n",
    "for key in MODEL_CONFIGS.keys():\n",
    "    print(MODEL_CONFIGS[key])\n",
    "    # Create evaluation job\n",
    "    job_name = f\"model-eval-{key}-{uuid.uuid4().hex[:8]}\"\n",
    "    try:\n",
    "        llm_as_judge_response = create_llm_judge_evaluation(\n",
    "            client=bedrock,\n",
    "            job_name=job_name,\n",
    "            role_arn=role_arn,\n",
    "            input_s3_uri=MODEL_CONFIGS[key]['s3_location'] ,\n",
    "            output_s3_uri=MODEL_CONFIGS[key]['output_location'],\n",
    "            evaluator_model_id=evaluator_model,\n",
    "            task_type=\"General\",\n",
    "            inference_Source_Id=MODEL_CONFIGS[key]['model_name'],\n",
    "        )\n",
    "        print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\n",
    "        MODEL_CONFIGS[key]['job_arn']=job_arn=llm_as_judge_response['jobArn']\n",
    "        MODEL_CONFIGS[key]['job_name']=job_arn=job_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checkpoint Configuration Update\n",
    "\n",
    "This step persists the updated model configuration dictionary, which now contains evaluation job ARNs and identifiers. This follows AWS's recommended checkpoint pattern for long-running notebook workflows.\n",
    "\n",
    "> **Best Practice:** Regularly saving state after significant operations ensures you can recover your workflow progress even if the notebook kernel restarts or times out during extended job processing. This is particularly important after launching Bedrock evaluation jobs that may run for hours.\n",
    "\n",
    "The JSON serialization preserves all reference information needed to check job status or retrieve results in future sessions, creating a reliable recovery point in your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the MODEL_CONFIGS dictionary\n",
    "with open('model_configs.json', 'w') as f:\n",
    "    json.dump(MODEL_CONFIGS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job Status Monitoring Loop\n",
    "\n",
    "This code implements AWS's recommended pattern for monitoring long-running Bedrock evaluation jobs, with features for session recovery and status tracking.\n",
    "\n",
    "##### Key Components\n",
    "\n",
    "1. **Session Reinitialization**: Recreates the Bedrock client and reloads saved configuration, enabling workflow continuity across notebook sessions\n",
    "2. **Controlled Polling**: Implements a 5-minute interval checking pattern to efficiently monitor job status without overwhelming the service API \n",
    "3. **Status Handling**: Provides real-time status updates while waiting for job completion or failure\n",
    "\n",
    "> **Best Practice:** The polling approach with appropriate sleep intervals follows AWS's guidance for monitoring asynchronous operations while minimizing API calls. For production implementations, consider EventBridge rules as an alternative to active polling.\n",
    "\n",
    "This pattern allows you to monitor evaluation jobs that may run for extended periods without maintaining a continuous notebook session, supporting flexible workflow resumption after breaks or interruptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize bedrock client (you'll need to do this again in your new session)\n",
    "bedrock = boto3.client('bedrock')\n",
    "\n",
    "# Load your saved configuration\n",
    "with open('model_configs.json', 'r') as f:  # or .pkl if you used pickle\n",
    "    MODEL_CONFIGS = json.load(f)\n",
    "\n",
    "# Check status for each job\n",
    "for key in MODEL_CONFIGS.keys():\n",
    "    job_arn = MODEL_CONFIGS[key]['job_arn']\n",
    "    print(f\"Checking status for {key} model:\")\n",
    "    \n",
    "    status = ''\n",
    "    while status not in ['Completed', 'Failed']:\n",
    "        job_response = bedrock.get_evaluation_job(jobIdentifier=job_arn)\n",
    "        status = job_response['status']\n",
    "        if status == 'Failed':\n",
    "            print(job_response)\n",
    "        elif status == 'Completed':\n",
    "            print(datetime.now(), \": \", status)\n",
    "            break\n",
    "        else: \n",
    "            print(datetime.now(), \": \", status)\n",
    "            time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Evaluation Artifacts from S3\n",
    "\n",
    "This code retrieves the evaluation result files generated by completed Bedrock evaluation jobs. The implementation follows AWS best practices for structured artifact retrieval and verification.\n",
    "\n",
    "##### Artifact Discovery Process\n",
    "\n",
    "The code implements a robust evaluation file retrieval workflow:\n",
    "\n",
    "1. **Path Construction**: Uses job ARNs and model identifiers to deterministically construct the correct S3 key prefixes\n",
    "2. **Latest Artifact Selection**: Identifies the most recent evaluation output files when multiple versions exist\n",
    "3. **Multi-Model Consolidation**: Organizes results from both base and student models for comparative analysis\n",
    "4. **Validation Checks**: Prints constructed paths and retrieved files to verify successful discovery\n",
    "\n",
    "> **Best Practice:** The structured path construction approach follows AWS's recommended pattern for locating artifacts in predictable locations without hardcoding paths. This enables consistent retrieval across different evaluation runs and model versions.\n",
    "\n",
    "By programmatically discovering evaluation files rather than using hardcoded paths, this approach maintains flexibility when job IDs or output locations change, making your workflow more resilient and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_files = get_evaluation_files(MODEL_CONFIGS, bucket)\n",
    "print(evaluation_files)\n",
    "\n",
    "print(\"\\nFound evaluation files:\")\n",
    "print(f\"Base model file key: {evaluation_files['base']}\")\n",
    "print(f\"Student model file key: {evaluation_files['student']}\")\n",
    "\n",
    "# Debug the constructed prefixes\n",
    "base_prefix = construct_evaluation_key(MODEL_CONFIGS, 'base')\n",
    "student_prefix = construct_evaluation_key(MODEL_CONFIGS, 'student')\n",
    "print(\"\\nConstructed prefixes:\")\n",
    "print(f\"Base prefix: {base_prefix}\")\n",
    "print(f\"Student prefix: {student_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key_base = evaluation_files['base']\n",
    "file_key_student = evaluation_files['student']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric Analysis and Visualization Generation\n",
    "\n",
    "This code executes a comprehensive analysis of evaluation metrics from both models and generates standardized visualization artifacts. The implementation follows AWS's recommended patterns for evaluation result processing and presentation.\n",
    "\n",
    "##### Analysis Workflow\n",
    "\n",
    "The `analyze_and_plot_metrics` function performs multiple data processing and visualization steps:\n",
    "\n",
    "1. **S3 Data Retrieval**: Downloads evaluation results from both models using AWS SDK's optimized retrieval patterns\n",
    "2. **Metric Extraction**: Parses complex JSON evaluation outputs to extract comparable metrics\n",
    "3. **Comparative Analysis**: Performs side-by-side comparison of model performance across multiple dimensions\n",
    "4. **Visualization Creation**:\n",
    "   - Generates radar plots showing relative performance across metrics\n",
    "   - Creates formatted comparison tables with improvement indicators\n",
    "   - Produces CSV files for further analysis or reporting\n",
    "\n",
    "> **Best Practice:** Automating the generation of standardized visualizations and comparison artifacts ensures consistent evaluation methodology across models and versions. This approach supports data-driven decision making by presenting complex evaluation results in accessible formats.\n",
    "\n",
    "The resulting visualizations provide clear, actionable insights into the performance differences between the base and student models, highlighting where knowledge distillation has maintained or improved capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis and create the plot\n",
    "analyze_and_plot_metrics(bucket, file_key_base, file_key_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance Comparison Report Generation\n",
    "\n",
    "This code dynamically generates a comprehensive markdown report comparing operational performance metrics between the base and student models. The implementation follows AWS best practices for automated reporting and evaluation documentation.\n",
    "\n",
    "##### Report Generation Process\n",
    "\n",
    "The `generate_model_comparison_report` function creates a structured performance analysis:\n",
    "\n",
    "1. **Data-Driven Content**: Dynamically populates the report using metrics captured in the CSV file from previous steps\n",
    "2. **Metric Interpretation**: Automatically calculates improvement percentages and provides contextual explanations\n",
    "3. **Formatted Presentation**: Generates properly structured markdown tables with consistent formatting\n",
    "4. **Visual Integration**: Incorporates references to previously generated visualization assets\n",
    "\n",
    "> **Best Practice:** Automating report generation with standardized templates ensures consistency across evaluation runs while providing a reproducible record of model improvements. This approach supports AWS's recommendation for maintaining comprehensive model documentation throughout the ML lifecycle.\n",
    "\n",
    "The dynamic report includes detailed sections on:\n",
    "- Latency metrics with percentage improvements\n",
    "- Throughput comparisons showing token processing efficiency\n",
    "- Success rate analysis and reliability metrics\n",
    "- Operational insights for production deployment considerations\n",
    "\n",
    "By displaying the report as rendered Markdown directly in the notebook, this approach creates a self-documenting workflow that combines code, visualizations, and insights in a single interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = generate_model_comparison_report('model_metrics_comparison.csv')\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Evaluation Report Generation\n",
    "\n",
    "This code dynamically generates a detailed qualitative comparison between the base and student models focusing on knowledge representation and response quality. The implementation aligns with AWS best practices for holistic model evaluation beyond performance metrics.\n",
    "\n",
    "##### Knowledge Assessment Process\n",
    "\n",
    "The `generate_model_comparison_report_knowledge` function synthesizes evaluation results from LLM-as-judge assessments:\n",
    "\n",
    "1. **Multi-dimensional Analysis**: Extracts metrics across quality dimensions including correctness, completeness, helpfulness, and coherence\n",
    "2. **Trend Identification**: Quantifies improvements and declines across knowledge dimensions\n",
    "3. **Contextual Interpretation**: Provides explanations for observed differences in knowledge representation\n",
    "4. **Decision Support**: Includes cost-benefit analysis and deployment considerations\n",
    "\n",
    "> **Best Practice:** Combining quantitative metrics with qualitative assessments follows AWS's recommended approach for comprehensive model evaluation. This dual approach ensures both operational performance and response quality are considered in model selection decisions.\n",
    "\n",
    "The generated report integrates with visualizations created earlier, providing stakeholders with both high-level insights and detailed breakdowns of knowledge differences between models. This documentation approach helps teams make informed decisions about model deployment while maintaining a record of observed knowledge transfer effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the report\n",
    "report = generate_model_comparison_report_knowledge('metrics_comparison.csv')\n",
    "\n",
    "# Display the formatted report in the notebook\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Best Practices\n",
    "\n",
    "This section implements AWS's recommended cleanup procedures for resources created during the knowledge distillation workflow. Following proper cleanup practices helps manage costs, maintain security, and keep your AWS environment organized.\n",
    "\n",
    "### Resource Termination Strategy\n",
    "\n",
    "AWS recommends a systematic approach to resource cleanup, following a dependency-aware sequence. The implementation below follows this pattern, starting with application-level resources before removing supporting infrastructure components.\n",
    "\n",
    "#### Custom Model Cleanup Process\n",
    "\n",
    "The first step targets Bedrock custom models, which should be removed before dependent roles and permissions:\n",
    "\n",
    "1. **Resource Identification**: Retrieves model identifiers from the stored configuration\n",
    "2. **Controlled Deletion**: Uses the Bedrock API to properly terminate custom models\n",
    "3. **Error Handling**: Implements robust exception handling for common deletion scenarios\n",
    "4. **Verification**: Confirms successful removal of resources\n",
    "\n",
    "> **Best Practice:** Always remove resources in the reverse order of creation to avoid dependency conflicts. Custom models should be deleted before removing the IAM roles that granted access to them, preventing orphaned permissions in your account.\n",
    "\n",
    "By properly terminating custom models in Bedrock, you prevent ongoing charges for unused model storage and maintain a clean resource inventory. This approach aligns with AWS Well-Architected Framework guidance on operational excellence and cost optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration Recovery for Cleanup\n",
    "\n",
    "This step reloads the saved configuration state, ensuring accurate resource identification during the cleanup process. This pattern follows AWS best practices for resource lifecycle management.\n",
    "\n",
    "> **Reminder:** Loading the persisted configuration provides precise resource identifiers even if your notebook session has restarted since resource creation. This ensures cleanup targets exactly the right resources, preventing orphaned resources or errors from mismatched identifiers.\n",
    "\n",
    "By extracting the student model name from the configuration, we maintain consistent resource referencing throughout the entire lifecycle—from creation through evaluation to deletion—supporting AWS's recommended approach to resource traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved configuration\n",
    "with open('model_configs.json', 'r') as f:  # or .pkl if you used pickle\n",
    "    MODEL_CONFIGS = json.load(f)\n",
    "student_model_name=MODEL_CONFIGS['student']['model_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bedrock_custom_model(model_name):\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    try:\n",
    "        bedrock_client.delete_imported_model(modelIdentifier=model_name)\n",
    "        print(f\"Successfully deleted Bedrock custom model: {model_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        error_code = error.response['Error']['Code']\n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"Error deleting Bedrock custom model: The provided model name is invalid. Model Name: {model_name}\")\n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            print(f\"Error: The model '{model_name}' was not found in Bedrock.\")\n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            print(\"Error: You do not have permission to delete this model.\")\n",
    "        elif error_code == 'ConflictException':\n",
    "            print(\"Error: The model is currently in use or in a state that doesn't allow deletion.\")\n",
    "        else:\n",
    "            print(f\"Error deleting Bedrock custom model: {error}\")\n",
    "\n",
    "# Replace with your actual model name\n",
    "MODEL_NAME = student_model_name\n",
    "\n",
    "delete_bedrock_custom_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IAM Role Cleanup\n",
    "\n",
    "This section implements a thorough IAM role cleanup process following AWS security best practices. Properly removing IAM resources after use is a critical component of maintaining a secure AWS environment.\n",
    "\n",
    "##### IAM Resource Removal Process\n",
    "\n",
    "The `delete_iam_role` function implements AWS's recommended multi-step approach to IAM cleanup:\n",
    "\n",
    "1. **Permission Detachment**: Systematically removes all inline policies before attempting role deletion\n",
    "2. **Managed Policy Detachment**: Identifies and detaches all managed policies from the role\n",
    "3. **Permissions Boundary Removal**: Addresses permissions boundaries that might prevent deletion\n",
    "4. **Role Deletion**: Removes the role only after all dependencies have been addressed\n",
    "5. **Multiple Role Handling**: Processes both custom roles created during this workflow\n",
    "\n",
    "> **Security Best Practice:** Complete removal of temporary IAM roles follows the principle of least privilege by ensuring permissions exist only when needed. This approach minimizes the security footprint of your environment and prevents permission accumulation over time.\n",
    "\n",
    "By programmatically performing this cleanup rather than manual console operations, the workflow ensures consistent, thorough removal of all permission components, reducing the risk of orphaned policies or forgotten permissions that could create security vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_iam_role(role_name):\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        # Delete inline policies\n",
    "        inline_policies = iam.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy in inline_policies:\n",
    "            iam.delete_role_policy(RoleName=role_name, PolicyName=policy)\n",
    "            \n",
    "        # Detach managed policies\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "            \n",
    "        # Delete permissions boundary if it exists\n",
    "        try:\n",
    "            iam.delete_role_permissions_boundary(RoleName=role_name)\n",
    "        except iam.exceptions.NoSuchEntityException:\n",
    "            pass\n",
    "        \n",
    "        # Finally delete the role\n",
    "        iam.delete_role(RoleName=role_name)\n",
    "        print(f\"Successfully deleted IAM role: {role_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting IAM role: {error}\")\n",
    "\n",
    "# Delete Sagemaker_Bedrock_import_role\n",
    "delete_iam_role(\"Sagemaker_Bedrock_import_role\")\n",
    "delete_iam_role(\"Bedrock_Evaluation_Role\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices\n",
    "\n",
    "This section highlights key AWS best practices specifically relevant to knowledge distillation workflows using SageMaker JumpStart and Bedrock Custom Model Import.\n",
    "\n",
    "##### Cost Optimization Strategies\n",
    "\n",
    "1. **Training Optimization**\n",
    "    - **Spot Instance Training**: Reduce distillation costs by up to 90% using SageMaker managed spot training\n",
    "    - **Hyperparameter Optimization**: Start with smaller sample size for initial HP tuning jobs before full dataset runs\n",
    "    - **Model Size Selection**: Balance teacher model size against inference costs; larger isn't always better for knowledge transfer\n",
    "    - **Resource Cleanup**: Implement automated cleanup workflows to remove temporary artifacts and unused models\n",
    "\n",
    "2. **Inference Optimization**\n",
    "    - **Custom Model Import**: For high-volume inference workloads, Bedrock Custom Models can offer more predictable pricing than on-demand foundation models\n",
    "    - **Evaluation First**: Test student models thoroughly before production deployment to confirm knowledge transfer quality\n",
    "    - **Batch Processing**: Use Bedrock batch operations for dataset-wide inference to reduce costs and improve throughput\n",
    "\n",
    "> **Resource:** See [AWS Cost Optimization for Machine Learning](https://docs.aws.amazon.com/whitepapers/latest/ml-best-practices-public-sector-organizations/cost-optimization.html) for additional strategies.\n",
    "\n",
    "                     \n",
    "##### Security Implementation\n",
    "\n",
    "1. **Data and Model Protection**\n",
    "    - **IAM Role Lifecycle**: Create purpose-specific roles with minimal permissions and remove when tasks complete\n",
    "    - **KMS Encryption**: Encrypt training data, model artifacts, and evaluation results using KMS keys\n",
    "    - **Bucket Policies**: Implement restrictive S3 policies that limit access to specific principals and actions\n",
    "\n",
    "2. **Operational Security**\n",
    "    - **Monitoring**: Configure CloudWatch alarms for abnormal inference patterns or cost spikes\n",
    "    - **API Protection**: Implement retry policies with exponential backoff for Bedrock API interactions\n",
    "    - **Artifact Validation**: Verify model artifacts before import to prevent poisoning or tampering\n",
    "\n",
    "> **Resource:** Review the [AWS ML Security Best Practices](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/security-pillar-best-practices-3.html) for comprehensive security guidance.\n",
    "\n",
    "Following these practices will help you implement knowledge distillation workflows that are cost-effective, secure, and production-ready while leveraging the strengths of both SageMaker and Bedrock services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrates a complete knowledge distillation workflow that leverages Amazon SageMaker JumpStart for training and Amazon Bedrock Custom Model Import for deployment. By following this approach, you can create specialized, efficient models that retain the capabilities of larger foundation models while reducing operational costs and latency.\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "Our knowledge distillation implementation demonstrated several key benefits:\n",
    "\n",
    "- **Performance Improvements**: The student model achieved significantly better latency characteristics, with response times improving by 40-60% compared to the base model\n",
    "- **Knowledge Transfer**: The distilled model maintained comparable quality metrics on domain-specific questions, particularly excelling in metrics like correctness and completeness\n",
    "- **Resource Efficiency**: The distilled 1B parameter model requires significantly less compute resources than the 405B teacher model, while maintaining task-specific capabilities\n",
    "- **Cost Optimization**: Deployment through Bedrock Custom Model Import provides a cost-effective inference option compared to SageMaker endpoints or on-demand Bedrock foundation models\n",
    "### Lessons Learned\n",
    "\n",
    "This implementation revealed several valuable insights about knowledge distillation workflows in AWS:\n",
    "\n",
    "- **Custom Model Import Advantages**: Bedrock Custom Model Import provides serverless scaling with predictable pricing and native integration with Bedrock features, making it preferable to SageMaker endpoints for many use cases\n",
    "- **Distillation Process**: Creating effective teacher datasets requires careful prompt engineering and quality filtering to ensure knowledge transfer\n",
    "- **Evaluation Methodology**: Combining operational metrics with LLM-as-judge evaluations provides a more complete assessment of model quality than either approach alone\n",
    "- **Workflow Management**: Long-running processes like Bedrock evaluations benefit from robust state management through configuration persistence\n",
    "### Future Improvements\n",
    "\n",
    "To build upon this implementation, consider these potential enhancements:\n",
    "\n",
    "#### Enhanced Distillation Techniques\n",
    "- Implement more advanced distillation methods like progressive knowledge distillation or born-again networks\n",
    "- Experiment with different temperature settings in teacher model responses to balance diversity and precision\n",
    "- Incorporate domain-specific data augmentation strategies to improve specialization\n",
    "#### Workflow Automation\n",
    "- Implement an end-to-end workflow using AWS Step Functions or integrate with SageMaker Pipelines for MLOps automation, incorporating model training, hyperparameter tuning, evaluation, custom model import, and automated canary deployments with CloudWatch alarms for quality gates\n",
    "- Implement CI/CD integration for regular model updates as new data becomes available\n",
    "- Add automated A/B testing between model versions before promotion to production\n",
    "#### Advanced Deployment Options\n",
    "- Explore Bedrock Provisioned Throughput for high-traffic applications with predictable usage patterns\n",
    "- Implement Bedrock Knowledge Bases with the distilled model for RAG applications\n",
    "- Create a model ensemble combining multiple specialized student models for different domains\n",
    "#### Governance and Monitoring\n",
    "- Implement Model Cards for documentation of model characteristics and limitations\n",
    "- Develop drift detection mechanisms to identify when retraining is needed\n",
    "- Create comprehensive monitoring dashboards specific to distilled model performance\n",
    "### Bedrock Custom Model Import vs. Alternatives\n",
    "\n",
    "This implementation highlights several advantages of Bedrock Custom Model Import over other deployment options:\n",
    "\n",
    "**Compared to SageMaker Endpoints:**\n",
    "- **Serverless Architecture**: No endpoint management or capacity planning\n",
    "- **Cost Structure**: Pay only for actual usage rather than provisioned instances\n",
    "- **Feature Access**: Direct integration with Bedrock guardrails, knowledge bases, and agents\n",
    "- **Operational Simplicity**: Reduced operational overhead without endpoint management\n",
    "**Compared to Base Bedrock Models:**\n",
    "- **Specialization**: Improved performance on domain-specific tasks\n",
    "- **Predictable Pricing**: More stable pricing structure for high-volume applications\n",
    "- **Size Efficiency**: Smaller models with specialized knowledge can outperform general-purpose models\n",
    "- **Cost Efficiency**: Potentially lower costs for specialized, high-volume applications\n",
    "\n",
    "By leveraging this knowledge distillation pattern with Bedrock Custom Model Import, organizations can deploy specialized AI capabilities with optimal performance characteristics while maintaining control over costs and quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
