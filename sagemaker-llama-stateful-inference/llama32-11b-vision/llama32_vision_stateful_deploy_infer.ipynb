{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35429457",
   "metadata": {},
   "source": [
    "# Llama 3.2 vision stateful inference with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc57174",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "This notebook uses SageMaker notebook instance `conda_pytorch_p310` kernel, demonstrates how to use TorchServe to deploy Llama 3.2 vision Model on SageMaker. \n",
    "\n",
    "This is the code accompanying the workshop <TODO>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f165f5",
   "metadata": {},
   "source": [
    "## Step 0: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version && aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2b9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip\n",
    "!pip install -Uq sagemaker\n",
    "!pip install torch-model-archiver\n",
    "!pip install -Uq botocore\n",
    "!pip install -Uq boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat > .env <<EOF\n",
    "TS_HF_TOKEN_VALUE=\"hf_....\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f472b",
   "metadata": {},
   "source": [
    "Make sure you have accepted Meta terms and conditions to download llama models [here](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision)  \n",
    "Generate a hugging face access token  [Learn more](https://huggingface.co/docs/hub/en/security-tokens)  \n",
    "Open the .env file and add the access token to the .env file as a value for TS_HF_TOKEN_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0afe0-7d63-465d-bb53-2758f09ed3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)  # Loads the variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4f583-1349-4322-afed-9cbfeb476db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import importlib\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58626ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1ec20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "barebone_session = sagemaker.session.Session()  # barebone sagemaker session to get current region\n",
    "# region name of the current SageMaker Studio environment\n",
    "region = barebone_session._region_name\n",
    "boto3_session=boto3.session.Session(region_name=region)\n",
    "# Create a SageMaker runtime client object using your IAM role ARN\n",
    "smr = boto3.client('sagemaker-runtime', region_name=region)\n",
    "# Create a SageMaker client object\n",
    "sm = boto3.client('sagemaker', region_name=region)\n",
    "# execution role for the endpoint\n",
    "role = sagemaker.get_execution_role()  \n",
    "# sagemaker session for interacting with different AWS APIs\n",
    "sess= sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  \n",
    "# account_id of the current SageMaker Studio environment\n",
    "account = sess.account_id()  \n",
    "\n",
    "# Configuration:\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "model_name = \"llama32vision-sm\"\n",
    "print(f'account={account}, region={region}, role={role}, output_path={output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1db99",
   "metadata": {},
   "source": [
    "## Step 1: Build a BYOD TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6d968",
   "metadata": {},
   "source": [
    "1. Create an ECR repo: https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html\n",
    "2. Get Base Image: https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbaa005-d32b-41af-bf90-6ddabbdc2579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseimage = f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker\"\n",
    "reponame = \"llama32-11b-vision-stateful\"\n",
    "versiontag = \"1.0\"\n",
    "print(\"use the output from the print below to run ./build_and_push.sh in a termianl. You get better feedback in terminal.\")\n",
    "print (f\"cd docker && ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}\")\n",
    "print(\"if you do endup running this command in a terminal , you can skip the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e412ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture build_output\n",
    "\n",
    "# # Build our own docker image\n",
    "# !cd docker && ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15351a14-0a4f-4e69-a737-4801042e6735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update container\n",
    "container = f\"{account}.dkr.ecr.{region}.amazonaws.com/{reponame}:{versiontag}\"\n",
    "container\n",
    "print(baseimage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc696a",
   "metadata": {},
   "source": [
    "## Step2: Build TorchServe Model Artifacts and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe9031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf code/{model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0438c7-a038-4e6d-bf9c-801ab3c511a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd code && torch-model-archiver --model-name {model_name} --version 1.0 --handler handler/custom_handler.py --config-file handler/model-config.yaml --archive-format no-archive --extra-files handler/ -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b41e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd code && aws s3 cp {model_name} {output_path}/{model_name} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223542c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_uri = f\"{output_path}/{model_name}/\"\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a07123",
   "metadata": {},
   "source": [
    "## Step3: Create SageMaker Endpont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d13c4",
   "metadata": {},
   "source": [
    "### 3.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa91ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# we are deploying this model in a single GPU memory. Each GPU in g5 instance has 24GB of GPU memory. \n",
    "# The model size is 22 GB at 16 bits per weight. We are cutting it a bit close by using g5 instances.\n",
    "# For a production use case it is better to ml.p4.24xlarge or higher since p4d has 40GB of GPU memory per GPU\n",
    "# We have kept the instance type as g5 to reduce the cost and also make more accessible for people who want to \n",
    "# understand how stateful inference works\n",
    "instance_type = \"ml.g5.4xlarge\"  \n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name)\n",
    "\n",
    "model = Model(\n",
    "    name=model_name + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    # Enable SageMaker uncompressed model artifacts via \"S3DataType\": \"S3Prefix\"\n",
    "    model_data={\n",
    "        \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "        }\n",
    "    },\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    env={\n",
    "        # TorchServe configuration file\n",
    "        \"TS_CONFIG_FILE\": \"/home/model-server/config.properties\",\n",
    "        # Disable token authorization for REST APIs\n",
    "        \"TS_DISABLE_TOKEN_AUTHORIZATION\": \"true\", \n",
    "        # Headers to indicate Session ID\n",
    "        \"TS_HEADER_KEY_SEQUENCE_ID\": \"X-Amzn-SageMaker-Session-Id\",\n",
    "        \"TS_REQUEST_SEQUENCE_ID\": \"X-Amzn-SageMaker-Session-Id\",\n",
    "        # Headers to indicate closed session\n",
    "        \"TS_HEADER_KEY_SEQUENCE_END\": \"X-Amzn-SageMaker-Closed-Session-Id\",\n",
    "        \"TS_REQUEST_SEQUENCE_END\": \"X-Amzn-SageMaker-Closed-Session-Id\",\n",
    "        # Enable system metrics aggregation\n",
    "        \"TS_DISABLE_SYSTEM_METRICS\": \"false\",\n",
    "        \"TS_HF_TOKEN\": os.environ[\"TS_HF_TOKEN_VALUE\"]\n",
    "    },\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c3f57",
   "metadata": {},
   "source": [
    "### 3.2 Deploy Model and Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99966473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1, # increase the number of instances based on your load\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    #volume_size=512, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, \n",
    "    container_startup_health_check_timeout=3600, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b043907",
   "metadata": {},
   "source": [
    "### 3.3 Create a Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8073073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=model.endpoint_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b26d-fa4a-4225-a547-9402ce27e51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor = sagemaker.predictor.Predictor(\n",
    "#     endpoint_name='llava-sm-2024-09-04-06-35-10-354',\n",
    "#     sagemaker_session=sess\n",
    "# )\n",
    "# print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1d5c6",
   "metadata": {},
   "source": [
    "## Step4: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96ce7f-c63d-4e22-b0b4-c67b507c34eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add necessary modules path to sys.path\n",
    "import os, sys\n",
    "\n",
    "demo_data_path = os.path.join(os.getcwd(), \"code/handler\")\n",
    "if demo_data_path not in sys.path:\n",
    "    sys.path.append(demo_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ede86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install dependencies\n",
    "!pip install torch dataclasses_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b229206",
   "metadata": {},
   "source": [
    "### 4.1 Open Session 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77cd2a-a20f-420d-9910-a65ed0f7edee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_url=\"https://images.pexels.com/photos/1519753/pexels-photo-1519753.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c388f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from data_types import (\n",
    "    BaseRequest,\n",
    "    CloseSessionRequest,\n",
    "    StartSessionRequest,\n",
    "    TextPromptRequest,\n",
    "    OpenSessionResponse,\n",
    "    TextPromptResponse,\n",
    "    CloseSessionResponse\n",
    ")\n",
    "\n",
    "ts_request_sequence_id = \"SessionId\"\n",
    "\n",
    "\n",
    "def send_and_check_request(r, seq_id):\n",
    "    response = smr.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=r.to_json(),\n",
    "        ContentType=\"application/json\",\n",
    "        SessionId=seq_id\n",
    "    )\n",
    "    assert response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200, f\"Sending request failed: {r}\"\n",
    "    return response['Body'].readlines()[0]\n",
    "\n",
    "open_request = StartSessionRequest(\n",
    "    type=\"start_session\",\n",
    "    path=image_url,\n",
    ")\n",
    "\n",
    "open_response = send_and_check_request(open_request, \"NEW_SESSION\")\n",
    "open_response = OpenSessionResponse.from_json(open_response)\n",
    "print(open_response)\n",
    "assert open_response.session_id.startswith(\"ts-seq-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f86c2e-d1ef-4b14-9fc9-4eaf5ba74b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "open_response.session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6466cb",
   "metadata": {},
   "source": [
    "### 4.2 Send Text Promt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d996a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "text_prompt_request1 = TextPromptRequest(\n",
    "    type=\"send_text_prompt\",\n",
    "    session_id=open_response.session_id,\n",
    "    prompt_text=\"describe the picture\"\n",
    ")\n",
    "\n",
    "text_prompt_response1 = send_and_check_request(text_prompt_request1, open_response.session_id)\n",
    "text_prompt_response1 = TextPromptResponse.from_json(text_prompt_response1)\n",
    "print(text_prompt_response1.response_text)\n",
    "assert text_prompt_response1.response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfacc14b",
   "metadata": {},
   "source": [
    "### 4.3 Send Text Promt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502be89c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "text_prompt_request2 = TextPromptRequest(\n",
    "    type=\"send_text_prompt\",\n",
    "    session_id=open_response.session_id,\n",
    "    prompt_text=\"is there a mountain in the picture, describe it\"\n",
    ")\n",
    "\n",
    "text_prompt_response2 = send_and_check_request(text_prompt_request2, open_response.session_id)\n",
    "text_prompt_response2 = TextPromptResponse.from_json(text_prompt_response2)\n",
    "print(text_prompt_response2.response_text)\n",
    "assert text_prompt_response2.response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7b2b",
   "metadata": {},
   "source": [
    "### 4.4 Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf9239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# close session\n",
    "close_request = CloseSessionRequest(\n",
    "    type=\"close_session\",\n",
    "    session_id=open_response.session_id,\n",
    ")\n",
    "    \n",
    "close_response = send_and_check_request(\n",
    "    close_request, open_response.session_id\n",
    ")\n",
    "\n",
    "close_response = CloseSessionResponse.from_json(close_response)\n",
    "assert close_response.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3f455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0388105-8b40-4d7a-a74b-a544bd7b79ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
