{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question (RAG) Application with Llama3-8B on SageMaker JumpStart using LangChain\n",
    "\n",
    "RAG Application use cases with Llama3-8B on SageMaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of [Llama3-8B](https://huggingface.co/meta-llama/Llama-2-13b) text generation combined with [BGE Large En v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on a SageMaker Notebook. This notebook, powered by an `ml.t3.medium instance`, enables the deployment of LLMs on [SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/). These can be called with an API endpoint created by SageMaker, which we then use to build, experiment with, and tune for comparing Advanced RAG application techniques using [LangChain](https://www.langchain.com/). Additionally, we showcase how the [FAISS](https://github.com/facebookresearch/faiss) Embedding store can be utilized to archive and retrieve embeddings, integrating it into your RAG workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f4f95-dfab-4c64-956e-6c29274131d0",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597937-7c3e-42cc-b595-309bd8e5ae28",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Llama3-8B Text Generation` and `BGE Large En v1.5` models, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for endpoint usage\n",
    "   - `ml.g5.2xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad641d-5a50-4493-b308-563f280f2b2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2277979-97a1-41a6-ac40-0f267326b40a",
   "metadata": {},
   "source": [
    "### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama3-8B Text Generation: `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, and `ml.p4d.24xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287fd5-1c6f-4a05-a7cc-085cb10b4508",
   "metadata": {},
   "source": [
    "### Local setup (Optional):\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33af4f4-69fd-4a9f-acb3-f06bbe64fb21",
   "metadata": {},
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19e16-86b3-4e27-94bf-00e2f832605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "\n",
    "1. [Requirements](#Requirements)\n",
    "2. [Model Deployment](#Model-Deployment)\n",
    "3. [Setup LangChain](#Setup-LangChain)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Question Answering with LangChain Vector Store Wrapper](#Question-Answering-with-LangChain-Vector-Store-Wrapper)\n",
    "7. [Improving RAG Responses for Long Documents with Langchain](#Improving-RAG-Responses-for-Long-Documents-with-Langchain)\n",
    "8. [Conclusion](#Conclusion)\n",
    "9. [Clean Up Resources](#Clean-Up-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb282-2718-4911-a92b-4ef084441239",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d071-695d-4102-a8cd-6fba6c4678a3",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d152-2889-4004-8eba-a0d9028708db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "pypdf==4.1.0\n",
    "faiss-cpu==1.8.0\n",
    "boto3==1.34.58\n",
    "sqlalchemy==2.0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Before proceeding, please verify that you have the correct version of the SQLAlchemy library installed. This notebook requires SQLAlchemy >= 2.0.0.\n",
    "\n",
    "To check your installed SQLAlchemy version, you can run the following code:\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)\n",
    "```\n",
    "\n",
    "If the version displayed is less than 2.0.0, and you have already installed the correct version using `pip`, you may need to \"<span style=\"color:green;\">restart</span>\" or \"<span style=\"color:green;\">shutdown</span>\" the Jupyter Notebook kernel to load the updated library.\n",
    "\n",
    "To restart the kernel, go to the \"Kernel\" menu and select \"Restart Kernel\". If that doesn't work, try shutting down the notebook completely and relaunching it.\n",
    "\n",
    "Restarting or shutting down the kernel will resolve any dependency issues and ensure that the correct SQLAlchemy version is loaded.\n",
    "\n",
    "If you haven't installed SQLAlchemy >= 2.0.0 yet, you can do so by running the following command in your terminal or command prompt:\n",
    "\n",
    "```\n",
    "pip install sqlalchemy>=2.0.29\n",
    "```\n",
    "\n",
    "Once the installation is complete, restart or shutdown the Jupyter Notebook kernel as described above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4370c7-8453-478b-aefd-9c861bf7f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bf9f6-1127-4444-b684-f0c933c6158a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    !pip install sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193291-1bf2-478e-afff-d6afd33a358b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705564e-143f-411e-8537-a337255f256f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `Llama 3 8B Instruct` LLM model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0b3ea-8482-4288-8ed2-e03d7bafcb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model ID for the HuggingFace Llama 3 8b Instruct LLM model\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "accept_eula = True\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "# predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fdb95-bd4b-445c-b597-755fbd9a432a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `BGE Large En` embedding model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c22418-d962-46b2-8e5e-0bcb74e6ff64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "model_id = \"huggingface-sentencesimilarity-bge-large-en-v1-5\"\n",
    "text_embedding_model = JumpStartModel(model_id=model_id)\n",
    "# embedding_predictor = text_embedding_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c041-8d3c-4265-bb9b-e4d0dd0bb151",
   "metadata": {},
   "source": [
    "## Setup LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f757369-4769-442a-87ba-db9bf75a4dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0256b-6fd8-4e1a-8206-ff4ca6efda72",
   "metadata": {},
   "source": [
    "Get endpoint names from predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28065-2ccc-4174-98b1-c45fdb83cc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name\n",
    "llm_endpoint_name = \"meta-textgeneration-llama-3-8b-instruct-2024-05-24-00-31-15-408\"\n",
    "embedding_endpoint_name = \"hf-sentencesimilarity-bge-large-en-v1-5-2024-05-24-00-07-17-320\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e593b2-1433-4bef-b217-be724575e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transform input and output data to proccess API calls for`Llama 3 8B Instruct` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616478c-d454-4196-b64c-c9445e28839f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Llama38BContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"stop\": [\"<|eot_id|>\"],\n",
    "            },\n",
    "        }\n",
    "        input_str = json.dumps(\n",
    "            payload,\n",
    "        )\n",
    "        #print(input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        content = response_json[\"generated_text\"].strip()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5476939-6f13-4ba0-9bc9-b43a92592282",
   "metadata": {},
   "source": [
    "Instantiate the LLM with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425b6fe-a572-4725-a1ad-ee1c7bf29db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the content handler for Llama3-8B\n",
    "llama_content_handler = Llama38BContentHandler()\n",
    "\n",
    "# Setup for using the Llama3-8B model with SageMaker Endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "     endpoint_name=llm_endpoint_name,\n",
    "     region_name=region, \n",
    "     model_kwargs={\"max_new_tokens\": 1024, \"top_p\": 0.9, \"temperature\": 0.7},\n",
    "     content_handler=llama_content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58be6d-c4ff-4948-babc-52c41228c427",
   "metadata": {},
   "source": [
    "Transform input and output data to proccess API calls for`BGE Large En` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c2cf2-dda6-4617-b828-255b4aa4dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BGEContentHandlerV15(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, text_inputs: List[str], model_kwargs: dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            text_inputs (list[str]): A list of input text strings to be processed.\n",
    "            model_kwargs (Dict): Additional keyword arguments to be passed to the endpoint.\n",
    "               Possible keys and their descriptions:\n",
    "               - mode (str): Inference method. Valid modes are 'embedding', 'nn_corpus', and 'nn_train_data'.\n",
    "               - corpus (str): Corpus for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - top_k (int): Top K for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - queries (list[str]): Queries for Nearest Neighbor. Required when mode is 'nn_corpus' or 'nn_train_data'.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"text_inputs\": text_inputs,\n",
    "                **model_kwargs\n",
    "            }\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4749e-e437-4212-b69d-6c800aa0e21c",
   "metadata": {},
   "source": [
    "Instantiate the embedding model with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6e08e-c969-4a05-8587-ad49f9d9dca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bge_content_handler = BGEContentHandlerV15()\n",
    "sagemaker_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs={\"mode\": \"embedding\"},\n",
    "    content_handler=bge_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793e5-c562-48ce-858b-50c918ac5249",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e0dc2-718f-47af-aa60-30fa9a60cae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first download some of the files to build our document store.\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0cbc6-367a-443a-9e59-c63640a1e4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2024-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2023-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2022-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2021-10-K-Annual-Report.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2024, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1]),\n",
    "    dict(year=2022, source=filenames[2]),\n",
    "    dict(year=2021, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859253bf-4bb0-43bf-999a-e1abb1f6983b",
   "metadata": {},
   "source": [
    "If you take a look into the Amazon 10-Ks, the first 4 pages are all the very similar and may skew the responses if you they are kept in the embeddings. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 10-K (first 4 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac21b76-14b4-4c64-8cfe-408d877426c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "# Iterate over each PDF file\n",
    "for idx, local_pdf in enumerate(local_pdfs):\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    \n",
    "    if idx == 0:\n",
    "        # Keep the first 4 pages for the first document\n",
    "        for pagenum in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "    else:\n",
    "        # Remove the first 4 pages for other documents\n",
    "        for pagenum in range(4, len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "    # Write the modified content to a new file\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fa7ac-6605-4842-87f8-7cc844e01c12",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80600818-b41c-45b2-b86b-2e4c69271ed6",
   "metadata": {},
   "source": [
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cbcc0-908c-4fa3-adab-f9eae209cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "\n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27bc3e-9d2b-47cc-9764-9c8b16200b06",
   "metadata": {},
   "source": [
    "Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6183e-9ceb-429c-8042-935d56acf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e72f-6162-4aa5-9aa9-0bbb29b026ea",
   "metadata": {},
   "source": [
    "We had 4 PDF documents which have been split into smaller ~500 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550cd-1f0f-445b-9c3b-dbd7abf5294f",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cd64a-ba7a-419e-953a-307704772f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_embedding = np.array(sagemaker_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efb1a-8586-4a74-bbd4-b52d1730693b",
   "metadata": {
    "tags": []
   },
   "source": [
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993d824-e269-4e0c-80f0-1b3bde27302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    sagemaker_embeddings,\n",
    ")\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95675c9-7116-4bd3-ba63-30963750e36f",
   "metadata": {},
   "source": [
    "## Question Answering with LangChain Vector Store Wrapper\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d335bb-63bf-4870-8e6e-019a1b7c005d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2ff92-84dd-4ad7-856c-643e342cf5cd",
   "metadata": {},
   "source": [
    "*Note: In this example we are using `Llama 3 8B Instruct` as the LLM under Amazon SageMaker, this particular model performs best if the inputs are provided under `<|begin_of_text|><|start_header_id|>system<|end_header_id|>`, `{{system_message}}`, `<|eot_id|><|start_header_id|>user<|end_header_id|>`, `{{user_message}}`, and the model is requested to generate an output after `<|eot_id|><|start_header_id|>assistant<|end_header_id|>`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5188-4caa-414c-a76e-33ae8cec19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{query}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa490dbc-9abe-4bb6-a26c-26f6d10591ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37091fb6-02e7-4601-8210-83129076a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = wrapper_store_faiss.query(question=PROMPT.format(query=query), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bc809-6c6b-4989-8835-1cc7fcc731f7",
   "metadata": {},
   "source": [
    "We can ask another question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94688dd-2ef4-462e-9186-1a828b02e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"How much square footage did Amazon have in North America in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac316dbb-94c8-4bb6-872a-6c027df8f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = wrapper_store_faiss.query(question=PROMPT.format(query=query_2), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b411-b7ce-48e7-bda4-d8d2abcefc53",
   "metadata": {},
   "source": [
    "### Regular Retriever Chain\n",
    "---\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/) which can be specific to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b9a12-3407-47ce-8457-437059d84788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "This is a conversation between an AI assistant and a Human.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "#### Context ####\n",
    "{context}\n",
    "#### End of Context ####\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff4d62-82a5-4f07-9ed4-828b468b6356",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304e6ab-a8ed-4d85-be9b-35ed60721a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba165d2c-8ee5-40e7-91b5-94be5bdc9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some of the risk factors associated to Amazon?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4a7f8-6bc5-4ede-bacb-ffcd656e9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was Amazon involved in any lawsuits in 2022? What were they?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff4635-7987-42a2-aea6-b3d90110c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was Amazon's revenue in 2021?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4e722-9575-4be8-831b-974708f70e3a",
   "metadata": {},
   "source": [
    "## Improving RAG Responses for Long Documents with Langchain\n",
    "\n",
    "Let’s look at a more advanced RAG option with the help of ParentDocumentRetriever. When working with document retrieval, you may encounter a trade-off between storing small chunks of a document for accurate embeddings and larger documents to preserve more context. The parent document retriever strikes that balance by splitting and storing small chunks of data. Parent document retrievers provide LLMs with a twofold advantage: the specificity of child document embeddings for precise and relevant information retrieval, coupled with the invocation of parent documents for response generation, which enriches the LLM’s outputs with a layered and thorough context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c868c8-cf34-42f6-a06f-443e04a195f4",
   "metadata": {},
   "source": [
    "### Parent Document Retriever Chain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26568540-3442-41dc-ae12-ca8e624d5fff",
   "metadata": {},
   "source": [
    "In this scenario, let's have a look at a more advanced rag option with the help of [ParentDocumentRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever). When working with document retrieval, you may encounter a trade-off between storing small chunks of a document for accurate embeddings and larger documents to preserve more context. The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. \n",
    "\n",
    "The `ParentDocumentRetriever` uses an [InMemoryStore](https://api.python.langchain.com/en/v0.1.4/storage/langchain.storage.in_memory.InMemoryBaseStore.html) to store and manage the parent documents. By working with both parent and child documents, this approach aims to balance accurate embeddings with contextual information, providing more meaningful and relevant retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166d969-e079-4eaa-9479-b69a0ef05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7358d-00b3-4778-a981-8decddb5e1ec",
   "metadata": {},
   "source": [
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5f339-b513-4ba5-b262-82d504dbd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    child_splitter.split_documents(documents),\n",
    "    sagemaker_embeddings,\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d830d8-5101-4103-8958-f960c2728cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore_faiss,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15243983-e5e5-4024-9ecb-6b965827684c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(documents, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46bb5a-efb7-47d4-8d0d-3ec1afa95f25",
   "metadata": {},
   "source": [
    "Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7d44f-0669-4d22-9efc-64ee3b0ef247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore_faiss.similarity_search(\"What was Amazon's revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7c3d6-7eda-4d1a-8c0b-9889daec7f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2a1c9-ea18-47a6-ade7-d801c712e700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a54e4-0ac6-48b2-afb2-7568a0364ac0",
   "metadata": {},
   "source": [
    "Let’s now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54413ac3-0012-4b31-b926-1de798e3572c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"What was Amazon's revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c388a-494a-4a3f-aed1-a59e3bbccb04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab565fb-0240-4048-818a-da6a6b56b9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3152a7-d33d-4eaf-81fc-15e174d119d1",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ParentDocumentRetriever`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d573ec9-5865-4dc7-9a47-183b70afce7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73af7-fde2-4f30-8dba-c9f3d1dfac83",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320966d5-e056-452d-81ed-1cb67c873f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17a9cb-f084-4db5-9047-1d7b1244fa47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are some of the risk factors associated to Amazon?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e42c67-99f3-4722-aab4-48e486147b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was Amazon involved in any lawsuits in 2022? What were they?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54db0b8-bc30-4010-aae8-f4ff371e3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the net sales change from 2022 to 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4c55-0f80-47eb-83f7-6ae28fedf57f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32616f-9b97-4960-9ccc-47f20a95dcc6",
   "metadata": {},
   "source": [
    "Congratulations on completing the advanced retrieval augmented generation with `Llama3 8b`! Through this notebook, you were able to learn how to leverage the power of `Llama3 8b` with the precision of retrieval methods from `LangChain`. While building our QnA application, we are able to see that in contexts like detailing AWS’s transition from a simple service to a complex, multi-billion-dollar entity, or explaining Amazon's strategic successes, the Regular Retriever Chain lacks the precision the more sophisticated techniques offer, leading to less targeted information. For customers in industries such as HCLS, Telecommunications, and FSI who are looking to implement RAG in their applications,  the limitations of the Regular Retriever Chain in providing precision, avoiding redundancy, and effectively compressing information make them less suited to fulfilling these needs compared to the Parent Document Retriever that is able to distill vast amounts of information into the concentrated, impactful insights that customers need, while helping improve price performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d260d-8771-41ae-b173-0f81228b28dc",
   "metadata": {},
   "source": [
    "In the above implementation of Advanced RAG based Question Answering we have explored the following concepts and how to implement them using Amazon SageMaker JumpStart and it's LangChain integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d08cb-6828-4089-a8fc-55c12637a2f1",
   "metadata": {},
   "source": [
    "- Deploying models on Amazon SageMaker JumpStart\n",
    "- Setting up `Llama3-8b` and `BGE Large En v1.5` with LangChain\n",
    "- Loading documents of different kind and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question using the following approaches from LangChain\n",
    "    - Regular Retrieval Chain\n",
    "    - Parent Document Retriever Chain\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0319f2-5309-45ba-a15c-2ff7b3720133",
   "metadata": {},
   "source": [
    "### Take-aways\n",
    "---\n",
    "- Experiment with different retrieval techniques\n",
    "- Leverage `Llama3-8b` and `BGE Large En v1.5` models available under Amazon SageMaker JumpStart\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e4cb2-5ad3-439c-90ea-f538bc9872e8",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7aae-56a6-4c63-8cea-9ac73b930eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "llm_predictor.delete_model()\n",
    "llm_predictor.delete_endpoint()\n",
    "embedding_predictor.delete_model()\n",
    "embedding_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7dc0f-cf00-4bd7-a20c-8952ef75fa86",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
