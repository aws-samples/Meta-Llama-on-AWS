{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers  sagemaker seaborn sentence-transformers nltk scikit-learn \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 finetuning for Bedrock\n",
    "## Architecture\n",
    "This diagram illustrates an end-to-end ML workflow where a SageMaker Pipeline processes, trains, and evaluates a model using HuggingFace containers, then registers it before deploying to Amazon Bedrock through a Lambda function for inference, with model artifacts stored in S3 throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture Diagram](Llama3_finetuning_bedrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The next line of code uses a API token to login in the Huggingface account to use the model weights. You need to have access to \"meta-llama/Llama-3.2-3B-Instruct\" to use meta llama 3.2 3B model.\n",
    "- [Hugging Face Access Tokens Documentation](https://huggingface.co/docs/hub/en/security-tokens).\n",
    "- [Getting access to the mode](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172)\n",
    "- [meta llama 3.2 3B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20220929T161862')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sess.boto_region_name)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_S3Uri=\"s3://jumpstart-cache-prod-us-west-2/training-datasets/oasst_top/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = S3Downloader.download(s3_uri=dataset_S3Uri, local_path=f\"dataset/\")\n",
    "print(f\"Training config downloaded to:\")\n",
    "print(train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "# upload the model yaml file to s3\n",
    "train_dataset_path = \"dataset/train.jsonl\"\n",
    "train_s3_path = S3Uploader.upload(local_path=train_dataset_path, desired_s3_uri=f\"{input_path}/dataset\")\n",
    "\n",
    "print(f\"Training dataset uploaded to:\")\n",
    "print(train_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CacheConfig\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "#role = sagemaker.get_execution_role()\n",
    "# Define pipeline parameters\n",
    "region=sagemaker_session.boto_region_name\n",
    "model_name = \"llama3-qa-model\"\n",
    "instance_type_preprocessing = \"ml.m5.large\"\n",
    "instance_count = 1\n",
    "# Cache configuration to improve pipeline execution time\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    instance_type=instance_type_preprocessing,\n",
    "    instance_count=instance_count,\n",
    "    base_job_name=\"llama3-qa-preprocessing\",\n",
    "    role=role,\n",
    "    max_runtime_in_seconds=3600,  # Set a maximum runtime of 1 hour,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "inputs = [\n",
    "    ProcessingInput(source=train_s3_path, destination=\"/opt/ml/processing/input\"),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "    ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessQADataset\",\n",
    "    processor=preprocessing_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    \n",
    "    code=\"scripts/preprocessing/preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3_2_3B_fsdp_lora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Llama-3.2-3B-Instruct\"# Hugging Face model id\n",
    "max_seq_length:  512 #2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test\"   # path to where SageMaker saves test dataset\n",
    "#output_dir: \"/opt/ml/model\"            # path to where SageMaker will upload the model \n",
    "output_dir: \"/tmp/llama3\"            # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 10                   # number of training epochs\n",
    "per_device_train_batch_size: 16         # batch size per device during training\n",
    "per_device_eval_batch_size: 16          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: false                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"llama_3_2_3B_fsdp_lora.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import time\n",
    "\n",
    "# define Training Job Name with timestamp\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "job_name = f'llama3-8B-exp1-{timestamp}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'training/train_fsdp_lora.py',      # train script\n",
    "    model_dir            = '/opt/ml/model',\n",
    "    source_dir           = 'scripts/',  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.48xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.16xlarge',  # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_2_3B_fsdp_lora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    "    \n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=job_name,\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "        ),\n",
    "        \"config\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=train_config_s3_path,\n",
    "        ),\n",
    "        \"test\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "# Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hfp = HuggingFaceProcessor(\n",
    "    role=role, \n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.16xlarge',  # Use 'local' for local mode, or specify an instance type like 'ml.g4dn.xlarge' for SageMaker\n",
    "    transformers_version='4.36.0',  # Adjust version as needed\n",
    "    pytorch_version='2.1.0',  # Adjust version as needed\n",
    "    py_version= 'py310',\n",
    "    base_job_name='llama3-eval',\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"EvaluateLlama3Model\",\n",
    "    processor=hfp,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        ),\n",
    "        ProcessingInput(source=\"scripts/evaluation\", destination='/opt/ml/processing/input/code'),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"scripts/evaluation/evaluate.py\",\n",
    "    \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eval_args = hfp.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/input/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/data\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=\"./scripts/evaluation\",\n",
    "            destination=\"/opt/ml/processing/input/code\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/output\",destination=f's3://{sagemaker_session_bucket}/llama3-8B-exp1-{timestamp}/output/evaluation'),\n",
    "    ],\n",
    "    code=\"evaluate.py\",\n",
    "    source_dir=\"scripts/evaluation\",\n",
    "    arguments=['--bootstrap', '/opt/ml/processing/input/code/bootstrap.sh']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluation_step = ProcessingStep(name=\"EvaluateLlama3Model\", step_args=eval_args)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Register Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\",\n",
    "  region=region,\n",
    "  version=\"2.0\",\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "llm_model=HuggingFaceModel(\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py310\",\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model step\n",
    "llama_model_step = CreateModelStep(\n",
    "    name=\"CreateLlama3ModelStep\",\n",
    "    model=llm_model,\n",
    "    inputs=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    depends_on=[training_step],\n",
    ")\n",
    "    \n",
    "# Crete a RegisterModel step, which registers the model with Sagemaker Model Registry.\n",
    "model_package_group_name = \"Llama3Models\" \n",
    "step_register_model = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=llm_model,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.g5.12xlarge\"],\n",
    "    transform_instances=[\"ml.g5.12xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    depends_on=[training_step],\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Deployment step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('boto3-layer/python', exist_ok=True)\n",
    "\n",
    "# Install boto3 into the layer directory\n",
    "subprocess.check_call([\n",
    "    'pip', 'install', 'boto3==1.35.16', '-t', 'boto3-layer/python',\n",
    "    '--upgrade', '--no-cache-dir'\n",
    "])\n",
    "\n",
    "# Create zip file\n",
    "shutil.make_archive('boto3-layer', 'zip', 'boto3-layer')\n",
    "\n",
    "# Upload to AWS as a Lambda layer\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "with open('boto3-layer.zip', 'rb') as zip_file:\n",
    "    response = lambda_client.publish_layer_version(\n",
    "        LayerName='boto3-latest',\n",
    "        Description='Latest Boto3 layer',\n",
    "        Content={\n",
    "            'ZipFile': zip_file.read()\n",
    "        },\n",
    "        CompatibleRuntimes=['python3.10', 'python3.11']\n",
    "    )\n",
    "\n",
    "print(f\"Layer ARN: {response['LayerArn']}\")\n",
    "print(f\"Layer Version ARN: {response['LayerVersionArn']}\")\n",
    "lambda_layer_arn=response['LayerVersionArn']\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree('boto3-layer')\n",
    "os.remove('boto3-layer.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Role and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_execution_role(role_name, training_bucket, account_id, region):\n",
    "    iam = boto3.client('iam')\n",
    "    \n",
    "    # Define the trust relationship\n",
    "    trust_relationship = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define Bedrock permissions policy\n",
    "    bedrock_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:CreateModelImportJob\",\n",
    "                    \"bedrock:GetModelImportJob\",\n",
    "                    \"bedrock:ListModelImportJobs\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"iam:PassRole\"\n",
    "                ],\n",
    "                \"Resource\": f\"arn:aws:iam::{account_id}:role/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"iam:PassedToService\": \"bedrock.amazonaws.com\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define S3 permissions\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{training_bucket}\",\n",
    "                    f\"arn:aws:s3:::{training_bucket}/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def attach_policies(role_name):\n",
    "        # Attach the Bedrock permissions policy\n",
    "        iam.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName='BedrockAccessPolicy',\n",
    "            PolicyDocument=json.dumps(bedrock_policy)\n",
    "        )\n",
    "        print(\"Attached Bedrock permissions policy\")\n",
    "        \n",
    "        # Attach necessary AWS managed policies for Lambda basic execution\n",
    "        try:\n",
    "            iam.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "            )\n",
    "            print(\"Attached Lambda basic execution policy\")\n",
    "        except iam.exceptions.EntityAlreadyExistsException:\n",
    "            print(\"Lambda basic execution policy already attached\")\n",
    "        \n",
    "        # Attach S3 policy\n",
    "        iam.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName='S3AccessPolicy',\n",
    "            PolicyDocument=json.dumps(s3_policy)\n",
    "        )\n",
    "        print(\"Attached S3 access policy\")\n",
    "    \n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship),\n",
    "            Description=\"Execution role for Lambda function and Bedrock model import jobs\"\n",
    "        )\n",
    "        \n",
    "        role_arn = response['Role']['Arn']\n",
    "        print(f\"Created IAM role: {role_arn}\")\n",
    "        \n",
    "        # Attach all policies for new role\n",
    "        attach_policies(role_name)\n",
    "        \n",
    "        return role_arn\n",
    "    \n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f\"IAM role {role_name} already exists. Retrieving its ARN.\")\n",
    "        role = iam.get_role(RoleName=role_name)\n",
    "        \n",
    "        # Update the trust relationship\n",
    "        iam.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        \n",
    "        # Attach or update policies for existing role\n",
    "        attach_policies(role_name)\n",
    "        \n",
    "        return role['Role']['Arn']\n",
    "\n",
    "# Usage\n",
    "role_name = \"LambdaBedrockExecutionRole\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "execution_role_arn = create_lambda_execution_role(role_name, training_bucket, account_id, region)\n",
    "print(f\"Execution Role ARN: {execution_role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def handle_client_error(func, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def create_or_update_role(role_name, trust_relationship, permission_policy, iam_client=None, account_id=None):\n",
    "    iam = iam_client or boto3.client('iam')\n",
    "    account_id = account_id or boto3.client('sts').get_caller_identity()['Account']\n",
    "    \n",
    "    # Check and update/create role\n",
    "    role = handle_client_error(iam.get_role, RoleName=role_name)\n",
    "    if role:\n",
    "        iam.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        print(f\"Updated existing role: {role_name}\")\n",
    "    else:\n",
    "        iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        print(f\"Created new role: {role_name}\")\n",
    "\n",
    "    # Handle policy\n",
    "    policy_name = f\"{role_name}Policy\"\n",
    "    policy_arn = f\"arn:aws:iam::{account_id}:policy/{policy_name}\"\n",
    "    \n",
    "    # Attach or update policy\n",
    "    policy = handle_client_error(iam.get_policy, PolicyArn=policy_arn)\n",
    "    if policy:\n",
    "        iam.create_policy_version(\n",
    "            PolicyArn=policy_arn,\n",
    "            PolicyDocument=json.dumps(permission_policy),\n",
    "            SetAsDefault=True\n",
    "        )\n",
    "        # Cleanup old versions\n",
    "        versions = iam.list_policy_versions(PolicyArn=policy_arn)['Versions']\n",
    "        for version in versions:\n",
    "            if not version['IsDefaultVersion']:\n",
    "                iam.delete_policy_version(\n",
    "                    PolicyArn=policy_arn,\n",
    "                    VersionId=version['VersionId']\n",
    "                )\n",
    "        print(f\"Updated existing policy: {policy_name}\")\n",
    "    else:\n",
    "        iam.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(permission_policy)\n",
    "        )\n",
    "        print(f\"Created new policy: {policy_name}\")\n",
    "\n",
    "    # Attach policy to role if not already attached\n",
    "    attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    print(f\"Attached policy to role: {role_name}\")\n",
    "\n",
    "    return iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "\n",
    "\n",
    "# Set up variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"\n",
    "\n",
    "# Define policies\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "        \"Action\": \"sts:AssumeRole\",\n",
    "        \"Condition\": {\n",
    "            \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "            \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "        \"Resource\": [f\"arn:aws:s3:::{training_bucket}\", f\"arn:aws:s3:::{training_bucket}/*\"],\n",
    "        \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "role_arn = create_or_update_role(role_name, trust_relationship, permission_policy)\n",
    "print(f\"Role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "# Create Lambda function instance\n",
    "lambda_func = Lambda(\n",
    "    function_name=\"bedrock-model-import\",\n",
    "    execution_role_arn=execution_role_arn,\n",
    "    script=\"scripts/lambda/bedrock_model_import.py\",\n",
    "    handler='bedrock_model_import.lambda_handler',\n",
    "    timeout=900,  # 15 minutes, adjust as needed\n",
    "    memory_size=128,\n",
    "    runtime='python3.12',\n",
    "    layers=[lambda_layer_arn],  # Your boto3 layer ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "# Define the outputs\n",
    "lambda_outputs = [\n",
    "    LambdaOutput(output_name=\"model_arn\", output_type=LambdaOutputTypeEnum.String)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register_model.properties.ModelPackageArn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Lambda step\n",
    "lambda_step = LambdaStep(\n",
    "    name=\"BedrockModelImport\",\n",
    "    lambda_func=lambda_func,\n",
    "    inputs={\n",
    "        \"model_uri\": training_step.properties.ModelArtifacts.S3ModelArtifacts,  # Use the output from the training step\n",
    "        \"role_arn\": role,\n",
    "        \"model_name\": model_name\n",
    "    },\n",
    "    outputs=lambda_outputs,\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"1d\"),\n",
    "    depends_on=[step_register_model]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\"model_name\": \"llama3_model\",\n",
    "\"model_uri\":training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "\"role_arn\": role_arn,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline = Pipeline(\n",
    "        name=\"Llama3-QAPipeline\",\n",
    "        steps=[preprocessing_step, training_step,step_register_model,lambda_step ],\n",
    "        parameters=[role, model_name],\n",
    "        sagemaker_session=pipeline_session,\n",
    "    )\n",
    "    logging.info(\"Pipeline created successfully\")\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    logging.info(\"Pipeline upserted successfully\")\n",
    "\n",
    "    execution = pipeline.start()\n",
    "    logging.info(\"Pipeline started successfully\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    logging.error(f\"ValueError occurred: {str(ve)}\")\n",
    "    logging.error(f\"Error occurred in pipeline definition: {pipeline.definition()}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    logging.error(f\"Error type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_pipeline_status(execution):\n",
    "    try:\n",
    "        return execution.describe()['PipelineExecutionStatus']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting pipeline status: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_step_statuses(execution):\n",
    "    try:\n",
    "        steps = execution.list_steps()\n",
    "        return {step['StepName']: step['StepStatus'] for step in steps}\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting step statuses: {e}\")\n",
    "        return {}\n",
    "\n",
    "def is_pipeline_finished(status):\n",
    "    return status in ['Succeeded', 'Completed', 'Failed', 'Stopped']\n",
    "\n",
    "def print_progress(status, step_statuses):\n",
    "    print(f\"\\nPipeline status: {status}\")\n",
    "    print(\"Step statuses:\")\n",
    "    for step, status in step_statuses.items():\n",
    "        print(f\"  {step}: {status}\")\n",
    "\n",
    "def monitor_pipeline_execution(execution, check_interval=60):\n",
    "    print(\"Pipeline execution started.\")\n",
    "    print(\"Status updates (checking every minute):\")\n",
    "\n",
    "    previous_step_statuses = {}\n",
    "    while True:\n",
    "        status = get_pipeline_status(execution)\n",
    "        if status is None:\n",
    "            print(\"Failed to get pipeline status. Retrying...\")\n",
    "            time.sleep(check_interval)\n",
    "            continue\n",
    "\n",
    "        step_statuses = get_step_statuses(execution)\n",
    "        \n",
    "        if step_statuses != previous_step_statuses:\n",
    "            print_progress(status, step_statuses)\n",
    "            previous_step_statuses = step_statuses\n",
    "        else:\n",
    "            print(\".\", end='', flush=True)\n",
    "        \n",
    "        if is_pipeline_finished(status):\n",
    "            break\n",
    "\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "    print(\"\\nPipeline execution finished.\")\n",
    "    print_progress(status, step_statuses)\n",
    "\n",
    "# Usage example:\n",
    "monitor_pipeline_execution(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step.properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the model ARN from the Lambda step output\n",
    "model_arn = execution.step_outputs['BedrockModelImportStep']['model_arn']\n",
    "\n",
    "# Now you can use this model_arn to evaluate the model in Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def import_model_to_bedrock(role_arn, model_uri, model_name=\"llama3_sagemaker\", region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Call the Lambda function to import a model to Bedrock\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    role_arn : str\n",
    "        The ARN of the IAM role to be used for the import\n",
    "    model_uri : str\n",
    "        The S3 URI where the model is stored\n",
    "    model_name : str, optional\n",
    "        Name for the imported model (default: \"llama3_sagemaker\")\n",
    "    region : str, optional\n",
    "        AWS region (default: \"us-west-2\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Response from the Lambda function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Lambda client\n",
    "        lambda_client = boto3.client('lambda', region_name=region)\n",
    "        \n",
    "        # Prepare the payload\n",
    "        payload = {\n",
    "            \"role_arn\": role_arn,\n",
    "            \"model_uri\": model_uri,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "        \n",
    "        # Convert payload to JSON string\n",
    "        payload_json = json.dumps(payload)\n",
    "        \n",
    "        # Replace with your Lambda function name\n",
    "        function_name = \"bedrock-model-import\"\n",
    "        \n",
    "        # Invoke Lambda function\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=function_name,\n",
    "            InvocationType='RequestResponse',\n",
    "            Payload=payload_json\n",
    "        )\n",
    "        \n",
    "        # Read and parse the response\n",
    "        response_payload = json.loads(response['Payload'].read().decode('utf-8'))\n",
    "        \n",
    "        # Check if the job requires follow-up\n",
    "        if response_payload.get('requires_follow_up'):\n",
    "            print(\"Job started successfully but still in progress.\")\n",
    "            print(f\"Job Name: {response_payload.get('job_name')}\")\n",
    "            print(f\"Job ARN: {response_payload.get('job_arn')}\")\n",
    "        \n",
    "        return response_payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Lambda function: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def check_import_status(job_name, region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Check the status of a model import job\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    job_name : str\n",
    "        The name of the import job to check\n",
    "    region : str, optional\n",
    "        AWS region (default: \"us-west-2\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Current status of the import job\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bedrock = boto3.client('bedrock', region_name=region)\n",
    "        response = bedrock.list_model_import_jobs(\n",
    "            nameContains=job_name,\n",
    "            sortBy='CreationTime',\n",
    "            sortOrder='Descending'\n",
    "        )\n",
    "        \n",
    "        if response['modelImportJobSummaries']:\n",
    "            job = response['modelImportJobSummaries'][0]\n",
    "            return {\n",
    "                'status': job['status'],\n",
    "                'model_arn': job.get('importedModelArn'),\n",
    "                'error': job.get('failureReason', '')\n",
    "            }\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking job status: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call\n",
    "response = import_model_to_bedrock(\n",
    "    role_arn='arn:aws:iam::786045444066:role/Sagemaker_Bedrock_import_role',\n",
    "    model_uri='s3://sagemaker-us-west-2-786045444066/pipelines-9h6uct6juob9-llama3-8B-exp1-20241-bJ8cv1J5NG/output/model',\n",
    "    model_name='my_custom_model'\n",
    ")\n",
    "\n",
    "print(f\"Response: {json.dumps(response, indent=2)}\")\n",
    "\n",
    "# If the job is still running, check its status\n",
    "if response.get('requires_follow_up'):\n",
    "    job_name = response['job_name']\n",
    "    while True:\n",
    "        status = check_import_status(job_name)\n",
    "        print(f\"Current status: {status}\")\n",
    "        if status['status'] in ['Completed', 'Failed', 'Stopped']:\n",
    "            break\n",
    "        time.sleep(60)  # Wait for 60 seconds before checking again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arn='arn:aws:bedrock:us-west-2:786045444066:imported-model/686s53mpwdkw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import nltk\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Download required NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    print(\"Successfully downloaded punkt tokenizer\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "# Alternative tokenization function in case NLTK fails\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenization fallback\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Initialize BERT model for semantic similarity with increased timeout\n",
    "try:\n",
    "    print(\"Loading BERT model...\")\n",
    "    # Increase timeout for model download\n",
    "    requests.adapters.DEFAULT_TIMEOUT = 60  # Increase timeout to 60 seconds\n",
    "    \n",
    "    # Create a session with custom timeout\n",
    "    session = requests.Session()\n",
    "    session.request = lambda *args, **kwargs: requests.Session.request(\n",
    "        session, *args, **{**kwargs, 'timeout': 60}\n",
    "    )\n",
    "    \n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2', \n",
    "                               device='cpu',  # Explicitly use CPU\n",
    "                               cache_folder='./model_cache')  # Local cache directory\n",
    "    print(\"BERT model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading BERT model: {e}\")\n",
    "    print(\"Falling back to simpler similarity metrics...\")\n",
    "    model = None\n",
    "\n",
    "\n",
    "def calculate_similarity_metrics(expected, actual):\n",
    "    \"\"\"\n",
    "    Calculate multiple similarity metrics between expected and actual responses\n",
    "    \"\"\"\n",
    "    # Ensure inputs are strings\n",
    "    expected = str(expected)\n",
    "    actual = str(actual)\n",
    "    \n",
    "    # String similarity (simple ratio)\n",
    "    string_similarity = SequenceMatcher(None, expected, actual).ratio()\n",
    "    \n",
    "    # BLEU score calculation\n",
    "    try:\n",
    "        # Tokenize both texts\n",
    "        try:\n",
    "            reference_tokens = word_tokenize(expected.lower())\n",
    "            candidate_tokens = word_tokenize(actual.lower())\n",
    "        except:\n",
    "            # Fallback to simple tokenization\n",
    "            print(\"Falling back to simple tokenization\")\n",
    "            reference_tokens = simple_tokenize(expected)\n",
    "            candidate_tokens = simple_tokenize(actual)\n",
    "        \n",
    "        # Create reference as a list of tokens\n",
    "        references = [reference_tokens]\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        from nltk.translate.bleu_score import SmoothingFunction\n",
    "        smoother = SmoothingFunction()\n",
    "        \n",
    "        # Calculate final BLEU score with equal weights\n",
    "        bleu_score = sentence_bleu(references, candidate_tokens,\n",
    "                                 weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                                 smoothing_function=smoother.method1)\n",
    "        \n",
    "        print(\"\\nBLEU Score Details:\")\n",
    "        print(f\"Reference tokens: {references[0][:10]}...\")\n",
    "        print(f\"Candidate tokens: {candidate_tokens[:10]}...\")\n",
    "        print(f\"BLEU Score: {bleu_score:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU score: {e}\")\n",
    "        print(f\"Expected text: {expected[:100]}...\")\n",
    "        print(f\"Actual text: {actual[:100]}...\")\n",
    "        bleu_score = 0\n",
    "    \n",
    "    # Semantic similarity using BERT embeddings\n",
    "    try:\n",
    "        if model is not None:\n",
    "            embeddings = model.encode([expected, actual])\n",
    "            embedding1 = embeddings[0].reshape(1, -1)\n",
    "            embedding2 = embeddings[1].reshape(1, -1)\n",
    "            semantic_similarity = float(cosine_similarity(embedding1, embedding2)[0][0])\n",
    "        else:\n",
    "            semantic_similarity = string_similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating semantic similarity: {e}\")\n",
    "        semantic_similarity = 0\n",
    "    \n",
    "    return {\n",
    "        'string_similarity': string_similarity,\n",
    "        'bleu_score': bleu_score,\n",
    "        'semantic_similarity': semantic_similarity\n",
    "    }\n",
    "\n",
    "def load_conversations_from_s3(bucket_name, file_key, num_samples=None):\n",
    "    \"\"\"\n",
    "    Load conversations from JSON Lines file in S3 with optional sampling\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    line_count = 0\n",
    "    valid_conversations = 0\n",
    "    \n",
    "    print(f\"Attempting to read file from S3: s3://{bucket_name}/{file_key}\")\n",
    "    try:\n",
    "        # Initialize S3 client\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Get the object from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        \n",
    "        # Read the content line by line\n",
    "        for line in response['Body'].iter_lines():\n",
    "            line_count += 1\n",
    "            try:\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                data = json.loads(decoded_line)\n",
    "                \n",
    "                if 'messages' in data:\n",
    "                    conversations.append(data['messages'])\n",
    "                    valid_conversations += 1\n",
    "                    if valid_conversations % 100 == 0:\n",
    "                        print(f\"Loaded {valid_conversations} valid conversations\")\n",
    "                        \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_count}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"\\nLoading complete:\")\n",
    "        print(f\"Total lines processed: {line_count}\")\n",
    "        print(f\"Valid conversations found: {valid_conversations}\")\n",
    "        \n",
    "        # Apply sampling if specified\n",
    "        if num_samples and num_samples < len(conversations):\n",
    "            conversations = random.sample(conversations, num_samples)\n",
    "            print(f\"Sampled {num_samples} conversations for testing\")\n",
    "            \n",
    "        return conversations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from S3: {e}\")\n",
    "        return []\n",
    "    \n",
    "    \n",
    "\n",
    "class BedrockModelTester:\n",
    "    def __init__(self, model_id, region='us-west-2', max_retries=100):\n",
    "        self.model_id = model_id\n",
    "        self.region = region\n",
    "        self.config = Config(\n",
    "            retries={\n",
    "                'total_max_attempts': max_retries,\n",
    "                'mode': 'standard'\n",
    "            }\n",
    "        )\n",
    "        self.session = boto3.session.Session()\n",
    "        self.br_runtime = self.session.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=self.region,\n",
    "            config=self.config\n",
    "        )\n",
    "    \n",
    "    def invoke_model(self, conversation):\n",
    "        \"\"\"Invoke the model with a conversation\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get the last user message from the conversation\n",
    "            user_messages = [msg for msg in conversation if msg['role'] == 'user']\n",
    "            if not user_messages:\n",
    "                raise ValueError(\"No user message found in conversation\")\n",
    "            \n",
    "            last_user_message = user_messages[-1]['content']\n",
    "            \n",
    "            # Prepare the prompt\n",
    "            prompt = last_user_message\n",
    "            \n",
    "            # Make the API call similar to your working code\n",
    "            response = self.br_runtime.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                body=json.dumps({'prompt': prompt}),\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Parse response similar to your working code\n",
    "            response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "            \n",
    "            # Get expected response\n",
    "            expected_response = next((msg['content'] for msg in conversation if msg['role'] == 'assistant'), None)\n",
    "            \n",
    "            # Calculate similarity metrics\n",
    "            similarity_metrics = calculate_similarity_metrics(expected_response, str(response_body)) if expected_response else {}\n",
    "            \n",
    "            return {\n",
    "                'conversation': conversation,\n",
    "                'last_user_message': last_user_message,\n",
    "                'expected_response': expected_response,\n",
    "                'model_response': response_body,\n",
    "                'similarity_metrics': similarity_metrics,\n",
    "                'latency': end_time - start_time,\n",
    "                'status': 'success'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in invoke_model: {str(e)}\")\n",
    "            return {\n",
    "                'conversation': conversation,\n",
    "                'error': str(e),\n",
    "                'status': 'error'\n",
    "            }\n",
    "\n",
    "def analyze_and_visualize_results(results):\n",
    "    \"\"\"Analyze results and create visualizations with similarity metrics\"\"\"\n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame([{\n",
    "        'status': r['status'],\n",
    "        'latency': r.get('latency', 0),\n",
    "        'string_similarity': r.get('similarity_metrics', {}).get('string_similarity', 0),\n",
    "        'bleu_score': r.get('similarity_metrics', {}).get('bleu_score', 0),\n",
    "        'semantic_similarity': r.get('similarity_metrics', {}).get('semantic_similarity', 0),\n",
    "        'has_system_message': 'system_message' in r and r['system_message'] is not None\n",
    "    } for r in results if r['status'] == 'success'])\n",
    "    \n",
    "    # Check if we have any successful results\n",
    "    if len(df) == 0:\n",
    "        print(\"No successful results to analyze!\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "        \n",
    "    try:\n",
    "        # Create visualizations\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Status distribution\n",
    "        success_rate = len(df) / len(results) * 100\n",
    "        ax1.pie([success_rate, 100-success_rate], labels=['Success', 'Error'], autopct='%1.1f%%')\n",
    "        ax1.set_title('Test Status Distribution')\n",
    "        \n",
    "        # Latency distribution\n",
    "        if len(df) > 0:  # Only plot if we have data\n",
    "            sns.histplot(data=df, x='latency', ax=ax2)\n",
    "            ax2.set_title('Latency Distribution')\n",
    "            ax2.set_xlabel('Latency (seconds)')\n",
    "        \n",
    "        # Similarity metrics distributions\n",
    "        if len(df) > 0:  # Only plot if we have data\n",
    "            sns.boxplot(data=df[['string_similarity', 'bleu_score', 'semantic_similarity']], ax=ax3)\n",
    "            ax3.set_title('Similarity Metrics Distribution')\n",
    "            ax3.set_ylim(0, 1)\n",
    "        \n",
    "        # Scatter plot of semantic vs string similarity\n",
    "        if len(df) > 0:  # Only plot if we have data\n",
    "            sns.scatterplot(data=df, x='semantic_similarity', y='string_similarity', ax=ax4)\n",
    "            ax4.set_title('Semantic vs String Similarity')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Total tests: {len(results)}\")\n",
    "        print(f\"Successful tests: {len(df)}\")\n",
    "        print(f\"Failed tests: {len(results) - len(df)}\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(\"\\nSimilarity Metrics Statistics:\")\n",
    "            print(df[['string_similarity', 'bleu_score', 'semantic_similarity']].describe())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage example\n",
    "def run_tests_from_s3(model_id, s3_uri, num_samples=10):\n",
    "    \"\"\"\n",
    "    Run tests with specified number of samples from S3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Starting test with S3 URI: {s3_uri}\")\n",
    "        \n",
    "        # Parse S3 URI\n",
    "        s3_path = s3_uri.replace('s3://', '')\n",
    "        bucket_name = s3_path.split('/')[0]\n",
    "        prefix = '/'.join(s3_path.split('/')[1:])\n",
    "        \n",
    "        # Load conversations\n",
    "        conversations = load_conversations_from_s3(bucket_name, f\"{prefix}/test.json\", num_samples)\n",
    "        \n",
    "        if not conversations:\n",
    "            print(\"No conversations loaded!\")\n",
    "            return [], pd.DataFrame()\n",
    "        \n",
    "        # Initialize Bedrock Runtime client\n",
    "        br_runtime = boto3.client('bedrock-runtime')\n",
    "        results = []\n",
    "        \n",
    "        # Test each conversation\n",
    "        # Test each conversation\n",
    "        for idx, messages in enumerate(conversations, 1):\n",
    "            try:\n",
    "                print(f\"\\nTesting conversation {idx}/{len(conversations)}\")\n",
    "                \n",
    "                # Extract messages by role\n",
    "                system_message = next((msg['content'] for msg in messages if msg['role'] == 'system'), None)\n",
    "                user_messages = [msg for msg in messages if msg['role'] == 'user']\n",
    "                assistant_messages = [msg for msg in messages if msg['role'] == 'assistant']\n",
    "                \n",
    "                if not user_messages or not assistant_messages:\n",
    "                    print(\"Skipping conversation - missing user or assistant messages\")\n",
    "                    continue\n",
    "                \n",
    "                # Get the last user message and expected assistant response\n",
    "                input_text = user_messages[-1]['content']\n",
    "                expected_output = assistant_messages[-1]['content']\n",
    "                \n",
    "                # Construct prompt\n",
    "                prompt = f\"{system_message}\\n\\n{input_text}\" if system_message else input_text\n",
    "                \n",
    "                # Invoke model\n",
    "                body = json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'max_tokens': 2048,\n",
    "                    'temperature': 0.7,\n",
    "                    'top_p': 0.9,\n",
    "                })\n",
    "                \n",
    "                response = br_runtime.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    body=body,\n",
    "                    accept='application/json',\n",
    "                    contentType='application/json'\n",
    "                )\n",
    "                \n",
    "                # Parse response - Updated to handle the correct response format\n",
    "                response_body = json.loads(response['body'].read())\n",
    "                model_output = response_body['generation']  # Direct access to 'generation'\n",
    "                \n",
    "                # Print response details for debugging\n",
    "                print(\"\\nResponse details:\")\n",
    "                print(f\"Input text: {input_text[:100]}...\")\n",
    "                print(f\"Expected output: {expected_output[:100]}...\")\n",
    "                print(f\"Model output: {model_output[:100]}...\")\n",
    "                \n",
    "                # Calculate metrics\n",
    "                similarity_metrics = calculate_similarity_metrics(expected_output, model_output)\n",
    "                \n",
    "                # Store results with additional metadata\n",
    "                result = {\n",
    "                    'input': input_text,\n",
    "                    'expected_output': expected_output,\n",
    "                    'model_output': model_output,\n",
    "                    'system_message': system_message,\n",
    "                    'similarity_metrics': similarity_metrics,\n",
    "                    'full_conversation': messages,\n",
    "                    'response_metadata': {\n",
    "                        'generation_token_count': response_body.get('generation_token_count'),\n",
    "                        'prompt_token_count': response_body.get('prompt_token_count'),\n",
    "                        'stop_reason': response_body.get('stop_reason')\n",
    "                    }\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\" Success - Metrics:\")\n",
    "                print(f\"  String Similarity: {similarity_metrics['string_similarity']:.2f}\")\n",
    "                print(f\"  BLEU Score: {similarity_metrics['bleu_score']:.2f}\")\n",
    "                print(f\"  Semantic Similarity: {similarity_metrics['semantic_similarity']:.2f}\")\n",
    "                print(f\"  Tokens generated: {result['response_metadata']['generation_token_count']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing conversation {idx}: {str(e)}\")\n",
    "                print(f\"Full error details: {traceback.format_exc()}\")\n",
    "                print(f\"Response body: {json.dumps(response_body, indent=2)}\")  # Add this line for debugging\n",
    "                results.append({\n",
    "                    'input': input_text if 'input_text' in locals() else None,\n",
    "                    'error': str(e),\n",
    "                    'full_conversation': messages\n",
    "                })\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"Input text: {input_text[:100]}...\")\n",
    "                print(f\"Expected output: {expected_output[:100]}...\")\n",
    "                print(f\"Model output: {model_output[:100]}...\")\n",
    "                print(f\" Success - Metrics:\")\n",
    "                print(f\"  String Similarity: {similarity_metrics['string_similarity']:.2f}\")\n",
    "                print(f\"  BLEU Score: {similarity_metrics['bleu_score']:.2f}\")\n",
    "                print(f\"  Semantic Similarity: {similarity_metrics['semantic_similarity']:.2f}\")\n",
    "                \n",
    "            \n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        result_key = f\"{prefix}/test_results/bedrock_test_results_{timestamp}.json\"\n",
    "        \n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=result_key,\n",
    "            Body=json.dumps(results, indent=2)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nResults saved to s3://{bucket_name}/{result_key}\")\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        df = pd.DataFrame([{\n",
    "            'string_similarity': r['similarity_metrics']['string_similarity'],\n",
    "            'bleu_score': r['similarity_metrics']['bleu_score'],\n",
    "            'semantic_similarity': r['similarity_metrics']['semantic_similarity']\n",
    "        } for r in results if 'error' not in r])\n",
    "        \n",
    "        print(\"\\nTest Summary:\")\n",
    "        print(f\"Total tests: {len(results)}\")\n",
    "        print(f\"Successful tests: {len(df)}\")\n",
    "        if not df.empty:\n",
    "            print(f\"Average String Similarity: {df['string_similarity'].mean():.3f}\")\n",
    "            print(f\"Average BLEU Score: {df['bleu_score'].mean():.3f}\")\n",
    "            print(f\"Average Semantic Similarity: {df['semantic_similarity'].mean():.3f}\")\n",
    "        \n",
    "        return results, df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running tests: {e}\")\n",
    "        print(f\"Full error: {str(e)}\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "MODEL_ID = \"arn:aws:bedrock:us-west-2:786045444066:imported-model/686s53mpwdkw\"\n",
    "S3_URI = \"s3://sagemaker-us-west-2-786045444066/Llama3-QAPipeline/ihlejj4nmbtt/PreprocessQADataset/output/test\"  # Replace with your actual S3 URI\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "results, df = run_tests_from_s3(MODEL_ID, S3_URI, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
