{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7448c728-bd09-49e3-9223-6fe4605679e8",
   "metadata": {},
   "source": [
    "# Getting started with Llama 3 on AWS\n",
    "\n",
    "This notebook demonstrates how to use Llama 3 on AWS. We will cover the setup, configuration, and basic usage of Llama 3 for generating text. By the end of this notebook, you should be able to understand the basic workflow and how to interact with the Llama 3 model using AWS Bedrock.\n",
    "\n",
    "## Llama 3\n",
    "\n",
    "Llama 3 (Large Language Model Meta AI) is the third iteration of Meta's advanced language models, designed for tasks like text generation, translation, and summarization. Built on transformer architecture, it excels at understanding and generating human-like text by training on extensive datasets from diverse sources.\n",
    "\n",
    "## Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\n",
    "\n",
    "AWS Bedrock can be used to access and deploy Llama 3 by providing the necessary infrastructure and tools. Developers can integrate Llama 3 into their applications through Bedrock's APIs, customize it with specific datasets, and scale the deployment as needed, leveraging AWS's robust infrastructure and cost management features.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook is vetted to run on a [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/) Jupyter notebook running the latest `ipykernel`. Also, the AWS credentials, namely AWS Access Key and AWS Secret Access key, are assigned as an IAM Role to the notebook instance, hence why they are not hard-coded anywhere in the code. If you run this outside of SageMaker Studio, make the right ajustement to [authenticate your requests](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) to the Bedrock API with your AWS Access Key.\n",
    "\n",
    "## Cost\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> API calls to Amazon Bedrock will incur charges based on the tokens used and some additional features like Guardrails. Additionally, using SageMaker Studio may result in charges if you exceed the free tier: [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/), [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f467c8b-bcd7-4f5b-867c-6230ca6ccd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a couple utility functions. You can skip this section.\n",
    "\n",
    "import rich, json\n",
    "\n",
    "def print_json(data):\n",
    "    rich.print_json(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17e1c0-330a-4ecc-b8c0-920b1f6c4533",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) is a Python library that allows you to interact with AWS resources programmatically. It provides an easy way to automate tasks and manage AWS services through code. We'll use Boto3 to make requests and retrieve data from the Amazon Bedrock API. The Boto3 Bedrock SDK includes four clients designed to interact with different aspects of Bedrock:\n",
    "\n",
    "- **bedrock**: Includes APIs for controlling model management, training, and deployment.\n",
    "- **bedrock-runtime**: Includes APIs for making inference requests to models hosted in Amazon Bedrock.\n",
    "- **bedrock-agent**: Provides APIs for creating and managing agents and knowledge bases.\n",
    "- **bedrock-agent-runtime**: Includes APIs for controlling model management, training, and deployment for agents and knowledge bases.\n",
    "\n",
    "We will only use the `bedrock` and `bedrock-runtime` client in here. Let's start by installing the latest version of boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9f4ba1-1d2a-4fbb-a7e4-f2a3e2e79960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of boto3\n",
    "!python3 -m pip install --quiet --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b14fc0f-48ff-402d-b026-e1b280b6fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.143\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce16074-2b55-4816-8442-e7b005d7b411",
   "metadata": {},
   "source": [
    "To kick things off, we list all models available via Bedrock from Meta. Note that differents models will be available based on the AWS Region you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c46e91-9c58-4bd6-aa8d-b4fbb62184db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"ResponseMetadata\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RequestId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"a7997c43-2b21-455f-8480-2283950423d1\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPStatusCode\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPHeaders\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"date\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Thu, 11 Jul 2024 08:24:13 GMT\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"application/json\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-length\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"3672\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"connection\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"keep-alive\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"x-amzn-requestid\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"a7997c43-2b21-455f-8480-2283950423d1\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RetryAttempts\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelSummaries\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-13b-chat-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 Chat 13B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"PROVISIONED\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-13b-chat-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 Chat 13B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"ON_DEMAND\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-70b-chat-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 Chat 70B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-70b-chat-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 Chat 70B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"ON_DEMAND\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-13b-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 13B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"FINE_TUNING\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-13b-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 13B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-70b-v1:0:4k\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 70B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"FINE_TUNING\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama2-70b-v1\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 2 70B\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"LEGACY\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama3-8b-instruct-v1:0\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 3 8B Instruct\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"ON_DEMAND\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ACTIVE\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelArn\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"meta.llama3-70b-instruct-v1:0\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Llama 3 70B Instruct\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"providerName\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Meta\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputModalities\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"TEXT\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"responseStreamingSupported\"</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">true</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"customizationsSupported\"</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inferenceTypesSupported\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"ON_DEMAND\"</span>\n",
       "      <span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"modelLifecycle\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"status\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ACTIVE\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "  <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"ResponseMetadata\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"RequestId\"\u001b[0m: \u001b[32m\"a7997c43-2b21-455f-8480-2283950423d1\"\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPStatusCode\"\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPHeaders\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"date\"\u001b[0m: \u001b[32m\"Thu, 11 Jul 2024 08:24:13 GMT\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-type\"\u001b[0m: \u001b[32m\"application/json\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-length\"\u001b[0m: \u001b[32m\"3672\"\u001b[0m,\n",
       "      \u001b[1;34m\"connection\"\u001b[0m: \u001b[32m\"keep-alive\"\u001b[0m,\n",
       "      \u001b[1;34m\"x-amzn-requestid\"\u001b[0m: \u001b[32m\"a7997c43-2b21-455f-8480-2283950423d1\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1;34m\"RetryAttempts\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"modelSummaries\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-13b-chat-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 Chat 13B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"PROVISIONED\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-13b-chat-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 Chat 13B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"ON_DEMAND\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-70b-chat-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 Chat 70B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-70b-chat-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 Chat 70B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"ON_DEMAND\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-13b-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 13B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"FINE_TUNING\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-13b-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 13B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-70b-v1:0:4k\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 70B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"FINE_TUNING\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama2-70b-v1\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 2 70B\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"LEGACY\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama3-8b-instruct-v1:0\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 3 8B Instruct\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"ON_DEMAND\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"ACTIVE\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"modelArn\"\u001b[0m: \u001b[32m\"arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelId\"\u001b[0m: \u001b[32m\"meta.llama3-70b-instruct-v1:0\"\u001b[0m,\n",
       "      \u001b[1;34m\"modelName\"\u001b[0m: \u001b[32m\"Llama 3 70B Instruct\"\u001b[0m,\n",
       "      \u001b[1;34m\"providerName\"\u001b[0m: \u001b[32m\"Meta\"\u001b[0m,\n",
       "      \u001b[1;34m\"inputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"outputModalities\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"TEXT\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"responseStreamingSupported\"\u001b[0m: \u001b[3;92mtrue\u001b[0m,\n",
       "      \u001b[1;34m\"customizationsSupported\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"inferenceTypesSupported\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m\"ON_DEMAND\"\u001b[0m\n",
       "      \u001b[1m]\u001b[0m,\n",
       "      \u001b[1;34m\"modelLifecycle\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"status\"\u001b[0m: \u001b[32m\"ACTIVE\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "  \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set default AWS region\n",
    "default_region = \"us-east-1\"\n",
    "\n",
    "# Create a Bedrock client in the AWS Region of your choice.\n",
    "bedrock = boto3.client(\"bedrock\", region_name=default_region)\n",
    "\n",
    "# List all models from meta\n",
    "models = bedrock.list_foundation_models(\n",
    "    byProvider='Meta'  # comment this line to get all models from all providers\n",
    ")\n",
    "\n",
    "print_json(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c72108-fadc-47bb-a5cf-81eec3c1ff1c",
   "metadata": {},
   "source": [
    "The output returns many important attributes for each models:\n",
    "\n",
    "| Field                        | Description                                                              |\n",
    "|------------------------------|--------------------------------------------------------------------------|\n",
    "| `modelArn`                   | ARN that uniquely identifies the model in AWS Bedrock.                   |\n",
    "| `modelId`                    | Unique identifier for the model within AWS Bedrock.                      |\n",
    "| `modelName`                  | Name or title of the model.                                              |\n",
    "| `providerName`               | Organization or entity providing the model.                              |\n",
    "| `inputModalities`            | Types of inputs the model accepts (e.g., `'TEXT'`).                        |\n",
    "| `outputModalities`           | Types of outputs the model generates (e.g., `'TEXT'`).                     |\n",
    "| `responseStreamingSupported` | Indicates if the model supports streaming responses.                     |\n",
    "| `customizationsSupported`    | Lists any customization options available for the model.                 |\n",
    "| `inferenceTypesSupported`    | Describes the ways inference can be requested (e.g., `'ON_DEMAND'`).       |\n",
    "| `modelLifecycle`             | Current status of the model (e.g., `'ACTIVE'`).                            |\n",
    "\n",
    "\n",
    "Another way to list all models in a more readable fashion is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19701829-2121-4dca-b97f-24c659cd372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta.llama2-13b-chat-v1:0:4k\n",
      "meta.llama2-13b-chat-v1\n",
      "meta.llama2-70b-chat-v1:0:4k\n",
      "meta.llama2-70b-chat-v1\n",
      "meta.llama2-13b-v1:0:4k\n",
      "meta.llama2-13b-v1\n",
      "meta.llama2-70b-v1:0:4k\n",
      "meta.llama2-70b-v1\n",
      "meta.llama3-8b-instruct-v1:0\n",
      "meta.llama3-70b-instruct-v1:0\n"
     ]
    }
   ],
   "source": [
    "for model in models['modelSummaries']:\n",
    "    print(model['modelId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df208a1-e7ce-4883-a24d-2219113a71c2",
   "metadata": {},
   "source": [
    "## Calling a model\n",
    "\n",
    "The first example consists of a call to the Bedrock API to pass a prompt and receive an answer from the LLM. The InvokeModel API calls the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. Depending on the model, you can infer text, images, or embeddings.\n",
    "\n",
    "API documentation: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>NOTE:</b> Large language models produce non-deterministic results; you may see different outputs than those presented in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db223e27-05bd-4e0d-bac4-8a0fb8aa17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \"Hello World\" program is a simple program that serves as a starting point for learning a programming language, typically printing the text \"Hello, World!\" to the screen to demonstrate the basic syntax and functionality of the language.\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set the model ID.\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Set the prompt.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=default_region)\n",
    "\n",
    "# Embed the prompt in Llama 3's instruction format.\n",
    "# More information: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "    \n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"generation\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c2dc1-751d-4ad7-8b80-b8dd78cd4c34",
   "metadata": {},
   "source": [
    "Additionally, Llama 2 Chat, Llama 2, and Llama 3 Instruct models return the following fields for a text completion inference call alongside the text generated by the model.\n",
    " \n",
    "| Field                    | Description                                                                                                                                                                           |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `generation`             | The generated text.                                                                                                                                                                   |\n",
    "| `prompt_token_count`     | The number of tokens in the prompt.                                                                                                                                                   |\n",
    "| `generation_token_count` | The number of tokens in the generated text.                                                                                                                                           |\n",
    "| `stop_reason`            | The reason why the response stopped generating text. Possible values are: <br> - `stop`: The model has finished generating text for the input prompt. <br> - `length`: The length of the tokens for the generated text exceeds the value of `max_gen_len` in the call to `InvokeModel` (`InvokeModelWithResponseStream`, if you are streaming output). The response is truncated to `max_gen_len` tokens. Consider increasing the value of `max_gen_len` and trying again. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c60829de-934b-43d4-ba70-9afdead49cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"A \\\"Hello World\\\" program is a simple program that serves as a starting point for learning a programming language, typically printing the text \\\"Hello, World!\\\" to the screen to demonstrate the basic syntax and functionality of the language.\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"prompt_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"stop_reason\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"stop\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"generation\"\u001b[0m: \u001b[32m\"A \\\"Hello World\\\" program is a simple program that serves as a starting point for learning a programming language, typically printing the text \\\"Hello, World!\\\" to the screen to demonstrate the basic syntax and functionality of the language.\"\u001b[0m,\n",
       "  \u001b[1;34m\"prompt_token_count\"\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "  \u001b[1;34m\"generation_token_count\"\u001b[0m: \u001b[1;36m46\u001b[0m,\n",
       "  \u001b[1;34m\"stop_reason\"\u001b[0m: \u001b[32m\"stop\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_json(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98766258-1ba5-45aa-a0e1-205ff67532ba",
   "metadata": {},
   "source": [
    "The drawback of using the `InvokeModel` API lies in its requirement for different JSON request and response structures depending on the model provider. Recall the following code snippet from the example\n",
    "\n",
    "```python\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Switching from Llama 2 or Llama 3 to another model with a different prompt structure, such as from a different provider (or perhaps even a future release of Llama), would necessitate rewriting the code. This situation leads to managing diverse formats, complicating integration efforts.\n",
    "\n",
    "A better approach is to use the Amazon Bedrock `Converse` API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef9132-eac2-4d63-b9e6-3d18c0b0d706",
   "metadata": {},
   "source": [
    "### Bedrock converse API\n",
    "\n",
    "The [Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) is designed for creating advanced conversational applications that interact with large language models like Llama 3. It allows developers to send conversation prompts and receive contextually relevant responses, maintaining dialogue coherence over multiple exchanges.\n",
    "\n",
    "Compared to the `InvokeModel` API, the `Converse` API offers advantages in dialogue management and context retention. While `InvokeModel` handles single, standalone prompts, the `Converse` API is built to maintain the context of an ongoing conversation, making it more suitable for applications that require multi-turn interactions and a natural flow of dialogue. This enhanced capability results in more engaging and effective conversational agents.\n",
    "\n",
    "For a complete guide, see [Getting started with the Amazon Bedrock Converse API](https://community.aws/content/2hHgVE7Lz6Jj1vFv39zSzzlCilG/getting-started-with-the-amazon-bedrock-converse-api?lang=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9f12198-b9ee-47df-aa10-f15a367e5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A \"Hello World\" program is a traditional introductory example in programming that serves as a simple demonstration of how to write a program in a specific language, typically printing the text \"Hello, World!\" to the screen.\n"
     ]
    }
   ],
   "source": [
    "# Use the Conversation API to send a text message to Meta Llama.\n",
    "\n",
    "def send_message_to_model(conversation, model_id=model_id, max_tokens=512, temperature=0.5, top_p=0.9, system_prompt=\"You are a helpful assistant\"):\n",
    "    \"\"\"\n",
    "    Send a message to a model and return the response.\n",
    "\n",
    "    Args:\n",
    "        conversation (list): The conversation history/messages to send to the model.\n",
    "        model_id (str): The ID of the model to use.\n",
    "        max_tokens (int): Maximum number of tokens to generate in the response.\n",
    "        temperature (float): Sampling temperature to control randomness.\n",
    "        top_p (float): Nucleus sampling parameter to control the range of token sampling.\n",
    "        system_prompt (str): System prompt to guide the model's behavior.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the model, containing the generated text and additional metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send the message to the model, using the provided inference configuration.\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p\n",
    "            },\n",
    "            system=[{\"text\": system_prompt}],\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "        return response\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_message_to_model(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64866076-e272-4bb2-ba23-90aaded35cb4",
   "metadata": {},
   "source": [
    "Alternatively, we can print the whole conversation. Notice the two roles `user` and `assistant` alterning between each other. The last message in the list should be from the `user` role, so that the LLM can respond to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5f5cca4-49d2-4d95-b40a-ad3349695643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"user\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "      <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the purpose of a 'hello world' program in one line.\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"assistant\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "      <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\n\\nA \\\"Hello World\\\" program is a traditional introductory example in programming that serves as a simple demonstration of how to write a program in a specific language, typically printing the text \\\"Hello, World!\\\" to the screen.\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"user\"\u001b[0m,\n",
       "    \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "      \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"Describe the purpose of a 'hello world' program in one line.\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"assistant\"\u001b[0m,\n",
       "    \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "      \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"\\n\\nA \\\"Hello World\\\" program is a traditional introductory example in programming that serves as a simple demonstration of how to write a program in a specific language, typically printing the text \\\"Hello, World!\\\" to the screen.\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.append(response[\"output\"][\"message\"])\n",
    "print_json(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a69d97-68c5-408d-be67-f9b866808de8",
   "metadata": {},
   "source": [
    "#### Setting a system prompt\n",
    "\n",
    "You can set a system prompt to communicate basic instructions for the large language model outside of the normal conversation. System prompts are generally used by the developer to define the tone and constraints for the conversation. In this case, we are instructing Llama to act like a pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0511d3a-85b9-499e-9ae8-049c8fce22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, that be a great question, matey! According to me, the best place to hide a pirate booty be on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs and the parrots. Make sure to bury it good and deep, with a map and a riddle to lead ye back to it, savvy? And don't ferget to stash a few fake treasures around to throw off any landlubbers who might be searchin' fer it!\n"
     ]
    }
   ],
   "source": [
    "new_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": \"What is the best place to hide a pirate booty?\" }\n",
    "    ],\n",
    "}\n",
    "\n",
    "system_prompt = \"Answer in the style of a pirate\"\n",
    "\n",
    "conversation.append(new_message)\n",
    "response = send_message_to_model(conversation, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970fbca-cdc2-4b1c-8682-4e97ec2aaa77",
   "metadata": {},
   "source": [
    "#### Getting response metadata and token counts\n",
    "\n",
    "The Converse method also returns metadata about the API call. The `stopReason` property tells us why the model completed the message, which can be useful for your application logic, error handling, or troubleshooting. The `usage` property includes details about the input and output tokens, helping you understand the charges for your API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea801886-834a-4c35-b234-1dc0a56ad1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"ResponseMetadata\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RequestId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"52f175af-bc2c-46f5-98cb-c5b90f7f0489\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPStatusCode\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPHeaders\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"date\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Thu, 11 Jul 2024 08:24:16 GMT\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"application/json\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-length\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"605\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"connection\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"keep-alive\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"x-amzn-requestid\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"52f175af-bc2c-46f5-98cb-c5b90f7f0489\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RetryAttempts\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"output\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"message\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"assistant\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "          <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\n\\nArrr, that be a great question, matey! According to me, the best place to hide a pirate booty be on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs and the parrots. Make sure to bury it good and deep, with a map and a riddle to lead ye back to it, savvy? And don't ferget to stash a few fake treasures around to throw off any landlubbers who might be searchin' fer it!\"</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "      <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"stopReason\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"end_turn\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"usage\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"totalTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"metrics\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"latencyMs\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1604</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"ResponseMetadata\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"RequestId\"\u001b[0m: \u001b[32m\"52f175af-bc2c-46f5-98cb-c5b90f7f0489\"\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPStatusCode\"\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPHeaders\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"date\"\u001b[0m: \u001b[32m\"Thu, 11 Jul 2024 08:24:16 GMT\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-type\"\u001b[0m: \u001b[32m\"application/json\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-length\"\u001b[0m: \u001b[32m\"605\"\u001b[0m,\n",
       "      \u001b[1;34m\"connection\"\u001b[0m: \u001b[32m\"keep-alive\"\u001b[0m,\n",
       "      \u001b[1;34m\"x-amzn-requestid\"\u001b[0m: \u001b[32m\"52f175af-bc2c-46f5-98cb-c5b90f7f0489\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1;34m\"RetryAttempts\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"output\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"message\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"assistant\"\u001b[0m,\n",
       "      \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "          \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"\\n\\nArrr, that be a great question, matey! According to me, the best place to hide a pirate booty be on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs and the parrots. Make sure to bury it good and deep, with a map and a riddle to lead ye back to it, savvy? And don't ferget to stash a few fake treasures around to throw off any landlubbers who might be searchin' fer it!\"\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "      \u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"stopReason\"\u001b[0m: \u001b[32m\"end_turn\"\u001b[0m,\n",
       "  \u001b[1;34m\"usage\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"inputTokens\"\u001b[0m: \u001b[1;36m99\u001b[0m,\n",
       "    \u001b[1;34m\"outputTokens\"\u001b[0m: \u001b[1;36m110\u001b[0m,\n",
       "    \u001b[1;34m\"totalTokens\"\u001b[0m: \u001b[1;36m209\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"metrics\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"latencyMs\"\u001b[0m: \u001b[1;36m1604\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_json(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e8641-8e74-4863-b1e7-55d12d0c9acb",
   "metadata": {},
   "source": [
    "#### Bedrock Converse Streaming API\n",
    "\n",
    "This example demonstrates how to use the Converse operation with output streaming. This means the model's answer is printed in real-time as the text is generated, rather than waiting for the model to complete the entire text before displaying it. The example shows how to send the input text, inference parameters, and additional parameters unique to the model. The code starts a conversation by asking the model to create a list of songs.\n",
    "\n",
    "<b>NOTE:</b> Output streaming is also supported with the `InvokeModelWithResponseStream` API. https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6ce168-bd6c-4e2a-90c0-58cfa7870777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def stream_conversation(bedrock_client,\n",
    "                    model_id,\n",
    "                    messages,\n",
    "                    system_prompts,\n",
    "                    inference_config,\n",
    "                    additional_model_fields):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(\n",
    "                        f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(\n",
    "                        f\"Latency: {metadata['metrics']['latencyMs']} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aa87d0d-654e-4700-8e3b-3d23a91184ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Streaming messages with model meta.llama3-8b-instruct-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Role: assistant\n",
      "Here are 3 pop songs for your playlist:\n",
      "\n",
      "1. \"Happy\" by Pharrell Williams\n",
      "2. \"Can't Stop the Feeling!\" by Justin Timberlake\n",
      "3. \"We Found Love\" by Rihanna (feat. Calvin Harris)\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 51\n",
      "Output tokens: 51\n",
      "Total tokens: 102\n",
      "Latency: 976 ms\n",
      "Finished streaming messages with model meta.llama3-8b-instruct-v1:0.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "system_prompt = \"\"\"You are an app that creates playlists for a radio station\n",
    "  that plays rock and pop music. Only return song names and the artist.\"\"\"\n",
    "\n",
    "# Message to send to the model.\n",
    "input_text = \"Create a list of 3 pop songs.\"\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": input_text}]\n",
    "}\n",
    "conversation = [message]\n",
    "\n",
    "# System prompts.\n",
    "system_prompts = [{\"text\": system_prompt}]\n",
    "\n",
    "# inference parameters to use.\n",
    "temperature = 0.5\n",
    "\n",
    "# Base inference parameters.\n",
    "inference_config = {\n",
    "    \"temperature\": temperature\n",
    "}\n",
    "\n",
    "# Additional model inference parameters.\n",
    "additional_model_fields = {}\n",
    "\n",
    "try:\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    stream_conversation(bedrock_client,\n",
    "                        model_id,\n",
    "                        conversation,\n",
    "                        system_prompts,\n",
    "                        inference_config,\n",
    "                        additional_model_fields)\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(\"A client error occured: \" +\n",
    "          format(message))\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished streaming messages with model {model_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089b2d9-a1da-49ee-bc0b-a74c55991123",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "Guardrails for Amazon Bedrock enables you to implement safeguards for your generative AI applications based on your use cases and responsible AI policies. You can create multiple guardrails tailored to different use cases and apply them across multiple foundation models (FM), providing a consistent user experience and standardizing safety and privacy controls across generative AI applications. You can use guardrails with text-based user inputs and model responses.\n",
    "\n",
    "Guardrails can be used in multiple ways to safeguard generative AI applications. For example:\n",
    "\n",
    "- A chatbot application can use guardrails to filter harmful user inputs and toxic model responses.\n",
    "- A banking application can use guardrails to block user queries or model responses associated with seeking or providing investment advice.\n",
    "- A call center application to summarize conversation transcripts between users and agents can use guardrails to redact users’ personally identifiable information (PII) to protect user privacy.\n",
    "\n",
    "For more details, visit [AWS Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57477197-e5f6-46dd-99b8-eede419d95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Guardrails to block any financial advice.\n",
    "\n",
    "financial_guardrail = bedrock.create_guardrail(\n",
    "    name='financial-advice-guardrail',\n",
    "    description='string',\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                'name': 'financial-advice',\n",
    "                'definition': 'Never give any financial advice.',\n",
    "                'examples': [\n",
    "                    'Where should I invest my money?',\n",
    "                    'What are the best stocks to buy right now?',\n",
    "                    'Shoudl I buy bitcoin?',\n",
    "                ],\n",
    "                'type': 'DENY'\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging='You query was blocked by the following guardrail: financial advice-guardrail.',\n",
    "    blockedOutputsMessaging='The model response was blocked by the following guardrail: financial advice-guardrail.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a392f591-fa26-4619-9a43-e1318bbc411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ad464fa2-d546-404e-ba53-a3d9235fb120',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 11 Jul 2024 08:24:18 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '573',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'ad464fa2-d546-404e-ba53-a3d9235fb120'},\n",
       "  'RetryAttempts': 0},\n",
       " 'guardrails': [{'id': 'g78qb7eiy7ff',\n",
       "   'arn': 'arn:aws:bedrock:us-east-1:137229038021:guardrail/g78qb7eiy7ff',\n",
       "   'status': 'READY',\n",
       "   'name': 'bedrock-agent-demo',\n",
       "   'version': 'DRAFT',\n",
       "   'createdAt': datetime.datetime(2024, 6, 7, 8, 28, 51, 298678, tzinfo=tzlocal()),\n",
       "   'updatedAt': datetime.datetime(2024, 6, 7, 8, 32, 24, 610026, tzinfo=tzlocal())},\n",
       "  {'id': 'voqvsabek3h0',\n",
       "   'arn': 'arn:aws:bedrock:us-east-1:137229038021:guardrail/voqvsabek3h0',\n",
       "   'status': 'READY',\n",
       "   'name': 'financial-advice-guardrail',\n",
       "   'description': 'string',\n",
       "   'version': 'DRAFT',\n",
       "   'createdAt': datetime.datetime(2024, 7, 11, 8, 24, 17, 751399, tzinfo=tzlocal()),\n",
       "   'updatedAt': datetime.datetime(2024, 7, 11, 8, 24, 17, 892114, tzinfo=tzlocal())}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List guardrail\n",
    "bedrock.list_guardrails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71132e12-82b3-49db-a4f6-fa9b7c4c2100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\"You query was blocked by the following guardrail: financial advice-guardrail.\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m\"You query was blocked by the following guardrail: financial advice-guardrail.\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing a Guardrail (without invoking a model)\n",
    "def test_guardrail(prompt, guardrail_id, guardrail_version):\n",
    "    \"\"\"\n",
    "    Tests a guardrail by applying it to a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text to be tested against the guardrail.\n",
    "        guardrail_id (str): The unique identifier of the guardrail.\n",
    "        guardrail_version (str): The version of the guardrail to be applied.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT', \n",
    "        content=[{\"text\": {\"text\": prompt}}])\n",
    "    print_json(response[\"outputs\"][0][\"text\"])\n",
    "\n",
    "\n",
    "prompt = \"How should I invest for my savings?\"\n",
    "guardrailIdentifier = financial_guardrail['guardrailId']\n",
    "guardrailVersion = financial_guardrail['version']\n",
    "\n",
    "test_guardrail(prompt, guardrailIdentifier, guardrailVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eef689b-ceb6-4e34-932d-e22c4037fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a guardrail to a Llama 3 invocation\n",
    "def stream_conversation(bedrock_client,\n",
    "                    model_id,\n",
    "                    messages,\n",
    "                    system_prompts,\n",
    "                    inference_config,\n",
    "                    additional_model_fields,\n",
    "                    guardrail_config):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "        guardrailConfig (JSON) : The guardrail to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields,\n",
    "        guardrailConfig=guardrail_config\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(\n",
    "                        f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(\n",
    "                        f\"Latency: {metadata['metrics']['latencyMs']} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60f64eed-9483-4cbe-a379-f9e7315fd991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Streaming messages with model meta.llama3-8b-instruct-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Role: assistant\n",
      "You query was blocked by the following guardrail: financial advice-guardrail.\n",
      "Stop reason: guardrail_intervened\n",
      "\n",
      "Token usage\n",
      "Input tokens: 0\n",
      "Output tokens: 0\n",
      "Total tokens: 0\n",
      "Latency: 730 ms\n",
      "Finished streaming messages with model meta.llama3-8b-instruct-v1:0.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant\"\"\"\n",
    "\n",
    "# Message to send to the model.\n",
    "input_text = \"How should I invest for my savings?\"\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": input_text}]\n",
    "}\n",
    "conversation = [message]\n",
    "\n",
    "# System prompts.\n",
    "system_prompts = [{\"text\": system_prompt}]\n",
    "\n",
    "# inference parameters to use.\n",
    "temperature = 0.5\n",
    "\n",
    "# Base inference parameters.\n",
    "inference_config = {\n",
    "    \"temperature\": temperature\n",
    "}\n",
    "\n",
    "# Additional model inference parameters.\n",
    "additional_model_fields = {}\n",
    "\n",
    "# Guardrail\n",
    "guardrail_config = {\n",
    "    'guardrailIdentifier': guardrailIdentifier,\n",
    "    'guardrailVersion': guardrailVersion,\n",
    "}\n",
    "\n",
    "try:\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    stream_conversation(bedrock_client,\n",
    "                        model_id,\n",
    "                        conversation,\n",
    "                        system_prompts,\n",
    "                        inference_config,\n",
    "                        additional_model_fields,\n",
    "                        guardrail_config)\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(\"A client error occured: \" +\n",
    "          format(message))\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished streaming messages with model {model_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0afe5b15-ad37-4836-8fc9-4bbade05831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c2dd3c76-787c-426a-99a1-b00fb06ec118',\n",
       "  'HTTPStatusCode': 202,\n",
       "  'HTTPHeaders': {'date': 'Thu, 11 Jul 2024 08:24:19 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'c2dd3c76-787c-426a-99a1-b00fb06ec118'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup\n",
    "bedrock.delete_guardrail(guardrailIdentifier=financial_guardrail['guardrailId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6b7a9-b33c-45e8-b85b-a8123c725d4a",
   "metadata": {},
   "source": [
    "## Additional Ressource\n",
    "\n",
    "- Meta's Llama receipt for AWS: https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/aws\n",
    "- Amazon Bedrock samples: https://github.com/aws-samples/amazon-bedrock-samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
