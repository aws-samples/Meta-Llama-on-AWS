{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Document Summarization with Llama 3.1 on Bedrock with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate how you can use the [Llama 3.1 family of models](https://aws.amazon.com/about-aws/whats-new/2024/07/meta-llama-3-1-405b-generally-available-amazon-bedrock/) on Amazon Bedrock for document summarization tasks. \n",
    "\n",
    "All Llama 3.1 models demonstrate significant improvements over previous versions. The models support a 128K context length and exhibit improved reasoning for multilingual dialogue use cases in eight languages. The models access more information from lengthy text to make more informed decisions and leverage richer contextual data to generate more subtle and refined responses. According to Meta, Llama 3.1 405B is one of the best and largest publicly available foundation models and is well suited for synthetic data generation and model distillation. Llama 3.1 models also provide state-of-the-art capabilities in general knowledge, math, tool use, and multilingual translation.\n",
    "\n",
    "This notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs). It will show improvements that Llama 3.1 offers as compared with Llama 3.\n",
    "\n",
    "<b>Note (for reference):</b> Long Document Summarization with using Llama 3 on Bedrock with LangChain is discussed [here](https://github.com/aws-samples/Meta-Llama-on-AWS/blob/main/long-text-summarization/Llama3-Long-Document-summarization-LangChain.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "## Llama 3.1 Model Selection\n",
    "\n",
    "There are [three](https://aws.amazon.com/about-aws/whats-new/2024/07/meta-llama-3-1-405b-generally-available-amazon-bedrock/) Llama 3.1 models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Llama 3.1 8B\n",
    "\n",
    "- **Description:** Ideal for limited computational power and resources, faster training times, and edge devices. The model excels at text summarization, text classification, sentiment analysis, and language translation.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Text Classification, and Sentiment Analysis.\n",
    "\n",
    "### 2. Llama 3.1 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. The model excels at text summarization and accuracy, text classification and nuance, sentiment analysis and nuance reasoning, language modeling, dialogue systems, code generation, and following instructions.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "### 2. Llama 3.1 405B\n",
    "\n",
    "- **Description:** Ideal for enterprise level applications, research and development, synthetic data generation, and model distillation. The model excels at general knowledge, long-form text generation, machine translation, enhanced contextual understanding, advanced reasoning and decision making, better handling of ambiguity and uncertainty, increased creativity and diversity, steerability, math, tool use, multilingual translation, and coding.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below summarizes the model performance on the Massive Multitask Language Understanding ([MMLU](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md#instruction-tuned-models)) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Llama 3.1 8B | 69.4%      | \\$0.0003                   | \\$0.0006                    |\n",
    "| Llama 3.1 70B | 83.6%      | \\$0.00265                   | \\$0.0035                     |\n",
    "| Llama 3.1 405B | 87.3%      | \\$0.00532                   | \\$0.016                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Llama 3.1 Model Cards and Prompt Formats](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html). <I>We are using Notebooks on SageMaker as it provides the kernel we need to run these examples</I>.\n",
    "2. For Notebook Instance type, choose ml.t3.medium.\n",
    "3. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "1. AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "2. [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3\n",
    "botocore==1.34.142\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers\n",
    "rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>NOTE:</b> Restart the kernel with the updated packages that are installed through the dependencies above\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ddb25-b789-4b3c-8928-39f651c98ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import rich\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set config object and create bedrock runtime client.\n",
    "config = Config(read_timeout=2000)\n",
    "\n",
    "DEFAULT_REGION = \"us-west-2\"\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',\n",
    "                       region_name=DEFAULT_REGION,\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>NOTE:</b> Ensure that you have access to the Llama 3.1 model you wish to use through Bedrock in the selected region. At the time of writing, Llama 3.1 models are available only in us-west-2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the desired Llama 3.1 model ID\n",
    "llama3_8b_instruct = \"meta.llama3-8b-instruct-v1:0\"\n",
    "llama3_70b_instruct = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "llama3_1_8b_instruct = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "llama3_1_70b_instruct = \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "llama3_1_405b_instruct = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = llama3_1_70b_instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Meta Llama3.1 405b/70b/8b in `model_id` and pass the Llama3 inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "### Supported parameters\n",
    "\n",
    "The Llama models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"max_gen_len\": int\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bafb1-129d-486b-8c7a-445c8bce1bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this notebook, we will look at the 70b model in Llama 3 and Llama 3.1 and \n",
    "# review the results. We will use this method to return the ChatBedrock object\n",
    "# corresponding to the appropriate LLM by passing the model Id as string.\n",
    "def GetLLMChatBedrockObject(modelID=DEFAULT_MODEL):\n",
    "    # Instantiate the LangChain ChatBedrock object. This will allow you to use \n",
    "    # LangChain with Chat models on Amazon Bedrock\n",
    "    llm = ChatBedrock(\n",
    "        model_id=modelID,\n",
    "        model_kwargs={\n",
    "            \"max_gen_len\": 2048,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        },\n",
    "        client=bedrock,\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ChatBedrock object for the intended model\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Initialize conversation chain\n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the\n",
    "    # execution of the conversation chain. This can be set to true when\n",
    "    # you're debugging your conversation chain or trying to understand how\n",
    "    # it is working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Generate some text with a prompt.\n",
    "prediction = conversation.predict(input=\"Hi there! How are you doing? Please provide me a trivia question and the answer.\")\n",
    "\n",
    "# Having multilingual support built-in means that you can use Llama 3.1 to write prompts\n",
    "# and receive responses directly in those languages.\n",
    "# Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "# Few examples below, uncomment a line and run the next cell to see the results.\n",
    "# prediction = conversation.predict(input=\"Hallo! Wie geht es dir? Bitte stellen Sie mir eine Quizfrage und die Antwort.\")  # German\n",
    "# prediction = conversation.predict(input=\"Salut! Comment allez-vous? Veuillez me fournir une question triviale et la réponse.\")  # French\n",
    "# prediction = conversation.predict(input=\"Ciao! Come va? Per favore forniscimi una domanda banale e la risposta.\")  # Italian\n",
    "# prediction = conversation.predict(input=\"Olá! Como vai? Por favor, forneça-me uma pergunta trivial e a resposta.\")  # Portugese\n",
    "# prediction = conversation.predict(input=\"नमस्ते! आप कैसे हैं? कृपया मुझे एक सामान्य प्रश्न और उत्तर प्रदान करें।\")  # Hindi\n",
    "# prediction = conversation.predict(input=\"¡Hola! ¿Cómo estás? Por favor dame una pregunta de trivia y la respuesta.\")  # Spanish\n",
    "# prediction = conversation.predict(input=\"สวัสดี! เป็นอย่างไรบ้าง โปรดให้คำถามและคำตอบเรื่องไม่สำคัญแก่ฉัน\")  # Thai\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "To demonstrate summarization, we will be using an [AWS whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's first download the file to build our document store.\n",
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2024, source=filenames[0])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "\n",
    "# Here we load a PDF using pypdf into array of documents, where each document contains the page content and metadata with page number.\n",
    "# To access a subset of pages use something like: hipaa_document[70:84], for no of pages, use: len(hipaa_document)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "# Chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "# Chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nTotal number of pages: {len(hipaa_documents)}\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0b689-b4df-4a2c-9327-821ec1f5892e",
   "metadata": {},
   "source": [
    "### As Llama 3.1 has a larger context window, you can use the native invoke method directly without having to use langchain.\n",
    "\n",
    "However LangChain offers few techniques that provides you more control over the summarization task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34b6f507-11f8-493b-8772-3f1701194960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Taking a piece of text which is larger than the context window size for Llama 3\n",
    "text = hipaa_docs[50:97]\n",
    "\n",
    "prompt = f\"\"\"Write a detailed summary of the following text delimited by triple backquotes.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "```{text}```\n",
    "BULLET POINT SUMMARY:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa8534f3-4eec-4bc5-a366-c6159bb4ef34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a bullet point summary of the text:\n",
      "\n",
      "* Amazon CloudWatch Events delivers a near-real-time stream of system events that describe changes in AWS resources.\n",
      "* Customers should ensure that PHI does not flow into CloudWatch Events and that any AWS resource emitting a CloudWatch event that is storing, processing, or transmitting PHI is configured in accordance with the Guidance.\n",
      "* Amazon CloudWatch Logs can be used to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources.\n",
      "* Log data is encrypted while in transit and at rest, and customers do not need to re-encrypt PHI emitted by any other service and delivered to CloudWatch Logs.\n",
      "* Amazon Comprehend uses natural language processing to extract insights about the content of documents and can be used with data containing PHI.\n",
      "* Amazon Comprehend processes any text file in UTF-8 format and develops insights by recognizing entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "* Amazon Comprehend does not retain or store any data, and all calls to the API are encrypted with SSL/TLS.\n",
      "* Amazon Comprehend uses CloudTrail to log all API calls.\n",
      "* AWS Identity and Access Management (IAM) can be used to control access to Amazon Comprehend and credentials can be used to access the IAM.\n",
      "* Customers can use IAM policies to grant users permissions for specific resources and actions.\n",
      "* Amazon Comprehend can be used with AWS Lake Formation to create a data warehouse and data lake.\n",
      "* Customers can use AWS Lake Formation to create a data catalog and manage access to data.\n",
      "* Amazon Comprehend can be used with AWS Glue to create a data pipeline and manage data.\n",
      "* Customers can use AWS Glue to create a data catalog and manage access to data.\n",
      "* Amazon Comprehend can be used with Amazon S3 to store and manage data.\n",
      "* Customers can use Amazon S3 to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon DynamoDB to store and manage data.\n",
      "* Customers can use Amazon DynamoDB to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Elastic Block Store (EBS) to store and manage data.\n",
      "* Customers can use Amazon EBS to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Elastic File System (EFS) to store and manage data.\n",
      "* Customers can use Amazon EFS to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Elastic Container Registry (ECR) to store and manage data.\n",
      "* Customers can use Amazon ECR to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Elastic Container Service (ECS) to store and manage data.\n",
      "* Customers can use Amazon ECS to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Elastic Kubernetes Service (EKS) to store and manage data.\n",
      "* Customers can use Amazon EKS to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon ElastiCache to store and manage data.\n",
      "* Customers can use Amazon ElastiCache to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon OpenSearch Service to store and manage data.\n",
      "* Customers can use Amazon OpenSearch Service to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon EMR to store and manage data.\n",
      "* Customers can use Amazon EMR to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon EventBridge to store and manage data.\n",
      "* Customers can use Amazon EventBridge to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon Forecast to store and manage data.\n",
      "* Customers can use Amazon Forecast to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon FSx to store and manage data.\n",
      "* Customers can use Amazon FSx to store and manage data, and can use IAM policies to control access to data.\n",
      "* Amazon Comprehend can be used with Amazon GuardDuty to store and manage data.\n",
      "* Customers can use Amazon GuardDuty to store and manage data, and can use IAM policies to control access to data.\n",
      "CPU times: user 13.1 ms, sys: 308 μs, total: 13.4 ms\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the prompt in Llama 3.1's instruction format.\n",
    "# More information: \n",
    "# https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\n",
    "\n",
    "# Switch between models here.\n",
    "# DEFAULTMODEL = llama3_70b_instruct  #  Will throw an error\n",
    "DEFAULTMODEL = llama3_1_70b_instruct  #  Takes about 2-3 mins to run.\n",
    "\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_gen_len\": 2048\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "inputTokens = outputTokens = 0\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = bedrock.invoke_model(modelId=DEFAULTMODEL, body=request)\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"generation\"]\n",
    "    \n",
    "    inputTokens = model_response[\"prompt_token_count\"]\n",
    "    outputTokens = model_response[\"generation_token_count\"]\n",
    "        \n",
    "    print(response_text)\n",
    "    \n",
    "    # Convert this to a lookup based on the model\n",
    "    # print(f\"Approx cost of this operation = {getCost(DEFAULT_REGION, DEFAULT_MODEL, inputTokens, outputTokens)}\")\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{DEFAULT_MODEL}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c3c2a37-d4dd-4f6d-9b2e-f1e508976d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"prompt\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\n&lt;|begin_of_text|&gt;\\n&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\nWrite a detailed summary of the following text delimited by triple backquotes.\\nReturn your response in bullet points which covers the key points of the text.\\n```[Document(page_content='For more details on conﬁguration requirements, see the Amazon CloudWatch Logs section.\\\\nAmazon CloudWatch Events\\\\nAmazon CloudWatch Events delivers a near-real-time stream of system events that describe \\\\nchanges in AWS resources. Customers should ensure that PHI does not ﬂow into CloudWatch \\\\nEvents, and any AWS resource emitting a CloudWatch event that is storing, processing, or \\\\ntransmitting PHI is conﬁgured in accordance with the Guidance.\\\\nCustomers can conﬁgure Amazon CloudWatch Events to register as an AWS API call in CloudTrail. \\\\nFor more information, see Creating a CloudWatch Events Rule That Triggers on an AWS API Call \\\\nUsing AWS CloudTrail.\\\\nAmazon CloudWatch Logs\\\\nCustomers can use Amazon CloudWatch Logs to monitor, store, and access their log ﬁles from \\\\nAmazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route\\\\xa053, and \\\\nother sources. They can then retrieve the associated log data from CloudWatch Logs. Log data is', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='other sources. They can then retrieve the associated log data from CloudWatch Logs. Log data is \\\\nencrypted while in transit and while it is at-rest. As a result, it is not necessary to re-encrypt PHI \\\\nemitted by any other service and delivered to CloudWatch Logs.\\\\nLambda@Edge 12', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon Comprehend\\\\nAmazon Comprehend uses natural language processing to extract insights about the content of \\\\ndocuments. Amazon Comprehend processes any text ﬁle in UTF-8 format. It develops insights \\\\nby recognizing the entities, key phrases, language, sentiments, and other common elements in \\\\na document. Amazon Comprehend can be used with data containing PHI. Amazon Comprehend \\\\ndoes not retain or store any data and all calls to the API are encrypted with SSL/TLS. Amazon \\\\nComprehend uses CloudTrail to log all API calls.\\\\nAWS Identity and Access Management\\\\nSecurity access functions such as authentication and authorization are required for accessing \\\\nAmazon Comprehend and can be controlled with AWS Identity and Access Management (IAM), and \\\\ncredentials can be used to access the IAM. For more information, see Authentication and Access \\\\nControl for Amazon Comprehend in the Amazon Comprehend User Guide .\\\\nAccount management', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Control for Amazon Comprehend in the Amazon Comprehend User Guide .\\\\nAccount management\\\\nBy default, IAM users don't have permission to create or modify Amazon Comprehend resources, or \\\\nperform tasks using the Amazon Comprehend API. To allow users to create or modify resources and \\\\nperform tasks, customers are responsible for leveraging IAM policies that grant users permissions \\\\nfor the speciﬁc resources (such as Amazon Comprehend and API actions) users need to use, and \\\\nthen attach policies to the users or groups that require speciﬁc permissions.\\\\nWith Amazon Comprehend you can use AWS Identity and Access Management (IAM) to create \\\\na user with an attached policy to enable Amazon Comprehend permissions. Optionally, you can \\\\nchoose to create custom policies to attach to a role. Then, you can add administrators to the \\\\nrole with the ability to invoke API's for Amazon Comprehend administration in accordance with \\\\norganization-deﬁned role-based access and least privilege principles.\\\\nIdentity and access\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='organization-deﬁned role-based access and least privilege principles.\\\\nIdentity and access\\\\nWith Amazon Comprehend you can require user to authenticate to AWS using multi-factor \\\\nauthentication in accordance with their organizational requirements for authentication.\\\\nUsing the AWS Management Console, IAM administrators can create a customer-managed policy \\\\nthat denies all permissions except those required for users to manage their own credentials and \\\\nMFA devices. A JSON policy template is available on the My Security Credential page  in the IAM \\\\nconsole.\\\\nAmazon Comprehend 13', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nOptionally, you can leverage compatible third-party MFA capabilities with IAM partners. For \\\\nadditional information, see IAM Partners.\\\\nAdministration\\\\nWe recommend that you Amazon Comprehend select identity-based policies in which account \\\\nadministrators can attach permissions policies to IAM identities (users, groups, and roles) and \\\\nthereby grant permissions to perform operations on Amazon Comprehend resources.\\\\nA list of API actions for Amazon Comprehend can be found in the API Reference guide. You \\\\nshould also consider authorizing access to predeﬁned IAM policies, customer IAM policies and API \\\\nactions to users or roles in accordance with their least privilege and role-based organizational \\\\nrequirements. For more information, see Using the Amazon Comprehend API in the Developer \\\\nGuide .\\\\nExternal authentication\\\\nAmazon Comprehend is compatible with identity federation using IAM roles. This enables Amazon', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon Comprehend is compatible with identity federation using IAM roles. This enables Amazon \\\\nComprehend your users to authenticate to AWS by assuming a role that administrators have \\\\nprovisioned. Users accessing AWS using credentials from their organization or a third-party assume \\\\na role indirectly.\\\\nAWS support for Kerberos and Active Directory provides the beneﬁts of single sign-on and \\\\ncentralized authentication of database users. AWS users can choose to manage and store user \\\\ncredentials either in AWS Directory Service for Microsoft Active Directory or in the customer on-\\\\npremises Active Directory.\\\\nData ﬂow enforcement\\\\nAWS customers and APN partners, acting either as data controllers or data processors, are \\\\nresponsible for any personal data that they put in the AWS Cloud and Amazon Comprehend. You \\\\nare responsible for controlling the ﬂow to data inputs and outputs for Amazon Comprehend using \\\\nIAM policies.\\\\nData protection and secrets management', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='IAM policies.\\\\nData protection and secrets management\\\\nThe AWS shared responsibility model applies to data protection in Amazon Comprehend. As \\\\ndescribed in this model, AWS is responsible for protecting the global infrastructure that runs all of \\\\nthe AWS Cloud. You are responsible for maintaining control over your content that is hosted on this \\\\nData protection and secrets management 14', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\ninfrastructure. This content includes the security conﬁguration and management tasks for the AWS \\\\nservices that you use. For more information about data privacy, see the Data Privacy FAQ.\\\\nThe Data Protection in Amazon Comprehend section in the Amazon Comprehend Developer Guide\\\\nprovides tips that you should consider in protecting data such as using TLS for transmission and \\\\navoiding placement of sensitive information into tags or free-form ﬁelds.\\\\nEncryption of data-at-rest\\\\nAmazon Comprehend works with AWS Key Management Service (AWS KMS) to provide enhanced \\\\nencryption for your data. Amazon Simple Storage Service (Amazon S3) already enables you to \\\\nencrypt your input documents when creating a text analysis, topic modeling, or custom Amazon \\\\nComprehend job. Integration with AWS KMS enables you to encrypt the data in the storage volume', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Comprehend job. Integration with AWS KMS enables you to encrypt the data in the storage volume \\\\nfor start* and create*  jobs, and it encrypts the output results of start* jobs using your own AWS \\\\nKMS key.\\\\nIt is best practice for Amazon Comprehend users to encrypt Amazon S3 buckets used for input \\\\ndocuments using available S3 encryption solutions in accordance with their organizational policies.\\\\nThe AWS Management Console, encrypts Amazon Comprehend custom models with its own AWS \\\\nKMS key. For the AWS CLI, Amazon Comprehend can encrypt custom models using either its own \\\\nAWS KMS key or a provided customer managed key (CMK).\\\\nIf selecting encryption when using the AWS Management Console, you can choose either or both of \\\\nthe following optional methods:\\\\n•Volume encryption - ensures that the data on an EBS Volume used by Comprehend is encrypted \\\\nduring training/inference (data is ﬂushed after training/inference, so this key is relevant only \\\\nwhile the job is in progress).', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='while the job is in progress).\\\\n•Output result encryption - for encrypting the output stored by comprehend in the customer’s \\\\nbucket using a customer provided AWS KMS key.\\\\nFor more information about encryption types such as volume encryption, see AWS KMS Encryption \\\\nin Amazon Comprehend.\\\\nPersonally identiﬁable information\\\\nYou can use the Amazon Comprehend console or APIs to detect personally identiﬁable information \\\\n(PII) in English text documents. For more information about about detecting and labeling PII \\\\nData protection and secrets management 15', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nentities and operating various PII analysis jobs, see the Personally identiﬁable information section \\\\nin the Amazon Comprehend Developer Guide .\\\\nData deletion\\\\nIf you are an Amazon Comprehend customer using Amazon S3 and choosing to manage your \\\\nown AWS KMS keys, you should consider revoking AWS KMS keys and deﬁning the procedural \\\\njustiﬁcation to do so in accordance with their organizational requirements. Revocation of the AWS \\\\nKMS key for Amazon S3 renders any data unusable/unreadable.\\\\nNetwork segmentation and hardening\\\\nAs a managed service, Amazon Comprehend adheres to the AWS Best Practices for Security, \\\\nIdentity, and Compliance.\\\\nFor recommended network security safeguards, see Infrastructure Security in Amazon Comprehend\\\\nin the Amazon Comprehend Developers Guide .\\\\nProtect jobs using an Amazon Virtual Private Cloud (Amazon VPC)', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Protect jobs using an Amazon Virtual Private Cloud (Amazon VPC)\\\\nAmazon Comprehend uses a variety of security measures to ensure the safety of your data with our \\\\njob containers where it's stored while being used by Amazon Comprehend. However, job containers \\\\naccess AWS resources—such as the Amazon S3 buckets where you store data and model artifacts—\\\\nover the internet.\\\\nTo control access to your data, we recommend that you create a virtual private cloud (VPC) and \\\\nconﬁgure it so that the data and containers aren't accessible over the internet. For information \\\\nabout creating and conﬁguring a VPC, see Getting Started With Amazon VPC in the Amazon VPC \\\\nUser Guide . Using a VPC helps to protect your data because you can conﬁgure your VPC so that it \\\\nis not connected to the internet. Using a VPC also allows you to monitor all network traﬃc in and \\\\nout of our job containers by using VPC ﬂow logs. For more information, see VPC Flow Logs in the\\\\nAmazon VPC User Guide .\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon VPC User Guide .\\\\nYou specify your VPC conﬁguration when you create a job, by specifying the subnets and security \\\\ngroups. When you specify the subnets and security groups, Amazon Comprehend creates elastic \\\\nnetwork interfaces (ENIs) that are associated with your security groups in one of the subnets. ENIs \\\\nallow our job containers to connect to resources in your VPC. For information about ENIs, see\\\\nElastic Network Interfaces in the Amazon VPC User Guide .\\\\nNetwork segmentation and hardening 16', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nNote\\\\nFor jobs, you can only conﬁgure subnets with a default tenancy VPC in which your instance \\\\nruns on shared hardware. For more information on the tenancy attribute for VPCs, see\\\\nDedicated Instances in the Amazon EC2 User Guide for Linux Instances.\\\\nYou can establish a private connection between your VPC and Amazon Comprehend by creating \\\\nan interface VPC endpoint. For more information, see Amazon Comprehend and Interface VPC \\\\nEndpoints (AWS PrivateLink).\\\\nHost and image hardening\\\\nBased on the AWS shared responsibility model, host and image hardening of the AWS environment \\\\nfor Amazon Comprehend is managed by AWS as a provided service.\\\\nMulti-tenancy\\\\nTo help make your recommendation more secure, we recommend that you implement the \\\\nfollowing multi-tenancy security recommendations:\\\\n•Use only a veriﬁed email address to authorize user access to a tenant based on domain match.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"•Use only a veriﬁed email address to authorize user access to a tenant based on domain match. \\\\nDo not trust email addresses and phone numbers unless your app veriﬁes them, or the external \\\\nIdP gives a proof of veriﬁcation. For more details on setting these permissions, see Attribute \\\\nPermissions and Scopes.\\\\n•Use immutable or mutable attributes for the user proﬁle attributes that identify tenants. \\\\nAdministrators must be able to change these attributes. Also, give app clients read-only access to \\\\nthe attributes.\\\\n•Use 1:1 mapping between external IdP and application client to prevent unauthorized cross-\\\\ntenant access. A user who has been authenticated by an external IdP, and who has a valid \\\\nAmazon Cognito session cookie, can access other tenant apps that trust the same IdP.\\\\n•When you implement tenant-matching and authorization logic in your application, restrict \\\\nusers so that they can't modify the criteria that authorize user access to the tenants. Also, if an\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"external IdP is being used for federation, restrict tenant identity provider administrators so that \\\\nthey can't modify user access.\\\\nHost and image hardening 17\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nCross-service confused deputy prevention\\\\nThe confused deputy problem is a multi-tenancy security issue where an entity that doesn't \\\\nhave permission to perform an action can coerce a more-privileged entity to perform the action. \\\\nIn AWS, cross-service impersonation can result in the confused deputy problem. Cross-service \\\\nimpersonation can occur when one service (the calling service) calls another service (the called \\\\nservice). The calling service can be manipulated to use its permissions to act on another customer's \\\\nresources in a way it should not otherwise have permission to access. To prevent this, AWS provides \\\\ntools that can help you protect your data for all services with service principals that have been \\\\ngiven access to resources in your account. For more information which includes safeguards you \\\\nshould consider for addressing this security issue, see Cross-service Confused Deputy Prevention in\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='the Amazon Comprehend Developer Guide .\\\\nAmazon Comprehend Medical\\\\nFor guidance, see the previous Amazon Comprehend section.\\\\nAmazon Connect\\\\nAmazon Connect is a self-service, cloud-based contact center service that enables dynamic, \\\\npersonal, and natural customer engagement at any scale. Customers should not include any PHI \\\\nin any ﬁelds associated with managing users, security proﬁles, and contact ﬂows within Amazon \\\\nConnect.\\\\nAmazon Connect Customer Proﬁles, a feature of Amazon Connect, equips contact center agents \\\\nwith a more uniﬁed view of a customer’s proﬁle with the most up to date information, to provide \\\\nmore personalized customer service. Customer Proﬁles is designed to automatically bring together \\\\ncustomer information from multiple applications into a uniﬁed customer proﬁle, delivering the \\\\nproﬁle directly to the agent as soon as the support call or interaction begins. Customers should \\\\nrefrain from naming domains or object keys with PHI data. The contents of Domains and Objects', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='refrain from naming domains or object keys with PHI data. The contents of Domains and Objects \\\\nare encrypted and protected, but the key identiﬁers are not.\\\\nAmazon DocumentDB (with MongoDB compatibility)\\\\nAmazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB) oﬀers encryption \\\\nat-rest during cluster creation via AWS KMS, which allows customers to encrypt databases using \\\\nAWS or customer-managed keys. On a database instance running with encryption enabled, data \\\\nCross-service confused deputy prevention 18', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nstored at-rest is encrypted consistent with the Guidance in eﬀect at the time of publication of this \\\\nwhitepaper, as are automated backups, read replicas, and snapshots. Because the Guidance might \\\\nbe updated, customers should continue to evaluate and determine whether Amazon DocumentDB \\\\nencryption satisﬁes their compliance and regulatory requirements. For more information on \\\\nencryption at-rest using Amazon DocumentDB, see Encrypting Amazon DocumentDB Data at Rest.\\\\nConnections to Amazon DocumentDB containing PHI must use endpoints that accept encrypted \\\\ntransport (HTTPS). By default, a newly created Amazon DocumentDB cluster only accepts secure \\\\nconnections using Transport Layer Security (TLS). For more information, see Encrypting Data in \\\\nTransit. Amazon DocumentDB uses AWS CloudTrail to log all API calls. For more information, see\\\\nLogging and Monitoring in Amazon DocumentDB.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Logging and Monitoring in Amazon DocumentDB.\\\\nFor certain management features, Amazon DocumentDB uses operational technology that is \\\\nshared with Amazon RDS. Amazon DocumentDB console, AWS CLI, and API calls are logged as calls \\\\nmade to the Amazon RDS API.\\\\nAmazon DynamoDB\\\\nConnections to Amazon DynamoDB containing PHI must use endpoints that accept encrypted \\\\ntransport (HTTPS). For a list of regional endpoints, see AWS service endpoints.\\\\nAmazon DynamoDB oﬀers DynamoDB encryption, which allows customers to encrypt databases \\\\nusing keys that customers manage through AWS KMS. On a database instance running with \\\\nAmazon DynamoDB encryption, data stored at-rest in the underlying storage is encrypted \\\\nconsistent with the Guidance in eﬀect at the time of publication of this whitepaper, as are \\\\nautomated backups, read replicas, and snapshots.\\\\nBecause the Guidance might be updated, customers should continue to evaluate and determine', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Because the Guidance might be updated, customers should continue to evaluate and determine \\\\nwhether Amazon DynamoDB encryption satisﬁes their compliance and regulatory requirements. \\\\nFor more information on encryption at-rest using Amazon DynamoDB, see DynamoDB Encryption \\\\nat Rest.\\\\nAmazon Elastic Block Store\\\\nAmazon EBS encryption at-rest is consistent with the Guidance that is in eﬀect at the time of \\\\npublication of this whitepaper. Because the Guidance might be updated, customers should \\\\ncontinue to evaluate and determine whether Amazon EBS encryption satisﬁes their compliance \\\\nand regulatory requirements. With Amazon EBS encryption, a unique volume encryption key is \\\\ngenerated for each EBS volume. Customers have the ﬂexibility to choose which KMS key from \\\\nAmazon DynamoDB 19', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nthe AWS Key Management Service is used to encrypt each volume key. For more information, see\\\\nAmazon EBS encryption.\\\\nAmazon Elastic Compute Cloud\\\\nAmazon EC2 is a scalable, user-conﬁgurable compute service that supports multiple methods \\\\nfor encrypting data at rest. For example, customers might elect to perform application- or ﬁeld-\\\\nlevel encryption of PHI as it is processed within an application or database platform hosted in \\\\nan Amazon EC2 instance. Approaches range from encrypting data using standard libraries in an \\\\napplication framework such as Java or .NET; leveraging Transparent Data Encryption features in \\\\nMicrosoft SQL or Oracle; or by integrating other third-party and software as a service (SaaS)-based \\\\nsolutions into their applications.\\\\nCustomers can choose to integrate their applications running in Amazon EC2 with AWS KMS SDKs,', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Customers can choose to integrate their applications running in Amazon EC2 with AWS KMS SDKs, \\\\nsimplifying the process of key management and storage. Customers can also implement encryption \\\\nof data at rest using ﬁle-level or full disk encryption (FDE) by using third-party software from AWS \\\\nMarketplace Partners or native ﬁle system encryption tools (such as dm-crypt, LUKS, etc.).\\\\nNetwork traﬃc containing PHI must encrypt data in transit. For traﬃc between external sources \\\\n(such as the internet or a traditional IT environment) and Amazon EC2, customers should use open \\\\nstandard transport encryption mechanisms such as Transport Layer Security (TLS) or IPsec virtual \\\\nprivate networks (VPNs), consistent with the Guidance. Internal to an Amazon Virtual Private Cloud \\\\n(VPC) for data traveling between Amazon EC2 instances, network traﬃc containing PHI must also \\\\nbe encrypted; most applications support TLS or other protocols providing in transit encryption that', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='can be conﬁgured to be consistent with the Guidance. For applications and protocols that do not \\\\nsupport encryption, sessions transmitting PHI can be sent through encrypted tunnels using IPsec or \\\\nsimilar implementations between instances.\\\\nAmazon Elastic Container Registry\\\\nAmazon Elastic Container Registry (Amazon ECR) is integrated with Amazon Elastic Container \\\\nService (Amazon ECS) and allows customers to easily store, run, and manage container images for \\\\napplications running on Amazon ECS. After customers specify the Amazon ECR repository in their \\\\nTask Deﬁnition, Amazon ECS will retrieve the appropriate images for their applications.\\\\nNo special steps are required to use Amazon ECR with container images that contain PHI. Container \\\\nimages are encrypted while in transit and stored encrypted while at-rest using Amazon S3 server-\\\\nside encryption (SSE-S3).\\\\nAmazon EC2 20', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon Elastic Container Service\\\\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container \\\\nmanagement service that supports Docker containers and allows customers to easily run \\\\napplications on a managed cluster of Amazon EC2 instances. Amazon ECS eliminates the need for \\\\ncustomers to install, operate, and scale their own cluster management infrastructure.\\\\nWith simple API calls, customers can launch and stop Docker-enabled applications, query the \\\\ncomplete state of their cluster, and access many familiar features like security groups, Elastic Load \\\\nBalancing, EBS volumes, and IAM roles. Customers can use Amazon ECS to schedule the placement \\\\nof containers across their cluster based on their resource needs and availability requirements.\\\\nUsing ECS with workloads that process PHI requires no additional conﬁguration. ECS acts as an', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Using ECS with workloads that process PHI requires no additional conﬁguration. ECS acts as an \\\\norchestration service that coordinates the launch of containers (images for which are stored in \\\\nS3) on EC2, and it does not operate with or upon data within the workload being orchestrated. \\\\nConsistent with HIPAA regulations and the AWS Business Associate Addendum, PHI should \\\\nbe encrypted in transit and at-rest when accessed by containers launched with ECS. Various \\\\nmechanisms for encrypting at-rest are available with each AWS storage option (for example, \\\\nS3, EBS, and KMS). Ensuring complete encryption of PHI sent between containers may also lead \\\\ncustomers to deploy an overlay network (such as VNS3, Weave Net or similar), in order to provide \\\\na redundant layer of encryption. Nevertheless, complete logging should also be enabled (for \\\\nexample, through CloudTrail), and all container instance logs should be directed to CloudWatch.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='example, through CloudTrail), and all container instance logs should be directed to CloudWatch.\\\\nUsing Firelens and AWS for Fluent Bit with workloads that process PHI requires no additional \\\\nconﬁguration, unless the logs contain PHI. If logs contain PHI, then they should not be emitted to \\\\nlog ﬁles, unless the disk encryption is enabled. Instead, conﬁgure your application to emit logs to \\\\nstandard out/error which will be automatically collected by FireLens. Similarly, do not enable ﬁle \\\\nbuﬀering for Fluent Bit, unless disk encryption is also enabled. Finally, the log destination must \\\\nsupport encryption-in-transit; all of the AWS Service output plugins in AWS for Fluent Bit will \\\\nalways use TLS encryption to export logs.\\\\nAmazon Elastic File System (Amazon EFS)\\\\nAmazon Elastic File System (Amazon EFS) provides simple, scalable, elastic ﬁle storage for use with \\\\nAWS Cloud services and on-premises resources. It is easy to use and oﬀers a simple interface that', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='AWS Cloud services and on-premises resources. It is easy to use and oﬀers a simple interface that \\\\nallows customers to create and conﬁgure ﬁle systems quickly and easily. Amazon EFS is built to \\\\nelastically scale on demand without disrupting applications, growing and shrinking automatically \\\\nas customers add and remove ﬁles.\\\\nAmazon ECS 21', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nTo satisfy the requirement that PHI be encrypted at-rest, two paths are available on EFS. EFS \\\\nsupports encryption at-rest when a new ﬁle system is created. During creation, the option for \\\\n“Enable encryption of data at rest” should be selected. Selecting this option ensures that all data \\\\nplaced on the EFS ﬁle system will be encrypted using AES-256 encryption and AWS KMS-managed \\\\nkeys. Customers may alternatively choose to encrypt data before it is placed on EFS, but they are \\\\nthen responsible for managing the encryption process and key management.\\\\nPHI should not be used as all or part of any ﬁle name or folder name. Encryption of PHI while \\\\nin transit for Amazon EFS is provided by Transport Layer Security (TLS) between the EFS service \\\\nand the instance mounting the ﬁle system. EFS oﬀers a mount helper to facilitate connecting to \\\\na ﬁle system using TLS. By default, TLS is not used and must be enabled when mounting the ﬁle', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='a ﬁle system using TLS. By default, TLS is not used and must be enabled when mounting the ﬁle \\\\nsystem using the EFS mount helper. Ensure that the mount command contains the “-o tls” option \\\\nto enable TLS encryption. Alternatively, customers who choose not to use the EFS mount helper \\\\ncan follow the instructions in the EFS documentation to conﬁgure their NFS clients to connect \\\\nthrough a TLS tunnel.\\\\nAmazon Elastic Kubernetes Service (Amazon EKS)\\\\nAmazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy \\\\nfor customers to run Kubernetes on AWS without needing to stand up or maintain their own \\\\nKubernetes control plane. Kubernetes is an open-source system for automating the deployment, \\\\nscaling, and management of containerized applications. For additional Security and Compliance \\\\ninformation, refer to the Architecting for HIPAA Security and Compliance on Amazon EKS\\\\nwhitepaper.\\\\nAmazon ElastiCache (Redis OSS)', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='whitepaper.\\\\nAmazon ElastiCache (Redis OSS)\\\\nAmazon ElastiCache (Redis OSS) is a Redis-compatible in-memory data structure service that can \\\\nbe used as a data store or cache. In order to store PHI, customers must ensure that they are running \\\\nthe latest HIPAA-eligible ElastiCache (Redis OSS) engine version and current generation node \\\\ntypes. Amazon ElastiCache (Redis OSS) supports storing PHI for the following node types and Redis \\\\nengine version:\\\\n•Node Types: current generation only (for example, as of the time of publication of this \\\\nwhitepaper, M4, M5, R4, R5, T2, T3)\\\\n•ElastiCache (Redis OSS) engine version: 3.2.6 and 4.0.10 onwards\\\\nAmazon EKS 22', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nFor more information about choosing current generation nodes, see Amazon ElastiCache pricing. \\\\nFor more information about choosing an ElastiCache (Redis OSS) engine, see What Is Amazon \\\\nElastiCache (Redis OSS)?\\\\nCustomers must also ensure that the cluster and nodes within the cluster are conﬁgured to \\\\nencrypt data at rest, enable transport encryption and enable authentication of Redis commands. In \\\\naddition, customers must also ensure that their Redis clusters are updated with the latest ‘Security’ \\\\ntype service updates on or before the ‘Recommended Apply by Date’ (the date by which it is \\\\nrecommended the update be applied) at all times. For more information, see the sections below.\\\\nTopics\\\\n•Encryption at Rest\\\\n•Transport Encryption\\\\n•Authentication\\\\n•Applying ElastiCache Service Updates\\\\nEncryption at Rest\\\\nAmazon ElastiCache (Redis OSS) provides data encryption for its cluster to help protect the data', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon ElastiCache (Redis OSS) provides data encryption for its cluster to help protect the data \\\\nat rest. When customers enable encryption at-rest for a cluster at the time of creation, Amazon \\\\nElastiCache (Redis OSS) encrypts data on disk and automated Redis backups. Customer data on \\\\ndisk is encrypted using hardware accelerated Advanced Encryption Standard (AES)-512 symmetric \\\\nkeys. Redis backups are encrypted through Amazon S3-managed encryption keys (SSE-S3). A S3 \\\\nbucket with server-side encryption enabled will encrypt the data using hardware-accelerated \\\\nAdvanced Encryption Standard (AES)-256 symmetric keys before saving it in the bucket.\\\\nFor more details on Amazon S3-managed encryption keys (SSE-S3), see Protecting Data Using \\\\nServer-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3). On an ElastiCache \\\\n(Redis OSS) cluster (single or multi-node) running with encryption, data stored at-rest is encrypted', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='consistent with the Guidance in eﬀect at the time of publication of this whitepaper. This includes \\\\ndata on disk and automated backups in S3 bucket. Because the Guidance might be updated, \\\\ncustomers should continue to evaluate and determine whether Amazon ElastiCache (Redis OSS) \\\\nencryption satisﬁes their compliance and regulatory requirements. For more information about \\\\nencryption at-rest using Amazon ElastiCache (Redis OSS), see What Is Amazon ElastiCache (Redis \\\\nOSS)?\\\\nEncryption at Rest 23', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nTransport Encryption\\\\nAmazon ElastiCache (Redis OSS) uses TLS to encrypt the data in transit. Connections to ElastiCache \\\\n(Redis OSS) containing PHI must use transport encryption and evaluate the conﬁguration for \\\\nconsistency with the Guidance. For more information, see CreateReplicationGroup. For more \\\\ninformation on enabling transport encryption, see ElastiCache (Redis OSS) In-Transit Encryption \\\\n(TLS).\\\\nAuthentication\\\\nAmazon ElastiCache (Redis OSS) clusters (single/multi node) that contain PHI must provide a Redis \\\\nAUTH token to enable authentication of Redis commands. Redis AUTH is available when both \\\\nencryption at-rest and encryption-in transit are enabled. Customers should provide a strong token \\\\nfor Redis AUTH with following constraints:\\\\n•Must be only printable ASCII characters\\\\n•Must be at least 16 characters and no more than 128 characters in length\\\\n•Cannot contain any of the following characters: \\\\'/\\\\', \\\\'\\\"\\\\', or \\\"@\\\"', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='•Cannot contain any of the following characters: \\\\'/\\\\', \\\\'\\\"\\\\', or \\\"@\\\"\\\\nThis token must be set from within the Request Parameter at the time of Redis replication group \\\\n(single/multi node) creation and can be updated later with a new value. AWS encrypts this token \\\\nusing AWS Key Management Service (AWS KMS). For more information on Redis AUTH, see\\\\nElastiCache (Redis OSS) In-Transit Encryption (TLS).\\\\nApplying ElastiCache Service Updates\\\\nAmazon ElastiCache (Redis OSS) clusters (single/multi node) that contain PHI must be updated \\\\nwith the latest ‘Security’ type service updates on or before the ‘Recommended Apply by \\\\nDate.’ ElastiCache oﬀers this as a self-service feature that customers can use to apply the \\\\nupdates anytime on demand and in real time. Each service update comes with a ‘Severity’ and \\\\n‘Recommended Apply by Date’ and is available only for the applicable Redis replication groups.\\\\nThe ‘SLA Met’ ﬁeld in the service update feature will state whether the update was applied on or', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='The ‘SLA Met’ ﬁeld in the service update feature will state whether the update was applied on or \\\\nbefore the ‘Recommended Apply by Date’. If customers choose to not apply the updates to the \\\\napplicable Redis replication groups by the ‘Recommended Apply by Date,’ ElastiCache will not take \\\\nany action to apply them. Customers can use the service updates history dashboard to review the \\\\napplication of updates to their Redis replication groups over time. For more information on how to \\\\nuse this feature, see Self-Service Updates in Amazon ElastiCache.\\\\nTransport Encryption 24', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon OpenSearch Service\\\\nAmazon OpenSearch Service enables customers to run a managed OpenSearch or legacy \\\\nElasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using \\\\nOpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later. \\\\nCustomers should ensure PHI is encrypted at-rest and in-transit within Amazon OpenSearch \\\\nService. Customers may use AWS KMS key encryption to encrypt data at rest in their OpenSearch \\\\nService domains, which is only available for OpenSearch and Elasticsearch 5.1 or later. For \\\\nmore information about how to encrypt data at rest, see Encryption of data at rest for Amazon \\\\nOpenSearch Service.\\\\nEach OpenSearch Service domain runs in its own VPC. Customers should enable node-to-node \\\\nencryption, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later. If', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='encryption, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later. If \\\\ncustomers send data to OpenSearch Service over HTTPS, node-to-node encryption helps ensure \\\\nthat their data remains encrypted as OpenSearch distributes (and redistributes) it throughout \\\\nthe cluster. If data arrives unencrypted over HTTP, OpenSearch Service encrypts the data after it \\\\nreaches the cluster. Therefore, any PHI that enters an Amazon OpenSearch Service cluster should \\\\nbe sent over HTTPS. For more information, see Node-to-node encryption for Amazon OpenSearch \\\\nService.\\\\nLogs from the OpenSearch Service conﬁguration API can be captured in AWS CloudTrail. For more \\\\ninformation, see Monitoring Amazon OpenSearch Service API calls with AWS CloudTrail.\\\\nAmazon EMR\\\\nAmazon EMR deploys and manages a cluster of Amazon EC2 instances into a customer’s account. \\\\nFor information on encryption with Amazon EMR, see Encryption Options.\\\\nAmazon EventBridge', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='For information on encryption with Amazon EMR, see Encryption Options.\\\\nAmazon EventBridge\\\\nAmazon EventBridge (formerly Amazon CloudWatch Events) is a serverless event bus that enables \\\\nyou to create scalable event-driven applications. EventBridge delivers a stream of real- time data \\\\nfrom event sources, such as Zendesk, Datadog, or Pagerduty, and routes that data to targets like \\\\nAWS Lambda.\\\\nBy default, EventBridge encrypts data using 256-bit Advanced Encryption Standard (AES-256)\\\\nunder an AWS owned CMK, which helps secure customer data from unauthorized access. Customers \\\\nAmazon OpenSearch Service 25', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nshould ensure that any AWS resource emitting an event that is storing, processing, or transmitting \\\\nPHI is conﬁgured in accordance with best practices.\\\\nAmazon EventBridge is integrated with AWS CloudTrail and customers can view the most \\\\nrecent events in the CloudTrail console in Event history. For more information, see EventBridge \\\\nInformation in CloudTrail.\\\\nAmazon Forecast\\\\nAmazon Forecast is a fully managed service that uses machine learning to deliver highly accurate \\\\nforecasts. Based on the same machine learning forecasting technology used by Amazon.com. \\\\nEvery interaction customers have with Amazon Forecast is protected by encryption. Any content \\\\nprocessed by Amazon Forecast is encrypted with customer keys through Amazon Key Management \\\\nService, and encrypted at-rest in the AWS Region where customers are using the service.\\\\nAmazon Forecast is integrated with AWS CloudTrail, a service that provides a record of actions', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon Forecast is integrated with AWS CloudTrail, a service that provides a record of actions \\\\ntaken by a user, role, or an AWS service in Amazon Forecast. CloudTrail captures all API calls for \\\\nAmazon Forecast as events. The calls captured include calls from the Amazon Forecast console and \\\\ncode calls to the Amazon Forecast API operations. If customers create a trail, customers can enable \\\\ncontinuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon \\\\nForecast. For more information, see Logging Forecast API Calls with AWS CloudTrail.\\\\nBy default, the log ﬁles delivered by CloudTrail to their bucket are encrypted by Amazon server-\\\\nside encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer \\\\nthat is directly manageable, customers can instead use server-side encryption with AWS KMS–\\\\nmanaged keys (SSE-KMS) for their CloudTrail log ﬁles. Enabling server-side encryption encrypts', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='managed keys (SSE-KMS) for their CloudTrail log ﬁles. Enabling server-side encryption encrypts \\\\nthe log ﬁles but not the digest ﬁles with SSE-KMS. Digest ﬁles are encrypted with Amazon S3-\\\\nmanaged encryption keys (SSE-S3).\\\\nAWS Forecast imports and exports data to/from S3 buckets. When importing and exporting data \\\\nfrom Amazon S3, customers should ensure S3 buckets are conﬁgured in a manner consistent with \\\\nthe guidance. For more information, see Getting Started.\\\\nAmazon FSx\\\\nAmazon FSx is a fully-managed service providing feature-rich and highly-performant ﬁle systems. \\\\nAmazon FSx for Windows File Server provides highly reliable and scalable ﬁle storage and is \\\\naccessible over the Server Message Block (SMB) protocol. Amazon FSx for Lustre provides high-\\\\nAmazon Forecast 26', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nperformance storage for compute workloads and is powered by Lustre, the world's most popular \\\\nhigh-performance ﬁle system.\\\\nAmazon FSx supports two forms of encryption for ﬁle systems, encryption of data in transit and \\\\nencryption at rest. Amazon FSx for Windows File Server also supports logging of all API calls using \\\\nAWS CloudTrail.\\\\nEncryption of data in transit is supported by Amazon FSx for Windows File Server on compute \\\\ninstances supporting SMB protocol 3.0 or newer, and by Amazon FSx for Lustre on Amazon EC2 \\\\ninstances that support encryption in transit. Alternatively, customers may encrypt data before \\\\nstoring on Amazon FSx but are then responsible for the encryption process and key management.\\\\nEncryption of data at rest is automatically enabled when creating an Amazon FSx ﬁle system, using \\\\nAES-256 encryption algorithm and AWS KMS-managed keys. Data and metadata are automatically\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='AES-256 encryption algorithm and AWS KMS-managed keys. Data and metadata are automatically \\\\nencrypted before being written to the ﬁle system, and automatically decrypted before being \\\\npresented to the application. PHI should not be used in any ﬁle or folder name.\\\\nAmazon GuardDuty\\\\nAmazon GuardDuty is a managed threat detection service that continuously monitors for malicious \\\\nor unauthorized behavior to help customers protect their AWS accounts and workloads. It monitors \\\\nfor activity such as unusual API calls or potentially unauthorized deployments that indicate a \\\\npossible account compromise. Amazon GuardDuty also detects potentially compromised instances \\\\nor reconnaissance by attackers.\\\\nAmazon GuardDuty continuously monitors and analyzes the following data sources: VPC Flow Logs, \\\\nAWS CloudTrail event logs, and DNS logs. It uses threat intelligence feeds, such as lists of malicious \\\\nIPs and domains, and machine learning to identify unexpected and potentially unauthorized and', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'})]```\\nBULLET POINT SUMMARY:\\n\\n&lt;|eot_id|&gt;\\n&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"temperature\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"top_p\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"max_gen_len\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"prompt\"\u001b[0m: \u001b[32m\"\\n<|begin_of_text|>\\n<|start_header_id|>user<|end_header_id|>\\nWrite a detailed summary of the following text delimited by triple backquotes.\\nReturn your response in bullet points which covers the key points of the text.\\n```[Document(page_content='For more details on conﬁguration requirements, see the Amazon CloudWatch Logs section.\\\\nAmazon CloudWatch Events\\\\nAmazon CloudWatch Events delivers a near-real-time stream of system events that describe \\\\nchanges in AWS resources. Customers should ensure that PHI does not ﬂow into CloudWatch \\\\nEvents, and any AWS resource emitting a CloudWatch event that is storing, processing, or \\\\ntransmitting PHI is conﬁgured in accordance with the Guidance.\\\\nCustomers can conﬁgure Amazon CloudWatch Events to register as an AWS API call in CloudTrail. \\\\nFor more information, see Creating a CloudWatch Events Rule That Triggers on an AWS API Call \\\\nUsing AWS CloudTrail.\\\\nAmazon CloudWatch Logs\\\\nCustomers can use Amazon CloudWatch Logs to monitor, store, and access their log ﬁles from \\\\nAmazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route\\\\xa053, and \\\\nother sources. They can then retrieve the associated log data from CloudWatch Logs. Log data is', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='other sources. They can then retrieve the associated log data from CloudWatch Logs. Log data is \\\\nencrypted while in transit and while it is at-rest. As a result, it is not necessary to re-encrypt PHI \\\\nemitted by any other service and delivered to CloudWatch Logs.\\\\nLambda@Edge 12', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon Comprehend\\\\nAmazon Comprehend uses natural language processing to extract insights about the content of \\\\ndocuments. Amazon Comprehend processes any text ﬁle in UTF-8 format. It develops insights \\\\nby recognizing the entities, key phrases, language, sentiments, and other common elements in \\\\na document. Amazon Comprehend can be used with data containing PHI. Amazon Comprehend \\\\ndoes not retain or store any data and all calls to the API are encrypted with SSL/TLS. Amazon \\\\nComprehend uses CloudTrail to log all API calls.\\\\nAWS Identity and Access Management\\\\nSecurity access functions such as authentication and authorization are required for accessing \\\\nAmazon Comprehend and can be controlled with AWS Identity and Access Management (IAM), and \\\\ncredentials can be used to access the IAM. For more information, see Authentication and Access \\\\nControl for Amazon Comprehend in the Amazon Comprehend User Guide .\\\\nAccount management', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Control for Amazon Comprehend in the Amazon Comprehend User Guide .\\\\nAccount management\\\\nBy default, IAM users don't have permission to create or modify Amazon Comprehend resources, or \\\\nperform tasks using the Amazon Comprehend API. To allow users to create or modify resources and \\\\nperform tasks, customers are responsible for leveraging IAM policies that grant users permissions \\\\nfor the speciﬁc resources (such as Amazon Comprehend and API actions) users need to use, and \\\\nthen attach policies to the users or groups that require speciﬁc permissions.\\\\nWith Amazon Comprehend you can use AWS Identity and Access Management (IAM) to create \\\\na user with an attached policy to enable Amazon Comprehend permissions. Optionally, you can \\\\nchoose to create custom policies to attach to a role. Then, you can add administrators to the \\\\nrole with the ability to invoke API's for Amazon Comprehend administration in accordance with \\\\norganization-deﬁned role-based access and least privilege principles.\\\\nIdentity and access\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='organization-deﬁned role-based access and least privilege principles.\\\\nIdentity and access\\\\nWith Amazon Comprehend you can require user to authenticate to AWS using multi-factor \\\\nauthentication in accordance with their organizational requirements for authentication.\\\\nUsing the AWS Management Console, IAM administrators can create a customer-managed policy \\\\nthat denies all permissions except those required for users to manage their own credentials and \\\\nMFA devices. A JSON policy template is available on the My Security Credential page  in the IAM \\\\nconsole.\\\\nAmazon Comprehend 13', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nOptionally, you can leverage compatible third-party MFA capabilities with IAM partners. For \\\\nadditional information, see IAM Partners.\\\\nAdministration\\\\nWe recommend that you Amazon Comprehend select identity-based policies in which account \\\\nadministrators can attach permissions policies to IAM identities (users, groups, and roles) and \\\\nthereby grant permissions to perform operations on Amazon Comprehend resources.\\\\nA list of API actions for Amazon Comprehend can be found in the API Reference guide. You \\\\nshould also consider authorizing access to predeﬁned IAM policies, customer IAM policies and API \\\\nactions to users or roles in accordance with their least privilege and role-based organizational \\\\nrequirements. For more information, see Using the Amazon Comprehend API in the Developer \\\\nGuide .\\\\nExternal authentication\\\\nAmazon Comprehend is compatible with identity federation using IAM roles. This enables Amazon', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon Comprehend is compatible with identity federation using IAM roles. This enables Amazon \\\\nComprehend your users to authenticate to AWS by assuming a role that administrators have \\\\nprovisioned. Users accessing AWS using credentials from their organization or a third-party assume \\\\na role indirectly.\\\\nAWS support for Kerberos and Active Directory provides the beneﬁts of single sign-on and \\\\ncentralized authentication of database users. AWS users can choose to manage and store user \\\\ncredentials either in AWS Directory Service for Microsoft Active Directory or in the customer on-\\\\npremises Active Directory.\\\\nData ﬂow enforcement\\\\nAWS customers and APN partners, acting either as data controllers or data processors, are \\\\nresponsible for any personal data that they put in the AWS Cloud and Amazon Comprehend. You \\\\nare responsible for controlling the ﬂow to data inputs and outputs for Amazon Comprehend using \\\\nIAM policies.\\\\nData protection and secrets management', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='IAM policies.\\\\nData protection and secrets management\\\\nThe AWS shared responsibility model applies to data protection in Amazon Comprehend. As \\\\ndescribed in this model, AWS is responsible for protecting the global infrastructure that runs all of \\\\nthe AWS Cloud. You are responsible for maintaining control over your content that is hosted on this \\\\nData protection and secrets management 14', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\ninfrastructure. This content includes the security conﬁguration and management tasks for the AWS \\\\nservices that you use. For more information about data privacy, see the Data Privacy FAQ.\\\\nThe Data Protection in Amazon Comprehend section in the Amazon Comprehend Developer Guide\\\\nprovides tips that you should consider in protecting data such as using TLS for transmission and \\\\navoiding placement of sensitive information into tags or free-form ﬁelds.\\\\nEncryption of data-at-rest\\\\nAmazon Comprehend works with AWS Key Management Service (AWS KMS) to provide enhanced \\\\nencryption for your data. Amazon Simple Storage Service (Amazon S3) already enables you to \\\\nencrypt your input documents when creating a text analysis, topic modeling, or custom Amazon \\\\nComprehend job. Integration with AWS KMS enables you to encrypt the data in the storage volume', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Comprehend job. Integration with AWS KMS enables you to encrypt the data in the storage volume \\\\nfor start* and create*  jobs, and it encrypts the output results of start* jobs using your own AWS \\\\nKMS key.\\\\nIt is best practice for Amazon Comprehend users to encrypt Amazon S3 buckets used for input \\\\ndocuments using available S3 encryption solutions in accordance with their organizational policies.\\\\nThe AWS Management Console, encrypts Amazon Comprehend custom models with its own AWS \\\\nKMS key. For the AWS CLI, Amazon Comprehend can encrypt custom models using either its own \\\\nAWS KMS key or a provided customer managed key (CMK).\\\\nIf selecting encryption when using the AWS Management Console, you can choose either or both of \\\\nthe following optional methods:\\\\n•Volume encryption - ensures that the data on an EBS Volume used by Comprehend is encrypted \\\\nduring training/inference (data is ﬂushed after training/inference, so this key is relevant only \\\\nwhile the job is in progress).', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='while the job is in progress).\\\\n•Output result encryption - for encrypting the output stored by comprehend in the customer’s \\\\nbucket using a customer provided AWS KMS key.\\\\nFor more information about encryption types such as volume encryption, see AWS KMS Encryption \\\\nin Amazon Comprehend.\\\\nPersonally identiﬁable information\\\\nYou can use the Amazon Comprehend console or APIs to detect personally identiﬁable information \\\\n(PII) in English text documents. For more information about about detecting and labeling PII \\\\nData protection and secrets management 15', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nentities and operating various PII analysis jobs, see the Personally identiﬁable information section \\\\nin the Amazon Comprehend Developer Guide .\\\\nData deletion\\\\nIf you are an Amazon Comprehend customer using Amazon S3 and choosing to manage your \\\\nown AWS KMS keys, you should consider revoking AWS KMS keys and deﬁning the procedural \\\\njustiﬁcation to do so in accordance with their organizational requirements. Revocation of the AWS \\\\nKMS key for Amazon S3 renders any data unusable/unreadable.\\\\nNetwork segmentation and hardening\\\\nAs a managed service, Amazon Comprehend adheres to the AWS Best Practices for Security, \\\\nIdentity, and Compliance.\\\\nFor recommended network security safeguards, see Infrastructure Security in Amazon Comprehend\\\\nin the Amazon Comprehend Developers Guide .\\\\nProtect jobs using an Amazon Virtual Private Cloud (Amazon VPC)', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Protect jobs using an Amazon Virtual Private Cloud (Amazon VPC)\\\\nAmazon Comprehend uses a variety of security measures to ensure the safety of your data with our \\\\njob containers where it's stored while being used by Amazon Comprehend. However, job containers \\\\naccess AWS resources—such as the Amazon S3 buckets where you store data and model artifacts—\\\\nover the internet.\\\\nTo control access to your data, we recommend that you create a virtual private cloud (VPC) and \\\\nconﬁgure it so that the data and containers aren't accessible over the internet. For information \\\\nabout creating and conﬁguring a VPC, see Getting Started With Amazon VPC in the Amazon VPC \\\\nUser Guide . Using a VPC helps to protect your data because you can conﬁgure your VPC so that it \\\\nis not connected to the internet. Using a VPC also allows you to monitor all network traﬃc in and \\\\nout of our job containers by using VPC ﬂow logs. For more information, see VPC Flow Logs in the\\\\nAmazon VPC User Guide .\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon VPC User Guide .\\\\nYou specify your VPC conﬁguration when you create a job, by specifying the subnets and security \\\\ngroups. When you specify the subnets and security groups, Amazon Comprehend creates elastic \\\\nnetwork interfaces (ENIs) that are associated with your security groups in one of the subnets. ENIs \\\\nallow our job containers to connect to resources in your VPC. For information about ENIs, see\\\\nElastic Network Interfaces in the Amazon VPC User Guide .\\\\nNetwork segmentation and hardening 16', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nNote\\\\nFor jobs, you can only conﬁgure subnets with a default tenancy VPC in which your instance \\\\nruns on shared hardware. For more information on the tenancy attribute for VPCs, see\\\\nDedicated Instances in the Amazon EC2 User Guide for Linux Instances.\\\\nYou can establish a private connection between your VPC and Amazon Comprehend by creating \\\\nan interface VPC endpoint. For more information, see Amazon Comprehend and Interface VPC \\\\nEndpoints (AWS PrivateLink).\\\\nHost and image hardening\\\\nBased on the AWS shared responsibility model, host and image hardening of the AWS environment \\\\nfor Amazon Comprehend is managed by AWS as a provided service.\\\\nMulti-tenancy\\\\nTo help make your recommendation more secure, we recommend that you implement the \\\\nfollowing multi-tenancy security recommendations:\\\\n•Use only a veriﬁed email address to authorize user access to a tenant based on domain match.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"•Use only a veriﬁed email address to authorize user access to a tenant based on domain match. \\\\nDo not trust email addresses and phone numbers unless your app veriﬁes them, or the external \\\\nIdP gives a proof of veriﬁcation. For more details on setting these permissions, see Attribute \\\\nPermissions and Scopes.\\\\n•Use immutable or mutable attributes for the user proﬁle attributes that identify tenants. \\\\nAdministrators must be able to change these attributes. Also, give app clients read-only access to \\\\nthe attributes.\\\\n•Use 1:1 mapping between external IdP and application client to prevent unauthorized cross-\\\\ntenant access. A user who has been authenticated by an external IdP, and who has a valid \\\\nAmazon Cognito session cookie, can access other tenant apps that trust the same IdP.\\\\n•When you implement tenant-matching and authorization logic in your application, restrict \\\\nusers so that they can't modify the criteria that authorize user access to the tenants. Also, if an\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"external IdP is being used for federation, restrict tenant identity provider administrators so that \\\\nthey can't modify user access.\\\\nHost and image hardening 17\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nCross-service confused deputy prevention\\\\nThe confused deputy problem is a multi-tenancy security issue where an entity that doesn't \\\\nhave permission to perform an action can coerce a more-privileged entity to perform the action. \\\\nIn AWS, cross-service impersonation can result in the confused deputy problem. Cross-service \\\\nimpersonation can occur when one service (the calling service) calls another service (the called \\\\nservice). The calling service can be manipulated to use its permissions to act on another customer's \\\\nresources in a way it should not otherwise have permission to access. To prevent this, AWS provides \\\\ntools that can help you protect your data for all services with service principals that have been \\\\ngiven access to resources in your account. For more information which includes safeguards you \\\\nshould consider for addressing this security issue, see Cross-service Confused Deputy Prevention in\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='the Amazon Comprehend Developer Guide .\\\\nAmazon Comprehend Medical\\\\nFor guidance, see the previous Amazon Comprehend section.\\\\nAmazon Connect\\\\nAmazon Connect is a self-service, cloud-based contact center service that enables dynamic, \\\\npersonal, and natural customer engagement at any scale. Customers should not include any PHI \\\\nin any ﬁelds associated with managing users, security proﬁles, and contact ﬂows within Amazon \\\\nConnect.\\\\nAmazon Connect Customer Proﬁles, a feature of Amazon Connect, equips contact center agents \\\\nwith a more uniﬁed view of a customer’s proﬁle with the most up to date information, to provide \\\\nmore personalized customer service. Customer Proﬁles is designed to automatically bring together \\\\ncustomer information from multiple applications into a uniﬁed customer proﬁle, delivering the \\\\nproﬁle directly to the agent as soon as the support call or interaction begins. Customers should \\\\nrefrain from naming domains or object keys with PHI data. The contents of Domains and Objects', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='refrain from naming domains or object keys with PHI data. The contents of Domains and Objects \\\\nare encrypted and protected, but the key identiﬁers are not.\\\\nAmazon DocumentDB (with MongoDB compatibility)\\\\nAmazon DocumentDB (with MongoDB compatibility) (Amazon DocumentDB) oﬀers encryption \\\\nat-rest during cluster creation via AWS KMS, which allows customers to encrypt databases using \\\\nAWS or customer-managed keys. On a database instance running with encryption enabled, data \\\\nCross-service confused deputy prevention 18', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nstored at-rest is encrypted consistent with the Guidance in eﬀect at the time of publication of this \\\\nwhitepaper, as are automated backups, read replicas, and snapshots. Because the Guidance might \\\\nbe updated, customers should continue to evaluate and determine whether Amazon DocumentDB \\\\nencryption satisﬁes their compliance and regulatory requirements. For more information on \\\\nencryption at-rest using Amazon DocumentDB, see Encrypting Amazon DocumentDB Data at Rest.\\\\nConnections to Amazon DocumentDB containing PHI must use endpoints that accept encrypted \\\\ntransport (HTTPS). By default, a newly created Amazon DocumentDB cluster only accepts secure \\\\nconnections using Transport Layer Security (TLS). For more information, see Encrypting Data in \\\\nTransit. Amazon DocumentDB uses AWS CloudTrail to log all API calls. For more information, see\\\\nLogging and Monitoring in Amazon DocumentDB.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Logging and Monitoring in Amazon DocumentDB.\\\\nFor certain management features, Amazon DocumentDB uses operational technology that is \\\\nshared with Amazon RDS. Amazon DocumentDB console, AWS CLI, and API calls are logged as calls \\\\nmade to the Amazon RDS API.\\\\nAmazon DynamoDB\\\\nConnections to Amazon DynamoDB containing PHI must use endpoints that accept encrypted \\\\ntransport (HTTPS). For a list of regional endpoints, see AWS service endpoints.\\\\nAmazon DynamoDB oﬀers DynamoDB encryption, which allows customers to encrypt databases \\\\nusing keys that customers manage through AWS KMS. On a database instance running with \\\\nAmazon DynamoDB encryption, data stored at-rest in the underlying storage is encrypted \\\\nconsistent with the Guidance in eﬀect at the time of publication of this whitepaper, as are \\\\nautomated backups, read replicas, and snapshots.\\\\nBecause the Guidance might be updated, customers should continue to evaluate and determine', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Because the Guidance might be updated, customers should continue to evaluate and determine \\\\nwhether Amazon DynamoDB encryption satisﬁes their compliance and regulatory requirements. \\\\nFor more information on encryption at-rest using Amazon DynamoDB, see DynamoDB Encryption \\\\nat Rest.\\\\nAmazon Elastic Block Store\\\\nAmazon EBS encryption at-rest is consistent with the Guidance that is in eﬀect at the time of \\\\npublication of this whitepaper. Because the Guidance might be updated, customers should \\\\ncontinue to evaluate and determine whether Amazon EBS encryption satisﬁes their compliance \\\\nand regulatory requirements. With Amazon EBS encryption, a unique volume encryption key is \\\\ngenerated for each EBS volume. Customers have the ﬂexibility to choose which KMS key from \\\\nAmazon DynamoDB 19', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nthe AWS Key Management Service is used to encrypt each volume key. For more information, see\\\\nAmazon EBS encryption.\\\\nAmazon Elastic Compute Cloud\\\\nAmazon EC2 is a scalable, user-conﬁgurable compute service that supports multiple methods \\\\nfor encrypting data at rest. For example, customers might elect to perform application- or ﬁeld-\\\\nlevel encryption of PHI as it is processed within an application or database platform hosted in \\\\nan Amazon EC2 instance. Approaches range from encrypting data using standard libraries in an \\\\napplication framework such as Java or .NET; leveraging Transparent Data Encryption features in \\\\nMicrosoft SQL or Oracle; or by integrating other third-party and software as a service (SaaS)-based \\\\nsolutions into their applications.\\\\nCustomers can choose to integrate their applications running in Amazon EC2 with AWS KMS SDKs,', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Customers can choose to integrate their applications running in Amazon EC2 with AWS KMS SDKs, \\\\nsimplifying the process of key management and storage. Customers can also implement encryption \\\\nof data at rest using ﬁle-level or full disk encryption (FDE) by using third-party software from AWS \\\\nMarketplace Partners or native ﬁle system encryption tools (such as dm-crypt, LUKS, etc.).\\\\nNetwork traﬃc containing PHI must encrypt data in transit. For traﬃc between external sources \\\\n(such as the internet or a traditional IT environment) and Amazon EC2, customers should use open \\\\nstandard transport encryption mechanisms such as Transport Layer Security (TLS) or IPsec virtual \\\\nprivate networks (VPNs), consistent with the Guidance. Internal to an Amazon Virtual Private Cloud \\\\n(VPC) for data traveling between Amazon EC2 instances, network traﬃc containing PHI must also \\\\nbe encrypted; most applications support TLS or other protocols providing in transit encryption that', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='can be conﬁgured to be consistent with the Guidance. For applications and protocols that do not \\\\nsupport encryption, sessions transmitting PHI can be sent through encrypted tunnels using IPsec or \\\\nsimilar implementations between instances.\\\\nAmazon Elastic Container Registry\\\\nAmazon Elastic Container Registry (Amazon ECR) is integrated with Amazon Elastic Container \\\\nService (Amazon ECS) and allows customers to easily store, run, and manage container images for \\\\napplications running on Amazon ECS. After customers specify the Amazon ECR repository in their \\\\nTask Deﬁnition, Amazon ECS will retrieve the appropriate images for their applications.\\\\nNo special steps are required to use Amazon ECR with container images that contain PHI. Container \\\\nimages are encrypted while in transit and stored encrypted while at-rest using Amazon S3 server-\\\\nside encryption (SSE-S3).\\\\nAmazon EC2 20', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon Elastic Container Service\\\\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container \\\\nmanagement service that supports Docker containers and allows customers to easily run \\\\napplications on a managed cluster of Amazon EC2 instances. Amazon ECS eliminates the need for \\\\ncustomers to install, operate, and scale their own cluster management infrastructure.\\\\nWith simple API calls, customers can launch and stop Docker-enabled applications, query the \\\\ncomplete state of their cluster, and access many familiar features like security groups, Elastic Load \\\\nBalancing, EBS volumes, and IAM roles. Customers can use Amazon ECS to schedule the placement \\\\nof containers across their cluster based on their resource needs and availability requirements.\\\\nUsing ECS with workloads that process PHI requires no additional conﬁguration. ECS acts as an', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Using ECS with workloads that process PHI requires no additional conﬁguration. ECS acts as an \\\\norchestration service that coordinates the launch of containers (images for which are stored in \\\\nS3) on EC2, and it does not operate with or upon data within the workload being orchestrated. \\\\nConsistent with HIPAA regulations and the AWS Business Associate Addendum, PHI should \\\\nbe encrypted in transit and at-rest when accessed by containers launched with ECS. Various \\\\nmechanisms for encrypting at-rest are available with each AWS storage option (for example, \\\\nS3, EBS, and KMS). Ensuring complete encryption of PHI sent between containers may also lead \\\\ncustomers to deploy an overlay network (such as VNS3, Weave Net or similar), in order to provide \\\\na redundant layer of encryption. Nevertheless, complete logging should also be enabled (for \\\\nexample, through CloudTrail), and all container instance logs should be directed to CloudWatch.', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='example, through CloudTrail), and all container instance logs should be directed to CloudWatch.\\\\nUsing Firelens and AWS for Fluent Bit with workloads that process PHI requires no additional \\\\nconﬁguration, unless the logs contain PHI. If logs contain PHI, then they should not be emitted to \\\\nlog ﬁles, unless the disk encryption is enabled. Instead, conﬁgure your application to emit logs to \\\\nstandard out/error which will be automatically collected by FireLens. Similarly, do not enable ﬁle \\\\nbuﬀering for Fluent Bit, unless disk encryption is also enabled. Finally, the log destination must \\\\nsupport encryption-in-transit; all of the AWS Service output plugins in AWS for Fluent Bit will \\\\nalways use TLS encryption to export logs.\\\\nAmazon Elastic File System (Amazon EFS)\\\\nAmazon Elastic File System (Amazon EFS) provides simple, scalable, elastic ﬁle storage for use with \\\\nAWS Cloud services and on-premises resources. It is easy to use and oﬀers a simple interface that', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='AWS Cloud services and on-premises resources. It is easy to use and oﬀers a simple interface that \\\\nallows customers to create and conﬁgure ﬁle systems quickly and easily. Amazon EFS is built to \\\\nelastically scale on demand without disrupting applications, growing and shrinking automatically \\\\nas customers add and remove ﬁles.\\\\nAmazon ECS 21', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nTo satisfy the requirement that PHI be encrypted at-rest, two paths are available on EFS. EFS \\\\nsupports encryption at-rest when a new ﬁle system is created. During creation, the option for \\\\n“Enable encryption of data at rest” should be selected. Selecting this option ensures that all data \\\\nplaced on the EFS ﬁle system will be encrypted using AES-256 encryption and AWS KMS-managed \\\\nkeys. Customers may alternatively choose to encrypt data before it is placed on EFS, but they are \\\\nthen responsible for managing the encryption process and key management.\\\\nPHI should not be used as all or part of any ﬁle name or folder name. Encryption of PHI while \\\\nin transit for Amazon EFS is provided by Transport Layer Security (TLS) between the EFS service \\\\nand the instance mounting the ﬁle system. EFS oﬀers a mount helper to facilitate connecting to \\\\na ﬁle system using TLS. By default, TLS is not used and must be enabled when mounting the ﬁle', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='a ﬁle system using TLS. By default, TLS is not used and must be enabled when mounting the ﬁle \\\\nsystem using the EFS mount helper. Ensure that the mount command contains the “-o tls” option \\\\nto enable TLS encryption. Alternatively, customers who choose not to use the EFS mount helper \\\\ncan follow the instructions in the EFS documentation to conﬁgure their NFS clients to connect \\\\nthrough a TLS tunnel.\\\\nAmazon Elastic Kubernetes Service (Amazon EKS)\\\\nAmazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy \\\\nfor customers to run Kubernetes on AWS without needing to stand up or maintain their own \\\\nKubernetes control plane. Kubernetes is an open-source system for automating the deployment, \\\\nscaling, and management of containerized applications. For additional Security and Compliance \\\\ninformation, refer to the Architecting for HIPAA Security and Compliance on Amazon EKS\\\\nwhitepaper.\\\\nAmazon ElastiCache (Redis OSS)', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='whitepaper.\\\\nAmazon ElastiCache (Redis OSS)\\\\nAmazon ElastiCache (Redis OSS) is a Redis-compatible in-memory data structure service that can \\\\nbe used as a data store or cache. In order to store PHI, customers must ensure that they are running \\\\nthe latest HIPAA-eligible ElastiCache (Redis OSS) engine version and current generation node \\\\ntypes. Amazon ElastiCache (Redis OSS) supports storing PHI for the following node types and Redis \\\\nengine version:\\\\n•Node Types: current generation only (for example, as of the time of publication of this \\\\nwhitepaper, M4, M5, R4, R5, T2, T3)\\\\n•ElastiCache (Redis OSS) engine version: 3.2.6 and 4.0.10 onwards\\\\nAmazon EKS 22', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nFor more information about choosing current generation nodes, see Amazon ElastiCache pricing. \\\\nFor more information about choosing an ElastiCache (Redis OSS) engine, see What Is Amazon \\\\nElastiCache (Redis OSS)?\\\\nCustomers must also ensure that the cluster and nodes within the cluster are conﬁgured to \\\\nencrypt data at rest, enable transport encryption and enable authentication of Redis commands. In \\\\naddition, customers must also ensure that their Redis clusters are updated with the latest ‘Security’ \\\\ntype service updates on or before the ‘Recommended Apply by Date’ (the date by which it is \\\\nrecommended the update be applied) at all times. For more information, see the sections below.\\\\nTopics\\\\n•Encryption at Rest\\\\n•Transport Encryption\\\\n•Authentication\\\\n•Applying ElastiCache Service Updates\\\\nEncryption at Rest\\\\nAmazon ElastiCache (Redis OSS) provides data encryption for its cluster to help protect the data', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon ElastiCache (Redis OSS) provides data encryption for its cluster to help protect the data \\\\nat rest. When customers enable encryption at-rest for a cluster at the time of creation, Amazon \\\\nElastiCache (Redis OSS) encrypts data on disk and automated Redis backups. Customer data on \\\\ndisk is encrypted using hardware accelerated Advanced Encryption Standard (AES)-512 symmetric \\\\nkeys. Redis backups are encrypted through Amazon S3-managed encryption keys (SSE-S3). A S3 \\\\nbucket with server-side encryption enabled will encrypt the data using hardware-accelerated \\\\nAdvanced Encryption Standard (AES)-256 symmetric keys before saving it in the bucket.\\\\nFor more details on Amazon S3-managed encryption keys (SSE-S3), see Protecting Data Using \\\\nServer-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3). On an ElastiCache \\\\n(Redis OSS) cluster (single or multi-node) running with encryption, data stored at-rest is encrypted', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='consistent with the Guidance in eﬀect at the time of publication of this whitepaper. This includes \\\\ndata on disk and automated backups in S3 bucket. Because the Guidance might be updated, \\\\ncustomers should continue to evaluate and determine whether Amazon ElastiCache (Redis OSS) \\\\nencryption satisﬁes their compliance and regulatory requirements. For more information about \\\\nencryption at-rest using Amazon ElastiCache (Redis OSS), see What Is Amazon ElastiCache (Redis \\\\nOSS)?\\\\nEncryption at Rest 23', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nTransport Encryption\\\\nAmazon ElastiCache (Redis OSS) uses TLS to encrypt the data in transit. Connections to ElastiCache \\\\n(Redis OSS) containing PHI must use transport encryption and evaluate the conﬁguration for \\\\nconsistency with the Guidance. For more information, see CreateReplicationGroup. For more \\\\ninformation on enabling transport encryption, see ElastiCache (Redis OSS) In-Transit Encryption \\\\n(TLS).\\\\nAuthentication\\\\nAmazon ElastiCache (Redis OSS) clusters (single/multi node) that contain PHI must provide a Redis \\\\nAUTH token to enable authentication of Redis commands. Redis AUTH is available when both \\\\nencryption at-rest and encryption-in transit are enabled. Customers should provide a strong token \\\\nfor Redis AUTH with following constraints:\\\\n•Must be only printable ASCII characters\\\\n•Must be at least 16 characters and no more than 128 characters in length\\\\n•Cannot contain any of the following characters: \\\\'/\\\\', \\\\'\\\"\\\\', or \\\"@\\\"', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='•Cannot contain any of the following characters: \\\\'/\\\\', \\\\'\\\"\\\\', or \\\"@\\\"\\\\nThis token must be set from within the Request Parameter at the time of Redis replication group \\\\n(single/multi node) creation and can be updated later with a new value. AWS encrypts this token \\\\nusing AWS Key Management Service (AWS KMS). For more information on Redis AUTH, see\\\\nElastiCache (Redis OSS) In-Transit Encryption (TLS).\\\\nApplying ElastiCache Service Updates\\\\nAmazon ElastiCache (Redis OSS) clusters (single/multi node) that contain PHI must be updated \\\\nwith the latest ‘Security’ type service updates on or before the ‘Recommended Apply by \\\\nDate.’ ElastiCache oﬀers this as a self-service feature that customers can use to apply the \\\\nupdates anytime on demand and in real time. Each service update comes with a ‘Severity’ and \\\\n‘Recommended Apply by Date’ and is available only for the applicable Redis replication groups.\\\\nThe ‘SLA Met’ ﬁeld in the service update feature will state whether the update was applied on or', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='The ‘SLA Met’ ﬁeld in the service update feature will state whether the update was applied on or \\\\nbefore the ‘Recommended Apply by Date’. If customers choose to not apply the updates to the \\\\napplicable Redis replication groups by the ‘Recommended Apply by Date,’ ElastiCache will not take \\\\nany action to apply them. Customers can use the service updates history dashboard to review the \\\\napplication of updates to their Redis replication groups over time. For more information on how to \\\\nuse this feature, see Self-Service Updates in Amazon ElastiCache.\\\\nTransport Encryption 24', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nAmazon OpenSearch Service\\\\nAmazon OpenSearch Service enables customers to run a managed OpenSearch or legacy \\\\nElasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using \\\\nOpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later. \\\\nCustomers should ensure PHI is encrypted at-rest and in-transit within Amazon OpenSearch \\\\nService. Customers may use AWS KMS key encryption to encrypt data at rest in their OpenSearch \\\\nService domains, which is only available for OpenSearch and Elasticsearch 5.1 or later. For \\\\nmore information about how to encrypt data at rest, see Encryption of data at rest for Amazon \\\\nOpenSearch Service.\\\\nEach OpenSearch Service domain runs in its own VPC. Customers should enable node-to-node \\\\nencryption, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later. If', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='encryption, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later. If \\\\ncustomers send data to OpenSearch Service over HTTPS, node-to-node encryption helps ensure \\\\nthat their data remains encrypted as OpenSearch distributes (and redistributes) it throughout \\\\nthe cluster. If data arrives unencrypted over HTTP, OpenSearch Service encrypts the data after it \\\\nreaches the cluster. Therefore, any PHI that enters an Amazon OpenSearch Service cluster should \\\\nbe sent over HTTPS. For more information, see Node-to-node encryption for Amazon OpenSearch \\\\nService.\\\\nLogs from the OpenSearch Service conﬁguration API can be captured in AWS CloudTrail. For more \\\\ninformation, see Monitoring Amazon OpenSearch Service API calls with AWS CloudTrail.\\\\nAmazon EMR\\\\nAmazon EMR deploys and manages a cluster of Amazon EC2 instances into a customer’s account. \\\\nFor information on encryption with Amazon EMR, see Encryption Options.\\\\nAmazon EventBridge', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='For information on encryption with Amazon EMR, see Encryption Options.\\\\nAmazon EventBridge\\\\nAmazon EventBridge (formerly Amazon CloudWatch Events) is a serverless event bus that enables \\\\nyou to create scalable event-driven applications. EventBridge delivers a stream of real- time data \\\\nfrom event sources, such as Zendesk, Datadog, or Pagerduty, and routes that data to targets like \\\\nAWS Lambda.\\\\nBy default, EventBridge encrypts data using 256-bit Advanced Encryption Standard (AES-256)\\\\nunder an AWS owned CMK, which helps secure customer data from unauthorized access. Customers \\\\nAmazon OpenSearch Service 25', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nshould ensure that any AWS resource emitting an event that is storing, processing, or transmitting \\\\nPHI is conﬁgured in accordance with best practices.\\\\nAmazon EventBridge is integrated with AWS CloudTrail and customers can view the most \\\\nrecent events in the CloudTrail console in Event history. For more information, see EventBridge \\\\nInformation in CloudTrail.\\\\nAmazon Forecast\\\\nAmazon Forecast is a fully managed service that uses machine learning to deliver highly accurate \\\\nforecasts. Based on the same machine learning forecasting technology used by Amazon.com. \\\\nEvery interaction customers have with Amazon Forecast is protected by encryption. Any content \\\\nprocessed by Amazon Forecast is encrypted with customer keys through Amazon Key Management \\\\nService, and encrypted at-rest in the AWS Region where customers are using the service.\\\\nAmazon Forecast is integrated with AWS CloudTrail, a service that provides a record of actions', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='Amazon Forecast is integrated with AWS CloudTrail, a service that provides a record of actions \\\\ntaken by a user, role, or an AWS service in Amazon Forecast. CloudTrail captures all API calls for \\\\nAmazon Forecast as events. The calls captured include calls from the Amazon Forecast console and \\\\ncode calls to the Amazon Forecast API operations. If customers create a trail, customers can enable \\\\ncontinuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon \\\\nForecast. For more information, see Logging Forecast API Calls with AWS CloudTrail.\\\\nBy default, the log ﬁles delivered by CloudTrail to their bucket are encrypted by Amazon server-\\\\nside encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer \\\\nthat is directly manageable, customers can instead use server-side encryption with AWS KMS–\\\\nmanaged keys (SSE-KMS) for their CloudTrail log ﬁles. Enabling server-side encryption encrypts', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='managed keys (SSE-KMS) for their CloudTrail log ﬁles. Enabling server-side encryption encrypts \\\\nthe log ﬁles but not the digest ﬁles with SSE-KMS. Digest ﬁles are encrypted with Amazon S3-\\\\nmanaged encryption keys (SSE-S3).\\\\nAWS Forecast imports and exports data to/from S3 buckets. When importing and exporting data \\\\nfrom Amazon S3, customers should ensure S3 buckets are conﬁgured in a manner consistent with \\\\nthe guidance. For more information, see Getting Started.\\\\nAmazon FSx\\\\nAmazon FSx is a fully-managed service providing feature-rich and highly-performant ﬁle systems. \\\\nAmazon FSx for Windows File Server provides highly reliable and scalable ﬁle storage and is \\\\naccessible over the Server Message Block (SMB) protocol. Amazon FSx for Lustre provides high-\\\\nAmazon Forecast 26', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content=\\\"Architecting for HIPAA Security and Compliance on Amazon Web Services AWS Whitepaper\\\\nperformance storage for compute workloads and is powered by Lustre, the world's most popular \\\\nhigh-performance ﬁle system.\\\\nAmazon FSx supports two forms of encryption for ﬁle systems, encryption of data in transit and \\\\nencryption at rest. Amazon FSx for Windows File Server also supports logging of all API calls using \\\\nAWS CloudTrail.\\\\nEncryption of data in transit is supported by Amazon FSx for Windows File Server on compute \\\\ninstances supporting SMB protocol 3.0 or newer, and by Amazon FSx for Lustre on Amazon EC2 \\\\ninstances that support encryption in transit. Alternatively, customers may encrypt data before \\\\nstoring on Amazon FSx but are then responsible for the encryption process and key management.\\\\nEncryption of data at rest is automatically enabled when creating an Amazon FSx ﬁle system, using \\\\nAES-256 encryption algorithm and AWS KMS-managed keys. Data and metadata are automatically\\\", metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'}), Document(page_content='AES-256 encryption algorithm and AWS KMS-managed keys. Data and metadata are automatically \\\\nencrypted before being written to the ﬁle system, and automatically decrypted before being \\\\npresented to the application. PHI should not be used in any ﬁle or folder name.\\\\nAmazon GuardDuty\\\\nAmazon GuardDuty is a managed threat detection service that continuously monitors for malicious \\\\nor unauthorized behavior to help customers protect their AWS accounts and workloads. It monitors \\\\nfor activity such as unusual API calls or potentially unauthorized deployments that indicate a \\\\npossible account compromise. Amazon GuardDuty also detects potentially compromised instances \\\\nor reconnaissance by attackers.\\\\nAmazon GuardDuty continuously monitors and analyzes the following data sources: VPC Flow Logs, \\\\nAWS CloudTrail event logs, and DNS logs. It uses threat intelligence feeds, such as lists of malicious \\\\nIPs and domains, and machine learning to identify unexpected and potentially unauthorized and', metadata={'year': 2024, 'source': 'AWS-security-whitepaper.pdf'})]```\\nBULLET POINT SUMMARY:\\n\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\u001b[0m,\n",
       "  \u001b[1;34m\"temperature\"\u001b[0m: \u001b[1;36m0.5\u001b[0m,\n",
       "  \u001b[1;34m\"top_p\"\u001b[0m: \u001b[1;36m0.9\u001b[0m,\n",
       "  \u001b[1;34m\"max_gen_len\"\u001b[0m: \u001b[1;36m2048\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Here is a bullet point summary of the text:\\n\\n* Amazon CloudWatch Events delivers a near-real-time stream of system events that describe changes in AWS resources.\\n* Customers should ensure that PHI does not flow into CloudWatch Events and that any AWS resource emitting a CloudWatch event that is storing, processing, or transmitting PHI is configured in accordance with the Guidance.\\n* Amazon CloudWatch Logs can be used to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources.\\n* Log data is encrypted while in transit and at rest, and customers do not need to re-encrypt PHI emitted by any other service and delivered to CloudWatch Logs.\\n* Amazon Comprehend uses natural language processing to extract insights about the content of documents and can be used with data containing PHI.\\n* Amazon Comprehend processes any text file in UTF-8 format and develops insights by recognizing entities, key phrases, language, sentiments, and other common elements in a document.\\n* Amazon Comprehend does not retain or store any data, and all calls to the API are encrypted with SSL/TLS.\\n* Amazon Comprehend uses CloudTrail to log all API calls.\\n* AWS Identity and Access Management (IAM) can be used to control access to Amazon Comprehend and credentials can be used to access the IAM.\\n* Customers can use IAM policies to grant users permissions for specific resources and actions.\\n* Amazon Comprehend can be used with AWS Lake Formation to create a data warehouse and data lake.\\n* Customers can use AWS Lake Formation to create a data catalog and manage access to data.\\n* Amazon Comprehend can be used with AWS Glue to create a data pipeline and manage data.\\n* Customers can use AWS Glue to create a data catalog and manage access to data.\\n* Amazon Comprehend can be used with Amazon S3 to store and manage data.\\n* Customers can use Amazon S3 to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon DynamoDB to store and manage data.\\n* Customers can use Amazon DynamoDB to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Block Store (EBS) to store and manage data.\\n* Customers can use Amazon EBS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic File System (EFS) to store and manage data.\\n* Customers can use Amazon EFS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Container Registry (ECR) to store and manage data.\\n* Customers can use Amazon ECR to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Container Service (ECS) to store and manage data.\\n* Customers can use Amazon ECS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Kubernetes Service (EKS) to store and manage data.\\n* Customers can use Amazon EKS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon ElastiCache to store and manage data.\\n* Customers can use Amazon ElastiCache to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon OpenSearch Service to store and manage data.\\n* Customers can use Amazon OpenSearch Service to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon EMR to store and manage data.\\n* Customers can use Amazon EMR to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon EventBridge to store and manage data.\\n* Customers can use Amazon EventBridge to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Forecast to store and manage data.\\n* Customers can use Amazon Forecast to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon FSx to store and manage data.\\n* Customers can use Amazon FSx to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon GuardDuty to store and manage data.\\n* Customers can use Amazon GuardDuty to store and manage data, and can use IAM policies to control access to data.\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"prompt_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9727</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">984</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"stop_reason\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"stop\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"generation\"\u001b[0m: \u001b[32m\"Here is a bullet point summary of the text:\\n\\n* Amazon CloudWatch Events delivers a near-real-time stream of system events that describe changes in AWS resources.\\n* Customers should ensure that PHI does not flow into CloudWatch Events and that any AWS resource emitting a CloudWatch event that is storing, processing, or transmitting PHI is configured in accordance with the Guidance.\\n* Amazon CloudWatch Logs can be used to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources.\\n* Log data is encrypted while in transit and at rest, and customers do not need to re-encrypt PHI emitted by any other service and delivered to CloudWatch Logs.\\n* Amazon Comprehend uses natural language processing to extract insights about the content of documents and can be used with data containing PHI.\\n* Amazon Comprehend processes any text file in UTF-8 format and develops insights by recognizing entities, key phrases, language, sentiments, and other common elements in a document.\\n* Amazon Comprehend does not retain or store any data, and all calls to the API are encrypted with SSL/TLS.\\n* Amazon Comprehend uses CloudTrail to log all API calls.\\n* AWS Identity and Access Management (IAM) can be used to control access to Amazon Comprehend and credentials can be used to access the IAM.\\n* Customers can use IAM policies to grant users permissions for specific resources and actions.\\n* Amazon Comprehend can be used with AWS Lake Formation to create a data warehouse and data lake.\\n* Customers can use AWS Lake Formation to create a data catalog and manage access to data.\\n* Amazon Comprehend can be used with AWS Glue to create a data pipeline and manage data.\\n* Customers can use AWS Glue to create a data catalog and manage access to data.\\n* Amazon Comprehend can be used with Amazon S3 to store and manage data.\\n* Customers can use Amazon S3 to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon DynamoDB to store and manage data.\\n* Customers can use Amazon DynamoDB to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Block Store (EBS) to store and manage data.\\n* Customers can use Amazon EBS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic File System (EFS) to store and manage data.\\n* Customers can use Amazon EFS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Container Registry (ECR) to store and manage data.\\n* Customers can use Amazon ECR to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Container Service (ECS) to store and manage data.\\n* Customers can use Amazon ECS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Elastic Kubernetes Service (EKS) to store and manage data.\\n* Customers can use Amazon EKS to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon ElastiCache to store and manage data.\\n* Customers can use Amazon ElastiCache to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon OpenSearch Service to store and manage data.\\n* Customers can use Amazon OpenSearch Service to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon EMR to store and manage data.\\n* Customers can use Amazon EMR to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon EventBridge to store and manage data.\\n* Customers can use Amazon EventBridge to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon Forecast to store and manage data.\\n* Customers can use Amazon Forecast to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon FSx to store and manage data.\\n* Customers can use Amazon FSx to store and manage data, and can use IAM policies to control access to data.\\n* Amazon Comprehend can be used with Amazon GuardDuty to store and manage data.\\n* Customers can use Amazon GuardDuty to store and manage data, and can use IAM policies to control access to data.\"\u001b[0m,\n",
       "  \u001b[1;34m\"prompt_token_count\"\u001b[0m: \u001b[1;36m9727\u001b[0m,\n",
       "  \u001b[1;34m\"generation_token_count\"\u001b[0m: \u001b[1;36m984\u001b[0m,\n",
       "  \u001b[1;34m\"stop_reason\"\u001b[0m: \u001b[32m\"stop\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Util method to print output.\n",
    "def print_json(data):\n",
    "    rich.print_json(json.dumps(data))\n",
    "\n",
    "# Print the request and response objects\n",
    "print_json(native_request)\n",
    "print_json(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19dff7-0514-4dfb-a412-e5239ade0f66",
   "metadata": {},
   "source": [
    "As you can see, for the data provided, the token count (```prompt_token_count``` **+** ```generation_token_count```) is about **10,711 tokens** which is larger than Llama 3 context window (**8192 tokens**). However, due to the increased context window limit of Llama 3.1, you can directly use the native invoke method to get summary results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41056234-520e-441f-abee-13aa85443bf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want. It is the default way to process documents with an LLM.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "    \n",
    "stuff_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a detailed summary of the following text delimited by triple backquotes.\\nReturn your response in bullet points which covers the key points of the text.\\n```{text}```\\nBULLET POINT SUMMARY:\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed summary of the following text delimited by triple backquotes.\\nReturn your response in bullet points which covers the key points of the text.\\n```{text}```\\nBULLET POINT SUMMARY:\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template  # Set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 ms, sys: 142 μs, total: 15.4 ms\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We will get an ERROR in Llama 3.\n",
    "# With Llama 3.1 this cell might take 1-2 minutes to run. But no error due to\n",
    "# the increaesd (128k) context window size.\n",
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.invoke(hipaa_docs[50:97])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f7ae9d5-ca3e-45a1-bac8-15b8b99aff16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a bullet point summary of the text:\n",
      "\n",
      "**Amazon CloudWatch Events**\n",
      "\n",
      "* Delivers a near-real-time stream of system events that describe changes in AWS resources\n",
      "* Customers should ensure PHI does not flow into CloudWatch Events\n",
      "* Can be configured to register as an AWS API call in CloudTrail\n",
      "\n",
      "**Amazon CloudWatch Logs**\n",
      "\n",
      "* Allows customers to monitor, store, and access log files from various sources\n",
      "* Log data is encrypted while in transit and at rest\n",
      "* Customers can retrieve log data from CloudWatch Logs\n",
      "\n",
      "**Amazon Comprehend**\n",
      "\n",
      "* Uses natural language processing to extract insights from documents\n",
      "* Can process data containing PHI\n",
      "* Does not retain or store any data, and all API calls are encrypted with SSL/TLS\n",
      "* Uses CloudTrail to log all API calls\n",
      "\n",
      "**Identity and Access Management**\n",
      "\n",
      "* Customers can use IAM to create a user with attached policies to enable Amazon Comprehend permissions\n",
      "* Can use multi-factor authentication to require users to authenticate to AWS\n",
      "* Can create a customer-managed policy to deny all permissions except those required for users to manage their own credentials and MFA devices\n",
      "\n",
      "**Data Protection and Secrets Management**\n",
      "\n",
      "* Customers are responsible for maintaining control over their content hosted on AWS infrastructure\n",
      "* Amazon Comprehend works with AWS Key Management Service (AWS KMS) to provide enhanced encryption for data\n",
      "* Customers can encrypt Amazon S3 buckets used for input documents using available S3 encryption solutions\n",
      "\n",
      "**Network Segmentation and Hardening**\n",
      "\n",
      "* Amazon Comprehend uses a variety of security measures to ensure the safety of data\n",
      "* Customers can create a virtual private cloud (VPC) to control access to their data\n",
      "* Can use VPC flow logs to monitor all network traffic in and out of job containers\n",
      "\n",
      "**Cross-Service Confused Deputy Prevention**\n",
      "\n",
      "* Amazon provides tools to help protect data for all services with service principals that have been given access to resources in a customer's account\n",
      "* Customers should consider safeguards to address this security issue\n",
      "\n",
      "**Other AWS Services**\n",
      "\n",
      "* Amazon Connect: customers should not include PHI in any fields associated with managing users, security profiles, and contact flows\n",
      "* Amazon DocumentDB: offers encryption at-rest during cluster creation via AWS KMS\n",
      "* Amazon DynamoDB: offers encryption at-rest using AWS KMS\n",
      "* Amazon Elastic Block Store: offers encryption at-rest consistent with the Guidance\n",
      "* Amazon Elastic Compute Cloud: customers can choose to integrate their applications with AWS KMS SDKs for key management and storage\n",
      "* Amazon Elastic Container Registry: no special steps are required to use with container images that contain PHI\n",
      "* Amazon Elastic Container Service: no additional configuration is required to use with workloads that process PHI\n",
      "* Amazon Elastic File System: supports encryption at-rest when a new file system is created\n",
      "* Amazon Elastic Kubernetes Service: customers should refer to the Architecting for HIPAA Security and Compliance on Amazon EKS whitepaper\n",
      "* Amazon ElastiCache: supports storing PHI for current generation node types and Redis engine versions\n",
      "* Amazon OpenSearch Service: customers should use OpenSearch or Elasticsearch 6.0 or later and ensure PHI is encrypted at-rest and in-transit\n",
      "* Amazon EMR: customers should refer to the Encryption Options documentation\n",
      "* Amazon EventBridge: customers should ensure any AWS resource emitting an event that is storing, processing, or transmitting PHI is configured in accordance with best practices\n",
      "* Amazon Forecast: customers should ensure S3 buckets are configured in a manner consistent with the guidance\n",
      "* Amazon FSx: supports two forms of encryption for file systems, encryption of data in transit and encryption at rest\n",
      "* Amazon GuardDuty: continuously monitors and analyzes data sources to help customers protect their AWS accounts and workloads\n"
     ]
    }
   ],
   "source": [
    "print(stuff_hipaa_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we don't get an error despite the context length being longer than 8192 tokens. Refer to the [Llama 3 notebook](https://github.com/aws-samples/Meta-Llama-on-AWS/blob/main/long-text-summarization/Llama3-Long-Document-summarization-LangChain.ipynb), where you will receive an error on this cell. \n",
    "\n",
    "Due to the larger context window, you can also bypass LangChain completely, and use the native invoke method, but as seen above, there are latency benefits whilst using LangChain.\n",
    "\n",
    "However, note that the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary. Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240a8bd-693c-4152-a186-850fdf1e4f59",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. Model generates individual summaries for all document chunks in parallel\n",
    "4. Reduce all these summaries to a condensed final summary\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/llama3mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes\n",
    "# this to an LLMChain. It then combines and iteratively reduces the mapped \n",
    "# document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                                chain_type=\"map_reduce\",\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "# Sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "# Sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 71)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 417 μs, total: 189 ms\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This cell might take 3-5 minutes to run on Llama 3.\n",
    "# This cell might take 2-3 minutes to run on Llama 3.1.\n",
    "\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:71])\n",
    "    # map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:97])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed summary of the text in bullet points:\n",
      "\n",
      "**Amazon CloudWatch Events**\n",
      "\n",
      "* Delivers a near-real-time stream of system events that describe changes in AWS resources.\n",
      "* Customers should ensure that Protected Health Information (PHI) does not flow into CloudWatch Events.\n",
      "* AWS resources emitting CloudWatch events that store, process, or transmit PHI must be configured according to the Guidance.\n",
      "\n",
      "**Amazon CloudWatch Logs**\n",
      "\n",
      "* Allows customers to monitor, store, and access log files from various sources.\n",
      "* Log data is stored and can be accessed through CloudWatch Logs.\n",
      "* Log data is encrypted while in transit and at rest.\n",
      "\n",
      "**Amazon Comprehend**\n",
      "\n",
      "* Uses natural language processing to extract insights from documents.\n",
      "* Supports text files in UTF-8 format.\n",
      "* Can be used with data containing Protected Health Information (PHI).\n",
      "* Does not retain or store any data.\n",
      "* All API calls are encrypted with SSL/TLS.\n",
      "* Uses CloudTrail to log all API calls.\n",
      "\n",
      "**Security and Access Management**\n",
      "\n",
      "* Security access functions (authentication and authorization) are required to access Amazon Comprehend.\n",
      "* Access can be controlled with AWS Identity and Access Management (IAM).\n",
      "* Credentials can be used to access IAM.\n",
      "* Default permissions: IAM users do not have permission to create or modify Amazon Comprehend resources or perform tasks using the Amazon Comprehend API.\n",
      "\n",
      "**Access Control**\n",
      "\n",
      "* Amazon Comprehend supports organization-defined role-based access and least privilege principles.\n",
      "* Authentication: users can be required to authenticate to AWS using multi-factor authentication (MFA) in accordance with organizational requirements.\n",
      "* Identity Management: IAM administrators can create a customer-managed policy to control user access.\n",
      "\n",
      "**Data Protection and Secrets Management**\n",
      "\n",
      "* Customers are responsible for protecting and managing their data and secrets in the AWS Cloud and Amazon Comprehend.\n",
      "* Data protection and secrets management are key aspects of the shared responsibility model in Amazon Comprehend.\n",
      "\n",
      "**Encryption**\n",
      "\n",
      "* Amazon Comprehend works with AWS Key Management Service (AWS KMS) to provide enhanced encryption for data.\n",
      "* Encryption of data-at-rest is possible through integration with AWS KMS.\n",
      "\n",
      "**Network Segmentation and Hardening**\n",
      "\n",
      "* Amazon Comprehend adheres to AWS Best Practices for Security, Identity, and Compliance.\n",
      "* Network security safeguards can be found in the Infrastructure Security section of the Amazon Comprehend Developer Guide.\n",
      "\n",
      "**VPC Configuration**\n",
      "\n",
      "* When creating a job, you specify your VPC configuration by selecting subnets and security groups.\n",
      "* Amazon Comprehend creates Elastic Network Interfaces (ENIs) associated with your security groups in one of the specified subnets.\n",
      "\n",
      "**Security and Compliance**\n",
      "\n",
      "* An external Identity Provider (IdP) is being used for federation, and tenant identity provider administrators should be limited in their abilities.\n",
      "* Host and image hardening should be implemented to enhance security.\n",
      "\n",
      "**The Confused Deputy Problem**\n",
      "\n",
      "* A multi-tenancy security issue where an entity without permission can coerce a more-privileged entity to perform an action.\n",
      "* AWS provides tools to help protect data and prevent the confused deputy problem for all services with service principals that have access to resources in an account.\n",
      "\n",
      "**Amazon Services**\n",
      "\n",
      "* Amazon Comprehend Medical: No specific guidance is provided, but users are referred to the previous Amazon Comprehend section.\n",
      "* Amazon Connect: Customers should not include Protected Health Information (PHI) in any fields associated with managing users, security profiles, or contact flows.\n",
      "* Amazon DocumentDB: Offers encryption at-rest during cluster creation via AWS Key Management Service (KMS).\n",
      "\n",
      "**Data Encryption and Security**\n",
      "\n",
      "* Avoid using PHI data in domain and object names, as the key identifiers are not encrypted.\n",
      "* Amazon DocumentDB stores data at-rest in an encrypted format, consistent with the Guidance in effect at the time of publication.\n",
      "\n",
      "**Logging and Monitoring**\n",
      "\n",
      "* Amazon DocumentDB uses AWS CloudTrail to log all API calls, and provides logging and monitoring capabilities as described in the \"Logging and Monitoring in Amazon DocumentDB\" resource.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary['output_text'].strip())\n",
    "# map_reduce_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "Llama 3.1 shows slight latency improvement over Llama 3 owing to its larger context window.\n",
    "\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method is a technique that allows us to recursively summarize our input data. It iteratively updates its answer by looping over the input documents. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce`. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. First chunk is sent to the model; Model returns the corresponding summary\n",
    "4. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have final summary that has been recursively updated using all the document chunks\n",
    "\n",
    "---\n",
    "\n",
    "![refine](imgs/llamarefine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "# llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Run an initial prompt on a small chunk of data to generate a summary.\n",
    "# Then, for each subsequent document, the output from the previous document is\n",
    "# passed in along with the new document, and the LLM is asked to refine the\n",
    "# output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                            chain_type=\"refine\",\n",
    "                                            verbose=False)\n",
    "\n",
    "# Refine summary chain for summarization\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm,\n",
    "                                                   chain_type=\"refine\",\n",
    "                                                   verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "# Refine llm chain prompt template\n",
    "refine_documents_prompt = PromptTemplate.from_template(\n",
    "    \"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 19.6 ms, total: 140 ms\n",
      "Wall time: 8min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This cell might take 8-10 minutes to run on Llama 3.\n",
    "# This cell might take 6-8 minutes to run on Llama 3.1.\n",
    "\n",
    "try:\n",
    "    refine_summary = refine_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I have refined the original summary to incorporate the new information about Amazon DocumentDB (with MongoDB compatibility) and additional security recommendations. Here is the refined summary:\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "* Amazon CloudWatch Events provides a near-real-time stream of system events related to AWS resources.\n",
      "* Customers should ensure that Protected Health Information (PHI) is not sent to CloudWatch Events.\n",
      "* AWS resources emitting CloudWatch events that involve PHI must be configured according to the Guidance.\n",
      "* Amazon Comprehend can be used with data containing PHI, and all API calls are encrypted with SSL/TLS.\n",
      "* Amazon Connect is a self-service, cloud-based contact center service that enables dynamic, personal, and natural customer engagement at any scale, and customers should not include any PHI in any fields associated with managing users, security profiles, and contact flows within Amazon Connect.\n",
      "* Amazon DocumentDB (with MongoDB compatibility) offers encryption at-rest during cluster creation via AWS KMS, which allows customers to encrypt databases using AWS or customer-managed keys.\n",
      "\n",
      "**Important Details:**\n",
      "\n",
      "* CloudWatch Events can be configured to register as an AWS API call in CloudTrail.\n",
      "* Amazon CloudWatch Logs can be used to monitor, store, and access log files from various sources, including EC2 instances, CloudTrail, and Route 53.\n",
      "* Log data can be retrieved from CloudWatch Logs, and since it is encrypted in transit and at rest, it is not necessary to re-encrypt PHI emitted by other services and delivered to CloudWatch Logs.\n",
      "* Amazon Comprehend uses CloudTrail to log all API calls, and security access functions can be controlled with AWS Identity and Access Management (IAM).\n",
      "* IAM policies can be used to grant users permissions to create or modify Amazon Comprehend resources and perform tasks, and custom policies can be created to attach to roles for administrators with Amazon Comprehend administration privileges.\n",
      "* Amazon Comprehend supports organization-defined role-based access and least privilege principles, allowing for fine-grained control over access to resources and data.\n",
      "* Multi-factor authentication can be required for users to authenticate to AWS, and IAM administrators can create custom policies to deny all permissions except those required for users to manage their own credentials and MFA devices.\n",
      "* Amazon Comprehend recommends using identity-based policies to grant permissions to perform operations on resources, and supports authorizing access to pre-defined IAM policies, customer IAM policies, and API actions to users or roles based on least privilege and role-based organizational requirements.\n",
      "* External authentication is also supported through identity federation using IAM roles, which enables users to authenticate to AWS by assuming a role provisioned by administrators.\n",
      "* AWS support for Kerberos and Active Directory provides the benefits of single sign-on and centralized authentication of database users, allowing users to manage and store user credentials in AWS Directory Service for Microsoft Active Directory or in the customer on-premises Active Directory.\n",
      "* Customers are responsible for controlling the flow of data inputs and outputs for Amazon Comprehend using IAM policies, and are also responsible for any personal data they put in the AWS Cloud and Amazon Comprehend.\n",
      "* The AWS shared responsibility model applies to data protection in Amazon Comprehend, where AWS is responsible for protecting the global infrastructure, and customers are responsible for maintaining control over their content hosted on this infrastructure.\n",
      "* Amazon Comprehend provides enhanced encryption for data at rest using AWS Key Management Service (AWS KMS), and integrates with Amazon Simple Storage Service (Amazon S3) to encrypt input documents.\n",
      "* Customers should consider using TLS for transmission and avoid placing sensitive information into tags or free-form fields to protect data in Amazon Comprehend.\n",
      "* Amazon Comprehend integrates with AWS KMS to encrypt data in storage volumes for start* and create* jobs, and encrypts output results of start* jobs using a customer's own AWS KMS key.\n",
      "* It is a best practice to encrypt Amazon S3 buckets used for input documents using available S3 encryption solutions in accordance with organizational policies.\n",
      "* The AWS Management Console encrypts Amazon Comprehend custom models with its own AWS KMS key, while the AWS CLI can encrypt custom models using either its own AWS KMS key or a provided customer-managed key (CMK).\n",
      "* When using the AWS Management Console, customers can choose to encrypt data using volume encryption, which ensures that data on an EBS Volume used by Comprehend is encrypted during training/inference.\n",
      "* Amazon Comprehend also provides output result encryption, which enables customers to encrypt the output stored in their bucket using a customer-provided AWS KMS key.\n",
      "* Additionally, Amazon Comprehend can detect personally identifiable information (PII) in English text documents using the console or APIs.\n",
      "* Customers using Amazon S3 and managing their own AWS KMS keys should consider revoking AWS KMS keys and defining the procedural justification to do so in accordance with their organizational requirements, as revocation of the AWS KMS key for Amazon S3 renders any data unusable/unreadable.\n",
      "* Amazon Comprehend adheres to the AWS Best Practices for Security, Identity, and Compliance, and recommends using network segmentation and hardening, including protecting jobs using an Amazon Virtual Private Cloud (Amazon VPC) to control access to data and containers, and monitor network traffic using VPC flow logs.\n",
      "* When creating a job, customers can specify their VPC configuration by selecting subnets and security groups, which allows Amazon Comprehend to create elastic network interfaces (ENIs) associated with the security groups in one of the subnets, enabling job containers to connect to resources in the VPC.\n",
      "* For jobs, customers can only configure subnets with a default tenancy VPC in which their instance runs on shared hardware.\n",
      "* Customers can establish a private connection between their VPC and Amazon Comprehend by creating an interface VPC endpoint.\n",
      "* Host and image hardening of the AWS environment for Amazon Comprehend is managed by AWS as a provided service.\n",
      "* To help make recommendations more secure, customers should implement multi-tenancy security recommendations, including:\n",
      "\t+ Using only a verified email address to authorize user access to a tenant based on domain match.\n",
      "\t+ Using immutable or mutable attributes for the user profile attributes that identify tenants, and giving app clients read-only access to the attributes.\n",
      "\t+ Using 1:1 mapping between external IdP and application client to prevent unauthorized cross-tenant access.\n",
      "\t+ Restricting users so that they can't modify the criteria that authorize user access to the tenants.\n",
      "* When using external IdP for federation, restrict tenant identity provider administrators so that they can't modify user access.\n",
      "* To prevent cross-service confused deputy problems, customers should consider implementing safeguards such as using service principals, IAM roles, and resource-based permissions to limit the permissions of services and prevent unauthorized access to resources.\n",
      "* Amazon Connect Customer Profiles, a feature of Amazon Connect, equips contact center agents with a unified view of a customer's profile with the most up-to-date information, to provide more personalized customer service. Customers should refrain from naming domains or object keys with PHI data.\n",
      "* Amazon DocumentDB (with MongoDB compatibility) offers encryption at-rest during cluster creation via AWS KMS, which allows customers to encrypt databases using AWS or customer-managed keys. On a database instance running with encryption enabled, data is encrypted and protected.\n",
      "* Connections to Amazon DocumentDB containing PHI must use endpoints that accept encrypted transport (HTTPS). By default, a newly created Amazon DocumentDB cluster only accepts secure connections using Transport Layer Security (TLS).\n",
      "* Amazon DocumentDB uses AWS CloudTrail to log all API calls.\n",
      "\n",
      "The refined summary incorporates the new information about Amazon DocumentDB (with MongoDB compatibility) and provides additional security recommendations for multi-tenancy and cross-service confused deputy prevention.\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "Llama 3.1 shows slight latency improvement over Llama 3 owing to its larger context window.\n",
    "\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent document chunks in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f979c-4d80-408d-9799-20043d9fbe0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we looked at three different summarization techniques using LangChain; **Stuff**, **Map_Reduce**, and **Refine**. Each of these methods has its own distinct advantages/uses. \n",
    "\n",
    "- ***Stuff*** is straighforward and is the fastest method out of the three since it makes a single call to the LLM and fits the entire document within the model's context window. Although as we saw with the HIPAA Compliance document, it does not scale well to work with large volumes of text.\n",
    "\n",
    "- ***Map_Reduce*** deals with the issue of the context window length while being able to parallelize generation of summaries for individual chunks, thereby speeding up the model's response while being able to process long documents. An issue with Map_Reduce is that since this is not a recursive process, we lose context between chunks during this process.\n",
    "\n",
    "- ***Refine*** deals with the issues that arise with the previous methods. It performs recursive summarization by incrementally generating summaries for each of the chunks while retaining context between them. While this method generates the most accurate and comprehensive summary out of all 3 methods, the calls made to the LLM cannot be parallelized. This can result in longer processing times. Additionally, more recent document chunks tend to carry more weight due to the order that they are processed in.\n",
    "\n",
    "We saw, for summarization, due to the larger context window of Llama 3.1 you can use native ```invoke``` method instead of using one the ```load_summarize_chain``` (e.g., stuff). This way you can completely avoid LangChain, however, there are latency benefits and better control whilst using LangChain's ```load_summarize_chain``` with its options (namely, stuff, map_reduce, refine).\n",
    "\n",
    "We can see both Llama 3.1 and Llama 3 work well with the Map Reduce and Refine methods, both of which are meant to work arond the context window limitation while using a large document as input. We do see slight latency improvement with Llama 3.1 on the same tasks owning to its larger context window (128k for Llama 3.1 as compared to 8k for Llama 3). This is just one aspect of comparison. For your usecase consider the approriate chunking strategy based on a holistic analysis considering all aspects as well. \n",
    "\n",
    "We also saw that given the direct support to 8 languages, it also allows you to prompt and get results in multiple languages. This can help you build powerful user experiences with multilingual chat interfaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
