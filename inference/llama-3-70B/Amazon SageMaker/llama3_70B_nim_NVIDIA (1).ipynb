{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy Llama 3 70B with NVIDIA NIM on Amazon SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "## What is NIM\n",
    "\n",
    "[NVIDIA NIM](https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x7b-instruct-v01) enables efficient deployment of large language models (LLMs) across various environments, including cloud, data centers, and workstations. It simplifies self-hosting LLMs by providing scalable, high-performance microservices optimized for NVIDIA GPUs. NIM's containerized approach allows for easy integration into existing workflows, with support for advanced language models and enterprise-grade security. Leveraging GPU acceleration, NIM offers fast inference capabilities and flexible deployment options, empowering developers to build powerful AI applications such as chatbots, content generators, and translation services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b92143-a092-4799-a492-8e0ee0908176",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "NIM abstracts away model inference internals such as execution engine and runtime operations. They are also the most performant option available whether it be with [TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm), and others. NIM offers the following high performance features:\n",
    "\n",
    "1. Scalable Deployment that is performant and can easily and seamlessly scale from a few users to millions.\n",
    "2. Advanced Language Model support with pre-generated optimized engines for a diverse range of cutting edge LLM architectures.\n",
    "3. Flexible Integration to easily incorporate the microservice into existing workflows and applications. Developers are provided with an OpenAI API compatible programming model and custom NVIDIA extensions for additional functionality.\n",
    "4. Enterprise-Grade Security emphasizes security by using safetensors, constantly monitoring and patching CVEs in our stack and conducting internal penetration tests.\n",
    "\n",
    "Here is a link to the [NIM Support Matrix](https://docs.nvidia.com/nim/large-language-models/latest/support-matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007649f-5837-4575-a7a9-a2b6c730f5a0",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "NIMs are packaged as container images on a per model/model family basis. Each NIM is its own Docker container with a model, such as llama3. These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations are optimized. These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations are optimized. In this sample, we will be using the [NVIDIA NIM public ECR gallery on AWS](https://gallery.ecr.aws/nvidia/nim)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `Llama3 70B` on a `p4d.24xlarge` instance with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d95e7-4bcc-4c83-90bd-1c492c67aa24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Card\n",
    "---\n",
    "### Llama 3 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. \n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935d62c-0675-4a62-9e46-d9e854491a26",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599941-1c76-4352-b1c3-eca6f4a65aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b>  To run NIM on SageMaker you will need to have your `NGC API KEY` to access NGC resources. Check out <a href=\"https://build.nvidia.com/meta/llama3-70b?signin=true\"> this LINK</a> to learn how to get an NGC API KEY. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd7912-b1cb-478f-b344-ceb2f0bfb770",
   "metadata": {},
   "source": [
    "##### 1. Setup and retrieve API key:\n",
    "\n",
    "1. First you will need to sign into [NGC](9https://ngc.nvidia.com/signin) with your NVIDIA account and password.\n",
    "2. Navigate to setup.\n",
    "3. Select “Get API Key”.\n",
    "4. Generate your API key.\n",
    "5. Keep your API key secret and in a safe place. Do not share it or store it in a place where others can see or copy it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "For more information on NIM, check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb4d59-c7ec-45de-9018-8c05052625ce",
   "metadata": {},
   "source": [
    "##### 2. You must have the appropriate push permissions associated with your execution role\n",
    "- Copy and paste the following json inline policy to your `Amazon SageMaker Execution Role` :\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"imagebuilder:GetComponent\",\n",
    "                \"imagebuilder:GetContainerRecipe\",\n",
    "                \"ecr:GetAuthorizationToken\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:GetDownloadUrlForLayer\",\n",
    "                \"ecr:PutImage\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kms:Decrypt\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"ForAnyValue:StringEquals\": {\n",
    "                    \"kms:EncryptionContextKeys\": \"aws:imagebuilder:arn\",\n",
    "                    \"aws:CalledVia\": [\n",
    "                        \"imagebuilder.amazonaws.com\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::ec2imagebuilder*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/imagebuilder/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "- Or add the `EC2InstanceProfileForImageBuilderECRContainerBuilds` permission policy to your `SageMaker Execution Role`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ea72c-6da0-4236-8975-f70b616ceaa2",
   "metadata": {},
   "source": [
    "##### 3. NIM public ECR image is currently available only in `us-east-1` region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbe75a-3b45-4ee6-814f-329c3b9d783b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4. This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Llama3 70B`, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `p4d.24xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a31e80-0a37-4e1a-a031-66c81244eb00",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acad648-c837-40bc-b2e3-94016333ea2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import json\n",
    "import os\n",
    "import sagemaker\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7acbd-edad-4e3e-9386-1e587879b2a5",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee757",
   "metadata": {},
   "source": [
    "In this example, since we are deploying `Llama3 70B` we define some configurations below for retrieving our ECR image for NIM along with some other requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f64791-1c45-4b84-9b1a-1e3ebc60d2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-3-70b\n",
    "public_nim_image = \"public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\"\n",
    "nim_model = \"nim-llama3-70b-instruct\"\n",
    "sm_model_name = \"nim-llama3-70b-instruct\"\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "payload_model = \"meta/llama3-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05462",
   "metadata": {},
   "source": [
    "### NIM Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851abe8-9ca4-403b-be7f-aef56dfa4b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first pull the NIM image from public ECR and then push it to private ECR repo within your account for deploying on SageMaker endpoint. \n",
    "\n",
    "Note, as mentioned previously:\n",
    "  - NIM ECR image is currently available only in `us-east-1` region\n",
    "  - You must have `ecr:InitiateLayerUpload` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944110e5-15bc-4a94-a731-7d4ea9344e9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error getting AWS account ID: {result.stderr}\")\n",
    "else:\n",
    "    account = result.stdout.strip()\n",
    "    print(f\"AWS account ID: {account}\")\n",
    "\n",
    "bash_script = f\"\"\"\n",
    "echo \"Public NIM Image: {public_nim_image}\"\n",
    "docker pull {public_nim_image}\n",
    "\n",
    "\n",
    "echo \"Resolved account: {account}\"\n",
    "echo \"Resolved region: {region}\"\n",
    "\n",
    "nim_image=\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "\n",
    "# Ensure the repository name adheres to AWS constraints\n",
    "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "docker tag {public_nim_image} $nim_image\n",
    "docker push $nim_image\n",
    "echo -n $nim_image\n",
    "\"\"\"\n",
    "nim_image=f\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "# Run the bash script and capture real-time output\n",
    "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == b'' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.decode().strip())\n",
    "\n",
    "stderr = process.stderr.read().decode()\n",
    "if stderr:\n",
    "    print(\"Errors:\", stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863fd4-0893-4022-9ab0-38e1af1512d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We print the private ECR NIM image in your account that we will be using for SageMaker deployment. \n",
    "- Should be similar to  `\"<ACCOUNT ID>.dkr.ecr.<REGION>.amazonaws.com/<NIM_MODEL>:latest\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb63ac-2a7a-4beb-b0dd-77a4473e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(nim_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8451e1-5683-4e88-995c-3a0250c99e39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9efc86-0cf2-403b-9502-fd294acb4cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before proceeding further, please set your NGC API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1f264-ebd8-4c6a-9926-7a21afd89ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your NGC API key here\n",
    "NGC_API_KEY = \"SET YOUR NGC API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9b93a-3fdf-49a4-8f89-494238008a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable. It's required for running NIM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5bce6-3807-43c3-866f-80543cfdedbf",
   "metadata": {},
   "source": [
    "We define the sagemaker model from the NIM container making sure to pass in **NGC_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image,\n",
    "    \"Environment\": {\"NGC_API_KEY\": NGC_API_KEY}\n",
    "}\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2f0e6-0377-4a13-9cc3-c345adf08c86",
   "metadata": {},
   "source": [
    "Next we create endpoint configuration, here we are deploying the Llama model on the specified instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 850\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sm_model_name\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "## Test Inference and Streaming Inference with Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36583-d6b0-4fdf-a659-c088f913034a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> The model's name in the inference request payload needs to be the name of the NIM model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail how Optimum Neuron helps compile LLMs for AWS infrastructure\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1112217-3d99-409c-8329-b8e86c17782b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail what inference engines and llm serving frameworks are\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d24a-a23e-40aa-afdb-da2a63f624fe",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05742b6-f8e4-4116-92ed-e9b156b6cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "---\n",
    "### Delete endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f467d8-a9ff-49d7-9425-37e83e54cd19",
   "metadata": {},
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
