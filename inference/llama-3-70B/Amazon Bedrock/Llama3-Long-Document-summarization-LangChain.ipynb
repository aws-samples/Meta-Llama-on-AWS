{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Document Summarization with Llama3 on Bedrock with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate using the [Llama3 family of models](https://aws.amazon.com/about-aws/whats-new/2024/04/meta-llama-3-foundation-models-aws/) on Amazon Bedrock for abstract document summarization tasks. Although the Llama3 8B and 70B models are powerful and versatile language models that can be used for a wide range of natural language processing tasks, they have relatively small context window sizes compared to other models in the class. As a result, when working with multiple large documents there are several challenges that can arise. One of the main challenges is that the input text might exceed the model's context length of 8k tokens. This limitation can lead to incomplete or inaccurate responses, as the model may not have access to all the relevant information within the document. Another challenge is that language models can sometimes hallucinate or generate factually incorrect responses when dealing with very long documents. This can happen because the model may lose track of the overall context or make incorrect inferences based on partial information. Additionally, processing large documents can lead to out-of-memory errors, especially on resource-constrained systems or when working with large language models that have high memory requirements.\n",
    "\n",
    "To address these challenges, this notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "## Llama 3 Model Selection\n",
    "\n",
    "Today, there are two Llama 3 models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Llama 3 8B\n",
    "\n",
    "- **Description:** Ideal for limited computational power and resources, faster training times, and edge devices.\n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Text Classification, and Sentiment Analysis.\n",
    "\n",
    "### 2. Llama 3 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. \n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Llama 3 8B | 68.4%      | \\$0.0004                   | \\$0.0006                    |\n",
    "| Llama 3 70B | 82.0%      | \\$0.00265                   | \\$0.0035                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Llama 3 8B Model Cards and Prompt Formats](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3==1.34.58\n",
    "botocore==1.34.101\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {},
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', \n",
    "                       region_name='us-west-2',\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Ensure that you have access to the Llama3 model you wish to use through Bedrock in the selected region.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Meta Llama3 70b/8b in `model_id` and pass the Llama3 inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "### Supported parameters\n",
    "\n",
    "The Llama models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"max_gen_len\": int\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired Llama3 model\n",
    "llama3_70b_instruct = \"meta.llama3-70b-instruct-v1:0\"\n",
    "llama3_8b_instruct = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = llama3_70b_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_gen_len\": 2048,  \n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! It\\'s lovely to chat with you! I\\'ve been running on a Linux-based operating system, specifically Ubuntu 20.04, and I\\'ve been trained on a massive dataset of text from the internet, which I like to call my \"knowledge graph.\" It\\'s a vast network of interconnected concepts, entities, and relationships that I can draw upon to answer your questions. By the way, did you know that I\\'ve been designed to learn and improve over time, so the more conversations I have, the more accurate and informative I become? What\\'s on your mind today?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize conversation chain \n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the execution of the conversation chain. This can be set to true when you're debugging your conversation chain or trying to understand how it's working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory() \n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8307f-d16d-427a-b9e7-d7deac5cdb05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {},
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "In this example to demonstrate summarization, we will be using a document that is a whitepaper from AWS. \n",
    "\n",
    "> The document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS.\n",
    "\n",
    "\n",
    "Let's first download the file to build our document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AWS Whitepaper\\nArchitecting for HIPAA Security and \\nCompliance on Amazon Web Services\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.' metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the HIPAA Security document: 237\n"
     ]
    }
   ],
   "source": [
    "#document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "#chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a712e-99d8-403d-85a2-cfa048ceaabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {},
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {},
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want. It is the default way to process documents with an LLM.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.invoke(hipaa_docs) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we see that an error is raised due to the prompt far exceeding the model's maximum context length. Since stuffing summarizes text by feeding the entire document to a large language model (LLM) in a single call, it is difficult to process long documents. The Llama models have a context length of 8k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html) to be able to mitigate the restrictions of your large documents not fitting into the context window of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. Model generates individual summaries for all document chunks in parallel\n",
    "4. Reduce all these summaries to a condensed final summary\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/llama3mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain, it then combines and iteratively reduces the mapped document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "#sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "#sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt= PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:71])  \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed summary of the text in bullet points, covering the key points:\n",
      "\n",
      "**Amazon CloudWatch**\n",
      "\n",
      "* Delivers a near-real-time stream of system events that describe changes in AWS resources\n",
      "* Ensure PHI does not flow into CloudWatch Events, and configure AWS resources emitting CloudWatch events according to the Guidance\n",
      "* Configure Amazon CloudWatch Events to register as an AWS API call in CloudTrail\n",
      "\n",
      "**Amazon CloudWatch Logs**\n",
      "\n",
      "* Allows customers to monitor, store, and access log files from various sources\n",
      "* Log data can be retrieved from CloudWatch Logs\n",
      "* Log data is encrypted while in transit and at rest, ensuring its security\n",
      "* No re-encryption is needed for PHI emitted by other services and delivered to CloudWatch Logs\n",
      "\n",
      "**Amazon Comprehend**\n",
      "\n",
      "* Uses natural language processing to extract insights from documents\n",
      "* Processes text files in UTF-8 format\n",
      "* Develops insights by recognizing entities, key phrases, language, sentiments, and other common elements in a document\n",
      "* Can be used with data containing Protected Health Information (PHI)\n",
      "* Does not retain or store any data\n",
      "* All API calls are encrypted with SSL/TLS\n",
      "* Uses CloudTrail to log all API calls\n",
      "\n",
      "**AWS Identity and Access Management (IAM)**\n",
      "\n",
      "* Controls security access functions such as authentication and authorization for Amazon Comprehend\n",
      "* Credentials can be used to access IAM\n",
      "* See the Amazon Comprehend User Guide for more information on authentication and access control for Amazon Comprehend\n",
      "\n",
      "**Summary of Control for Amazon Comprehend**\n",
      "\n",
      "* By default, IAM users do not have permission to create or modify Amazon Comprehend resources or perform tasks using the Amazon Comprehend API\n",
      "* Granting permissions allows users to create or modify resources and perform tasks\n",
      "* Amazon Comprehend can be used with AWS Identity and Access Management (IAM) to create a user with an attached policy to enable Amazon Comprehend permissions\n",
      "* Custom policies can be created to attach to a role, allowing administrators to invoke API's for Amazon Comprehend administration\n",
      "* Role-Based Access and least privilege principles can be implemented using Amazon Comprehend\n",
      "\n",
      "**Multi-Factor Authentication (MFA)**\n",
      "\n",
      "* Optionally, you can use compatible third-party MFA capabilities with IAM partners for added security\n",
      "* Administration recommends using identity-based policies in AWS IAM\n",
      "* API Actions can be found in the API Reference guide\n",
      "* Authorization involves authorizing access to pre-defined IAM policies, customer IAM policies, and API actions to users or roles based on their least privilege and role-based organizational requirements\n",
      "\n",
      "**External Authentication**\n",
      "\n",
      "* Amazon Comprehend is compatible with identity federation using IAM roles\n",
      "* This allows users to authenticate to AWS by assuming a role provisioned by administrators\n",
      "* Users can access AWS using credentials from their organization or a third-party, assuming a role indirectly\n",
      "* AWS supports Kerberos and Active Directory, providing single sign-on and centralized authentication of database users\n",
      "\n",
      "**Data Flow Enforcement**\n",
      "\n",
      "* AWS customers and APN partners are responsible for personal data in the AWS Cloud and Amazon Comprehend\n",
      "* Users are responsible for controlling data inputs and outputs for Amazon Comprehend using IAM policies\n",
      "\n",
      "**Data Protection and Secrets Management**\n",
      "\n",
      "* The AWS shared responsibility model applies to data protection in Amazon Comprehend\n",
      "* According to this model, AWS is responsible for protecting the global infrastructure that runs all of the AWS Cloud\n",
      "* The customer is responsible for maintaining control over their content that is hosted on this infrastructure\n",
      "\n",
      "**Comprehend Job Encryption**\n",
      "\n",
      "* Integration with AWS KMS enables encryption of data in storage volume for start and create jobs\n",
      "* Output results of start jobs are encrypted using a customer's own AWS KMS key\n",
      "\n",
      "**Best Practices for Amazon S3 Buckets**\n",
      "\n",
      "* Encrypt Amazon S3 buckets used for input documents using available S3 encryption solutions\n",
      "* Follow organizational policies for encryption\n",
      "\n",
      "**Custom Model Encryption**\n",
      "\n",
      "* AWS Management Console encrypts custom models with its own AWS KMS key\n",
      "* AWS CLI can encrypt custom models using either its own AWS KMS key or a provided customer managed key (CMK)\n",
      "\n",
      "**Optional Encryption Methods**\n",
      "\n",
      "* Volume encryption ensures data on an EBS Volume used by Comprehend is encrypted during training/inference\n",
      "* Data is flushed after training/inference, so the key is only relevant while the job is in progress\n",
      "\n",
      "**HIPAA Security and Compliance on Amazon Web Services**\n",
      "\n",
      "* Amazon Comprehend can analyze personally identifiable information (PII) and entities\n",
      "* Data Deletion involves revoking AWS KMS keys and defining the procedural justification to do so, as revocation renders data unusable/unreadable\n",
      "* Network Segmentation and Hardening involve implementing network security safeguards and using an Amazon Virtual Private Cloud (Amazon VPC) to protect jobs\n",
      "\n",
      "**Additional Security Measures**\n",
      "\n",
      "* Amazon Comprehend uses security measures to protect data while it's being used in job containers\n",
      "* Job containers access AWS resources over the internet\n",
      "* Creating a virtual private cloud (VPC) helps protect data by allowing you to configure it to not be connected to the internet, and also enables monitoring of all network traffic in and out of job containers using VPC flow logs\n",
      "\n",
      "**VPC Tenancy and Private Connection**\n",
      "\n",
      "* Only default tenancy VPCs are allowed for configuring subnets for jobs, which means instances run on shared hardware\n",
      "* A private connection can be established between a VPC and Amazon Comprehend using an interface VPC endpoint (AWS PrivateLink)\n",
      "\n",
      "**Host and Image Hardening**\n",
      "\n",
      "* AWS is responsible for host and image hardening of the AWS environment for Amazon Comprehend, as per the AWS shared responsibility model\n",
      "\n",
      "**Multi-Tenancy Security**\n",
      "\n",
      "* Implement the following to enhance security:\n",
      "\t+ Verify email addresses to authorize user access to a tenant based on domain match\n",
      "\t+ Use secure user profile attributes\n",
      "\t+ Establish a 1:1 mapping between external IdP and application client\n",
      "\t+ Restrict user modifications to prevent unauthorized access\n",
      "\n",
      "**Tenant Authorization**\n",
      "\n",
      "* Restrict tenant identity provider administrators to prevent them from modifying user access\n",
      "* This restriction is in place to ensure that only authorized personnel can manage user access and prevent unauthorized changes that could compromise security\n",
      "\n",
      "**Security Risks**\n",
      "\n",
      "* The Confused Deputy Problem: a multi-tenancy security issue where an entity without permission can coerce a more-privileged entity to perform an action\n",
      "* Cross-Service Impersonation: a scenario in AWS where one service calls another service, potentially leading to the confused deputy problem\n",
      "* Prevention measures: AWS provides tools to protect data across all services with service principals that have been given access to resources in an account\n",
      "\n",
      "**Amazon Comprehend and HIPAA Compliance**\n",
      "\n",
      "* Amazon Comprehend Medical guidance is the same as for Amazon Comprehend, with no additional specific recommendations provided\n",
      "* Do not include Protected Health Information (PHI) in any fields associated with managing users, security profiles, and contact flows within Amazon Connect\n",
      "\n",
      "**Data Protection**\n",
      "\n",
      "* Do not include PHI data in domain or object key names, as while the contents of domains and objects are encrypted and protected, the key identifiers are not\n",
      "* Amazon DocumentDB (with MongoDB compatibility) offers encryption at-rest during cluster creation using AWS Key Management Service (KMS)\n",
      "\n",
      "**HIPAA Security and Compliance on Amazon Web Services**\n",
      "\n",
      "* Data Encryption:\n",
      "\t+ Data stored at-rest is encrypted\n",
      "\t+ Automated backups, read replicas, and snapshots are also encrypted\n",
      "\t+ Customers should evaluate and determine whether Amazon DocumentDB encryption meets their compliance and regulatory requirements\n",
      "* Encryption in Transit:\n",
      "\t+ Connections to Amazon DocumentDB containing PHI must use endpoints that accept encrypted transport (HTTPS)\n",
      "\t+ Newly created Amazon DocumentDB clusters only accept secure connections using Transport Layer Security (TLS) by default\n",
      "* Logging and Monitoring:\n",
      "\t+ Amazon DocumentDB uses AWS CloudTrail to log all API calls\n",
      "\t+ For more information, see Logging and Monitoring in Amazon DocumentDB\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {},
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method is a technique that allows us to recursively summarize our input data. It iteratively updates its answer by looping over the input documents. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce`. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. First chunk is sent to the model; Model returns the corresponding summary\n",
    "4. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have final summary that has been recursively updated using all the document chunks\n",
    "\n",
    "---\n",
    "\n",
    "![refine](imgs/llamarefine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run an initial prompt on a small chunk of data to generate a summary. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False) #refine summary chain for summarization in french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "#refine llm chain prompt template\n",
    "refine_documents_prompt= PromptTemplate.from_template(\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    refine_summary = refine_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the additional context provided, I have refined the original summary to incorporate the new information. Here is the revised summary:\n",
      "\n",
      "The text discusses the use of Amazon CloudWatch and Amazon Comprehend in relation to Protected Health Information (PHI). The main points are:\n",
      "\n",
      "* Amazon CloudWatch Events delivers a near-real-time stream of system events that describe changes in AWS resources, and customers should ensure that PHI does not flow into CloudWatch Events.\n",
      "* Customers should configure AWS resources emitting CloudWatch events that store, process, or transmit PHI in accordance with the Guidance.\n",
      "* CloudWatch Events can be configured to register as an AWS API call in CloudTrail, and customers can find more information on how to do this in the \"Creating a CloudWatch Events Rule That Triggers on an AWS API Call Using AWS CloudTrail\" section.\n",
      "* Amazon CloudWatch Logs can be used to monitor, store, and access log files from various AWS services, including Amazon EC2 instances, AWS CloudTrail, and Amazon Route 53. Log data in CloudWatch Logs is encrypted while in transit and at rest, eliminating the need to re-encrypt PHI emitted by other services.\n",
      "* Amazon Comprehend can be used with data containing PHI, and it processes text files in UTF-8 format to extract insights. Amazon Comprehend does not retain or store any data, and all API calls are encrypted with SSL/TLS. Access to Amazon Comprehend can be controlled using AWS Identity and Access Management (IAM), which allows customers to create users with attached policies to enable Amazon Comprehend permissions, or create custom policies to attach to a role, ensuring organization-defined role-based access and least privilege principles.\n",
      "* Additionally, Amazon Comprehend supports multi-factor authentication, allowing users to authenticate to AWS using MFA in accordance with their organizational requirements. IAM administrators can create a customer-managed policy that denies all permissions except those required for users to manage their own credentials and MFA devices.\n",
      "* When using Amazon Comprehend, administrators should select identity-based policies and attach permissions policies to IAM identities (users, groups, and roles) to grant permissions to perform operations on Amazon Comprehend resources. It is also recommended to authorize access to pre-defined IAM policies, customer IAM policies, and API actions to users or roles in accordance with their least privilege and role-based organizational requirements.\n",
      "* Amazon Comprehend is compatible with identity federation using IAM roles, enabling users to authenticate to AWS by assuming a role that administrators have provisioned.\n",
      "* AWS provides support for Kerberos and Active Directory, offering single sign-on and centralized authentication of database users. Users can choose to manage and store user credentials either in AWS Directory Service for Microsoft Active Directory or in the customer on-premises Active Directory.\n",
      "* Customers are responsible for controlling the flow of data inputs and outputs for Amazon Comprehend using IAM policies, and for protecting personal data in the AWS Cloud and Amazon Comprehend.\n",
      "* The AWS shared responsibility model applies to data protection in Amazon Comprehend, where AWS is responsible for protecting the global infrastructure, and customers are responsible for maintaining control over their content hosted on this infrastructure.\n",
      "* Amazon Comprehend provides enhanced encryption for data-at-rest through integration with AWS Key Management Service (AWS KMS), and customers can encrypt their input documents when creating a text analysis, topic modeling, or custom Amazon Comprehend job using Amazon Simple Storage Service (Amazon S3).\n",
      "* For Amazon Comprehend jobs, integration with AWS KMS enables encryption of data in the storage volume for start and create jobs, and encrypts the output results of start jobs using the customer's own AWS KMS key.\n",
      "* It is a best practice for Amazon Comprehend users to encrypt Amazon S3 buckets used for input documents using available S3 encryption solutions in accordance with their organizational policies.\n",
      "* The AWS Management Console encrypts Amazon Comprehend custom models with its own AWS KMS key, while the AWS CLI can encrypt custom models using either its own AWS KMS key or a provided customer-managed key (CMK).\n",
      "* When using the AWS Management Console, customers can choose to encrypt custom models using volume encryption, which ensures that data on an EBS Volume used by Comprehend is encrypted during training/inference.\n",
      "* Amazon Comprehend also provides features for detecting personally identifiable information (PII) in English text documents, and supports output result encryption using a customer-provided AWS KMS key.\n",
      "* Additionally, customers should consider revoking AWS KMS keys and defining the procedural justification to do so in accordance with their organizational requirements, as revocation of the AWS KMS key for Amazon S3 renders any data unusable/unreadable.\n",
      "* Amazon Comprehend adheres to the AWS Best Practices for Security, Identity, and Compliance, and provides recommended network security safeguards, such as using an Amazon Virtual Private Cloud (Amazon VPC) to protect jobs. This includes creating a VPC and configuring it to ensure that data and containers are not accessible over the internet, and using VPC flow logs to monitor network traffic in and out of job containers.\n",
      "* Furthermore, Amazon Customer Profiles provides a unified customer profile by automatically bringing together customer information from multiple applications, delivering the profile directly to the agent as soon as the support call or interaction begins. Customers should refrain from naming domains or object keys with PHI data, as the key identifiers are not encrypted.\n",
      "* Additionally, Amazon DocumentDB (with MongoDB compatibility) offers encryption at-rest during cluster creation via AWS KMS, allowing customers to encrypt databases using AWS or customer-managed keys. On a database instance running with encryption enabled, data is encrypted. Connections to Amazon DocumentDB containing PHI must use endpoints that accept encrypted transport (HTTPS), and Amazon DocumentDB uses AWS CloudTrail to log all API calls.\n",
      "\n",
      "The revised summary incorporates the additional context provided, highlighting the importance of ensuring PHI does not flow into CloudWatch Events, configuring AWS resources in accordance with the Guidance, and securely using Amazon Comprehend with PHI data, including the use of IAM to control access and manage permissions, as well as support for multi-factor authentication, identity federation, and data protection and secrets management.\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent document chunks in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f979c-4d80-408d-9799-20043d9fbe0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully looked at three different summarization techniques using LangChain; **Stuff**, **Map_Reduce**, and **Refine**. Each of these methods has its own distinct advantages/uses. \n",
    "\n",
    "- ***Stuff*** is straighforward and is the fastest method out of the three since it makes a single call to the LLM and fits the entire document within the model's context window. Although as we saw with the HIPAA Compliance document, it does not scale well to work with large volumes of text.\n",
    "\n",
    "- ***Map_Reduce*** deals with the issue of the context window length while being able to parallelize generation of summaries for individual chunks, thereby speeding up the model's response while being able to process long documents. An issue with Map_Reduce is that since this is not a recursive process, we lose context between chunks during this process.\n",
    "\n",
    "- ***Refine*** deals with the issues that arise with the previous methods. It performs recursive summarization by incrementally generating summaries for each of the chunks while retaining context between them. While this method generates the most accurate and comprehensive summary out of all 3 methods, the calls made to the LLM cannot be parallelized. This can result in longer processing times. Additionally, more recent document chunks tend to carry more weight due to the order that they are processed in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
