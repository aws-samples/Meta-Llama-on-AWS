{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f1e07e-5f03-4ae1-aa92-b0d8698edc73",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3.2 11b Vision Model for Bedrock with SageMaker\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to fine-tune the Llama 3.2 11b vision model using Amazon SageMaker and deploy it to Amazon Bedrock. This process involves preparing the dataset, setting up the training environment, fine-tuning the model, and finally deploying it for inference.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load and prepare the dataset for fine-tuning\n",
    "2. Set up the SageMaker training environment\n",
    "3. Fine-tune the Llama 3.2 11b vision model\n",
    "4. Deploy the fine-tuned model to Amazon Bedrock\n",
    "5. Test the deployed model with sample queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81efa9",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up our SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Data processing and visualization\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# SageMaker-specific imports\n",
    "from sagemaker.jumpstart.types import JumpStartSerializablePayload\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Custom modules\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"utils\"] = utils\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "from utils import download_artifacts, remove_field_from_json, upload_artifacts, cleanup_local_files, wait_for_model_availability, test_image_processing\n",
    "from iam_role_helper import create_or_update_role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c269c7",
   "metadata": {},
   "source": [
    "This code sets up the SageMaker session and retrieves the necessary IAM role for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    role = iam.get_role(RoleName='role with required priviledges')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sess.boto_region_name)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe73b9",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Dataset Description: HuggingFaceM4/DocumentVQA\n",
    "The HuggingFaceM4/DocumentVQA dataset is a comprehensive collection designed for document visual question answering tasks. It contains a diverse set of document images along with corresponding questions and answers. This dataset is particularly useful for training models to understand and extract information from various types of documents, including forms, receipts, and other structured text documents.\n",
    "\n",
    "Key features of the dataset:\n",
    "\n",
    "- Document images in various formats and layouts\n",
    "- Question-answer pairs related to the content of each document\n",
    "- Diverse document types, including forms, invoices, and printed text\n",
    "- Suitable for training models in tasks such as information extraction, document understanding, and visual question answering\n",
    "\n",
    "By using this dataset, we aim to fine-tune our Llama 3.2 11b vision model to excel in document-based visual question answering tasks.\n",
    "\n",
    "Now, let's load and prepare our dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ffb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"HuggingFaceM4/DocumentVQA\"\n",
    "data = load_dataset(\n",
    "    dataset_name, cache_dir=\"./\"\n",
    ")\n",
    "\n",
    "# Function to process data\n",
    "def process_data(data, output_dir, num_ex):\n",
    "    local_data_file = f\"{output_dir}/metadata.jsonl\"\n",
    "    with open(local_data_file, \"w\") as f:\n",
    "        for i in tqdm(range(num_ex)):\n",
    "            each = data[i]\n",
    "            q = each[\"question\"]\n",
    "            each_img = each[\"image\"]\n",
    "            a = each[\"answers\"][0]\n",
    "\n",
    "            example = {\"file_name\": f\"images/img_{i}.jpg\", \"prompt\": q, \"completion\": a}\n",
    "            json.dump(example, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            each_img.save(f\"{output_dir}/images/img_{i}.jpg\")\n",
    "\n",
    "# Process train and validation data\n",
    "for split, num in [(\"train\", 1000), (\"validation\", 20)]:\n",
    "    os.makedirs(f\"docvqa/{split}\", exist_ok=True)\n",
    "    os.makedirs(f\"docvqa/{split}/images\", exist_ok=True)\n",
    "    process_data(data=data[split], output_dir=f\"./docvqa/{split}\", num_ex=num)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed616e",
   "metadata": {},
   "source": [
    "This code loads the DocumentVQA dataset and processes it into a format suitable for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a61fa-f07c-43e9-92ef-7b2b5a8ccf76",
   "metadata": {},
   "source": [
    "### Upload the dataset to the S3\n",
    "\n",
    "Given the dataset contains image, the uploading process will take a while depending on the size of examples you process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_dir = \"./docvqa/train/\"\n",
    "train_data_location = f\"s3://{sagemaker_session_bucket}/docvqa-1000-20\"\n",
    "S3Uploader.upload(local_data_dir, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e81a5f",
   "metadata": {},
   "source": [
    "This step uploads the processed dataset to the specified S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7b0d9-7a69-4b67-aff6-1b400779799c",
   "metadata": {},
   "source": [
    "## Model and Hyperparameter Configuration\n",
    "Now we'll set up our model configuration and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabdbb2-7c9a-4f62-ba17-aa05a562df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set model ID and version\n",
    "model_id, model_version = \"meta-vlm-llama-3-2-11b-vision-instruct\", \"*\"\n",
    "\n",
    "# Retrieve default hyperparameters\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "# Set number of epochs\n",
    "my_hyperparameters[\"epoch\"] = \"1\"\n",
    "\n",
    "# Validate hyperparameters\n",
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b45a4d",
   "metadata": {},
   "source": [
    "This code sets up the model configuration and hyperparameters for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee57c6",
   "metadata": {},
   "source": [
    "### Fine-tuning the Model\n",
    "With our data and configuration ready, we can now start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171491d-a8a2-4fa7-8b00-d570400f84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.p4de.24xlarge\",\n",
    "    role=role,\n",
    "    hyperparameters=my_hyperparameters,\n",
    ")\n",
    "# Start fine-tuning\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125443b7",
   "metadata": {},
   "source": [
    "This code initiates the fine-tuning process using the specified model and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b4def",
   "metadata": {},
   "source": [
    "## Import SageMaker Model in Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b0809",
   "metadata": {},
   "source": [
    "### Model Preparation\n",
    "After fine-tuning our model using SageMaker, we need to prepare it for import into Amazon Bedrock. This process involves downloading the model artifacts, modifying the tokenizer configuration, and uploading the modified files to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training job name and model URI\n",
    "training_job_name = estimator._current_job_name\n",
    "model_uri = estimator.model_data['S3DataSource']['S3Uri']\n",
    "\n",
    "# Download the model artifacts\n",
    "local_path = download_artifacts(\n",
    "    training_job_name=training_job_name,\n",
    "    model_uri=model_uri\n",
    ")\n",
    "\n",
    "# Remove the 'processor_class' field from the tokenizer config\n",
    "file_path = f\"{local_path}/tokenizer_config.json\"\n",
    "field_to_remove = \"processor_class\"\n",
    "remove_field_from_json(file_path, field_to_remove)\n",
    "\n",
    "# Upload the modified artifacts to S3\n",
    "s3_uri = upload_artifacts(\n",
    "    local_dir=local_path,\n",
    "    sagemaker_session=sess,\n",
    "    training_job_name=training_job_name,\n",
    "    prefix='llama3-multi-model-artifacts'\n",
    ")\n",
    "\n",
    "print(f\"Artifacts uploaded to: {s3_uri}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "cleanup_local_files('tmp_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe8ed6",
   "metadata": {},
   "source": [
    "In this section, we perform the following steps:\n",
    "\n",
    "- Retrieve the training job name and model URI from the SageMaker estimator.\n",
    "- Download the model artifacts to a local directory.\n",
    "- Modify the tokenizer_config.json file by removing the 'processor_class' field, which is required for custom import in Bedrock.\n",
    "- Upload the modified artifacts to an S3 bucket, which will be used later in the Bedrock import process.\n",
    "- Clean up temporary local files to free up space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3ec71",
   "metadata": {},
   "source": [
    "### Create IAM Role for Bedrock Import\n",
    "To import our custom model into Bedrock, we need to create an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120eecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"\n",
    "\n",
    "# Define policies\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "            \"Resource\": [f\"arn:aws:s3:::{training_bucket}\", f\"arn:aws:s3:::{training_bucket}/*\"],\n",
    "            \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "bedrock_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=permission_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4d68a",
   "metadata": {},
   "source": [
    "This code creates or updates an IAM role that will be used by Bedrock to access the S3 bucket containing our model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2509f49",
   "metadata": {},
   "source": [
    "### Import Model into Bedrock\n",
    "Now that we have prepared our model and created the necessary IAM role, we can import the model into Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88473cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bedrock = boto3.client(service_name='bedrock',\n",
    "                       region_name=region)\n",
    "# Generate a uni\n",
    "timestamp = int(time.time())\n",
    "random_number = random.randint(1000, 9999)\n",
    "JOB_NAME = f\"meta3-import-model-{timestamp}-{random_number}\"\n",
    "\n",
    "ROLE_ARN = bedrock_role_arn\n",
    "IMPORTED_MODEL_NAME = f\"llama32_multimodal_{timestamp}-{random_number}\"\n",
    "S3_URI = s3_uri\n",
    "\n",
    "# createModelImportJob API\n",
    "create_job_response = bedrock.create_model_import_job(\n",
    "    jobName=JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=ROLE_ARN,\n",
    "    modelDataSource={\n",
    "        \"s3DataSource\": {\n",
    "            \"s3Uri\": s3_uri\n",
    "        }\n",
    "    },\n",
    ")\n",
    "job_arn = create_job_response.get(\"jobArn\")\n",
    "print(f\"Model import job created with ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5a0cd",
   "metadata": {},
   "source": [
    "This code initiates the process of importing our custom model into Bedrock. The import job is created with a unique name, and we specify the S3 location of our model artifacts and the IAM role to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0f59b",
   "metadata": {},
   "source": [
    "### Monitor Import Job Progress\n",
    "After initiating the import job, we need to monitor its progress to ensure successful completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_filter = IMPORTED_MODEL_NAME  # Replace with your model name\n",
    "model_info = wait_for_model_availability(model_name_filter,max_attempts=30,delay=60)\n",
    "#\n",
    "if model_info:\n",
    "    model_arn=model_info[\"modelArn\"]\n",
    "    print(\"Model is now available in Bedrock.\")\n",
    "else:\n",
    "    print(\"Failed to find the model in Bedrock within the specified attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103eb82",
   "metadata": {},
   "source": [
    "This function periodically checks the status of our imported model in Bedrock, waiting for it to become available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29465983",
   "metadata": {},
   "source": [
    "## Testing the Deployed Model in Bedrock\n",
    "Now that we have imported our custom model into Bedrock, we can test it by making a simple query and verifying that the model is available for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5adfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_ID= model_arn\n",
    "\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100, \n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "message = \"Hello, what it is the weather in seattle?\"\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "br_runtime = session.client(service_name = 'bedrock-runtime', \n",
    "                                 region_name=region, \n",
    "                                 config=config)\n",
    "    \n",
    "try:\n",
    "    invoke_response = br_runtime.invoke_model(modelId=MODEL_ID, \n",
    "                                            body=json.dumps({'prompt': message}), \n",
    "                                            accept=\"application/json\", \n",
    "                                            contentType=\"application/json\")\n",
    "    invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    print(json.dumps(invoke_response, indent=4))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(e.__repr__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34931ec9",
   "metadata": {},
   "source": [
    "In this section, we perform the following steps:\n",
    "\n",
    "- Set the REGION_NAME variable to the AWS region where our Bedrock model is hosted, and assign the model_arn obtained earlier to the MODEL_ID variable.\n",
    "\n",
    "- Configure the Bedrock Runtime client to retry the API call up to 100 times (using the Config class) in case of any issues, such as network errors or temporary service unavailability.\n",
    "\n",
    "- Define a sample message, \"Hello, what it is the weather in seattle?\", to query the model.\n",
    "\n",
    "- Initialize the Bedrock Runtime client using the configured Config object and the specified region.\n",
    "\n",
    "- Invoke the model by calling the invoke_model method on the Bedrock Runtime client, passing the MODEL_ID, the sample message as the request body, and the expected content type and response format.\n",
    "\n",
    "- Parse the response by decoding the body field of the response and print the result as a formatted JSON.\n",
    "\n",
    "This code demonstrates how to make a simple query to the deployed Bedrock model and handle any exceptions that may occur during the process. By retrying the API call up to 100 times, we ensure that the model is available and responsive before proceeding with further testing or integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31daf4ed",
   "metadata": {},
   "source": [
    "## Evaluating Visual Multimodal Capabilities of Llama 3 in Bedrock\n",
    "\n",
    "In this section, we'll demonstrate the visual multimodal capabilities of our fine-tuned Llama 3 model in Bedrock. We'll use an image from our validation set and ask the model to describe what it sees.\n",
    "\n",
    "### Display the Test Image\n",
    "\n",
    "First, let's display the image we'll be using for our test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_path = \"./docvqa/validation/images/img_2.jpg\"\n",
    "# Open and display the image\n",
    "img = Image.open(\"./docvqa/validation/images/img_2.jpg\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e4840",
   "metadata": {},
   "source": [
    "This code will display the image in the notebook, allowing us to visually inspect what we're asking the model to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbbd32",
   "metadata": {},
   "source": [
    "### Prepare and Send the Request to Bedrock\n",
    "Now, we'll prepare our request and send it to the Bedrock model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Path to the image we want to analyze\n",
    "image_path = \"docvqa/validation/images/img_2.jpg\"\n",
    "\n",
    "MODEL_ID = model_arn\n",
    "\n",
    "# Print current working directory and absolute path\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Absolute image path: {os.path.abspath(image_path)}\")\n",
    "\n",
    "# Test the image processing (assuming test_image_processing function is defined)\n",
    "#encoded_image = test_image_processing(image_path)\n",
    "# Load JPEG image\n",
    "jpeg_image = Image.open(image_path)\n",
    "png_buffer = io.BytesIO()\n",
    "jpeg_image.save(png_buffer, format=\"PNG\")\n",
    "png_bytes = png_buffer.getvalue()\n",
    "\n",
    "#Use png_bytes in your messages structure\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"What can you see in this image?\"},\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": png_bytes},\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = br_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=messages,\n",
    "    )\n",
    "response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"###Response Output###\")\n",
    "print(response_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d61099-a7da-4d6e-93dd-f5132326d686",
   "metadata": {},
   "source": [
    "# Clean Up Resources\n",
    "\n",
    "After completing your experiments and evaluations, it's crucial to clean up the resources you've created to avoid ongoing charges. This section will guide you through the process of deleting all the resources used in this notebook.\n",
    "\n",
    "> ⚠️ **Warning:** The following steps will permanently delete resources. Make sure you've saved any important data or model artifacts before proceeding.\n",
    "\n",
    "### Delete the Bedrock Custom Model\n",
    "\n",
    "let's remove the custom model from Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc0c8f-8e4e-4a22-b502-91fa4f6c8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bedrock_custom_model(model_name):\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    try:\n",
    "        bedrock.delete_imported_model(modelIdentifier=model_name)\n",
    "        print(f\"Successfully deleted Bedrock custom model: {model_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        error_code = error.response['Error']['Code']\n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"Error deleting Bedrock custom model: The provided model name is invalid. Model Name: {model_name}\")\n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            print(f\"Error: The model '{model_name}' was not found in Bedrock.\")\n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            print(\"Error: You do not have permission to delete this model.\")\n",
    "        elif error_code == 'ConflictException':\n",
    "            print(\"Error: The model is currently in use or in a state that doesn't allow deletion.\")\n",
    "        else:\n",
    "            print(f\"Error deleting Bedrock custom model: {error}\")\n",
    "\n",
    "delete_bedrock_custom_model(IMPORTED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17ffca",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the process of fine-tuning the Llama 3.2 11b vision model using Amazon SageMaker and importing it into Amazon Bedrock as a custom model. We've covered the following key objectives:\n",
    "\n",
    "1. Loaded and prepared the HuggingFaceM4/DocumentVQA dataset for fine-tuning\n",
    "2. Set up the SageMaker training environment and configured hyperparameters\n",
    "3. Fine-tuned the Llama 3.2 11b vision model using SageMaker\n",
    "4. Prepared and imported the fine-tuned model into Amazon Bedrock\n",
    "5. Tested the deployed model with both text and image-based queries\n",
    "\n",
    "This workflow showcases the power of combining SageMaker's training capabilities with Bedrock's inference API, allowing for the creation of specialized, multi-modal AI models that can be easily integrated into various applications.\n",
    "\n",
    "Key takeaways from this notebook:\n",
    "\n",
    "- The HuggingFaceM4/DocumentVQA dataset provides a rich source of document images and related questions, enabling the model to learn document understanding tasks.\n",
    "- Fine-tuning a large language model like Llama 3.2 11b can be efficiently done using SageMaker's distributed training capabilities.\n",
    "- The process of importing a custom model into Bedrock involves several steps, including modifying the model artifacts and setting up the necessary IAM roles.\n",
    "- The imported model in Bedrock can handle both text-only and image-text queries, demonstrating its multi-modal capabilities.\n",
    "\n",
    "By following this notebook, you've learned how to create a powerful, custom vision-language model that can be used for a wide range of document understanding and visual question-answering tasks. This model can be further integrated into your applications using the Bedrock API, opening up possibilities for advanced document processing, information extraction, and more.\n",
    "\n",
    "Next steps:\n",
    "- Experiment with different datasets or combine multiple datasets to enhance the model's capabilities.\n",
    "- Explore advanced fine-tuning techniques such as parameter-efficient fine-tuning (PEFT) methods.\n",
    "- Integrate the custom Bedrock model into your applications and evaluate its performance on real-world tasks.\n",
    "- Consider optimizing the model for specific use cases by adjusting hyperparameters or using domain-specific data.\n",
    "\n",
    "Remember to clean up any resources you've created during this notebook to avoid unnecessary charges, and refer to the AWS documentation for best practices in managing and securing your AI models.\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
