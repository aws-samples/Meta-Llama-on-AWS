{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 for Amazon Bedrock using SageMaker Pipeline\n",
    "## Introduction\n",
    "This notebook demonstrates how to fine-tune a Llama 3 model using a SageMaker Pipeline and deploy it to Amazon Bedrock. It covers the entire process from data preparation to model deployment.\n",
    "This notebook demonstrates how to fine-tune a Llama 3 model using Amazon SageMaker and deploy it to Amazon Bedrock. It covers the entire machine learning workflow, including:\n",
    "\n",
    "- Data preparation and preprocessing\n",
    "- Model training using SageMaker Pipeline\n",
    "- Model registration\n",
    "- Deployment to Amazon Bedrock\n",
    "- Inference comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "The following diagram illustrates the end-to-end ML workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture Diagram](Llama3_finetuning_bedrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline processes, trains, and evaluates a model using HuggingFace containers, then registers it before deploying to Amazon Bedrock through a Lambda function for inference. Model artifacts are stored in S3 throughout the proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- An AWS account with appropriate permissions\n",
    "- SageMaker Studio or a SageMaker Notebook instance\n",
    "- Access to the Llama 3 model on Hugging Face (meta-llama/Llama-3.2-3B-Instruct)\n",
    "- Necessary Python libraries installed (transformers, sagemaker, boto3, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers  sagemaker seaborn sentence-transformers nltk scikit-learn \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import importlib.util\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CacheConfig, CreateModelStep\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "# Import custom modules\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"utils\"] = utils\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "# Import specific functions from custom modules if needed\n",
    "from iam_role_helper import create_lambda_execution_role, create_or_update_role,create_boto3_layer\n",
    "from utils import monitor_pipeline_execution, wait_for_model_availability, run_model_evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Hugging Face Credentials\n",
    "\n",
    "To access the Llama 3 model weights, you need to authenticate with Hugging Face. This step requires an API token and access to the `meta-llama/Llama-3.2-3B-Instruct` model.\n",
    "\n",
    "> ⚠️ **Important**: You must have explicit permission to access the Llama 3 model. If you don't have access, you'll need to request it from Meta AI.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Obtain a Hugging Face API token:\n",
    "   - Visit the [Hugging Face Access Tokens page](https://huggingface.co/settings/tokens)\n",
    "   - Create a new token or use an existing one\n",
    "   \n",
    "2. Ensure you have access to the Llama 3 model:\n",
    "   - Check your access [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B)\n",
    "   - If you don't have access, follow the instructions in this [discussion thread](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172)\n",
    "\n",
    "3. Set your Hugging Face API token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACE_TOKEN'] = 'your_huggingface_token_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Verify your access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "model_info = api.model_info(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "print(f\"You have access to: {model_info.modelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📘 Note: For more information on Hugging Face access tokens and security, refer to the [official documentation](https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker session and IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_execution_role = \"your_sagemaker_execution_role\"\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role)['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sess.boto_region_name)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Setup and Environment Configuration](#setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [SageMaker Pipeline Creation](#sagemaker-pipeline)\n",
    "4. [Deployment to Amazon Bedrock](#bedrock-deployment)\n",
    "6. [Pipeline Job Creation](#pipeline-execution)\n",
    "5. [Inference Analysis](#model-evaluation)\n",
    "6. [Resource Cleanup](#clean-up-resources)\n",
    "\n",
    "Follow along with this notebook to learn how to leverage AWS's machine learning ecosystem for fine-tuning and deploying custom language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this section, we'll prepare our dataset for fine-tuning the Llama 3 model. \n",
    "Proper data preparation is crucial for effective model training. We'll be:\n",
    "\n",
    "1. Downloading a pre-prepared dataset\n",
    "2. Uploading it to an S3 bucket for use in our SageMaker pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "\n",
    "We're using a shortened version of the [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1) (OpenAssistant Conversations Dataset) from Hugginface. \n",
    "This dataset has been specifically curated to focus on Question-Answer (QA) examples, \n",
    "making it ideal for fine-tuning our model for QA tasks. The dataset is currently \n",
    "stored in an Amazon S3 bucket for easy access and integration with our SageMaker pipeline.\n",
    "\n",
    "Using this pre-processed and focused dataset ensures that:\n",
    "1. Our data is relevant to the QA task we're training for\n",
    "2. The dataset size is manageable for this demonstration\n",
    "3. We can efficiently access the data through Amazon S3\n",
    "\n",
    "This preprocessing step saves time and computational resources while still providing \n",
    "a rich dataset for fine-tuning our Llama 3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_S3Uri=\"s3://jumpstart-cache-prod-us-west-2/training-datasets/oasst_top/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = S3Downloader.download(s3_uri=dataset_S3Uri, local_path=f\"dataset/\")\n",
    "print(f\"Training config downloaded to:\")\n",
    "print(train_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the processed data to S3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "# upload the model yaml file to s3\n",
    "train_dataset_path = \"dataset/train.jsonl\"\n",
    "train_s3_path = S3Uploader.upload(local_path=train_dataset_path, desired_s3_uri=f\"{input_path}/dataset\")\n",
    "\n",
    "print(f\"Training dataset uploaded to:\")\n",
    "print(train_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Pipeline\n",
    "## Why Use a SageMaker Pipeline?\n",
    "\n",
    "SageMaker Pipelines allow us to create reusable workflows for machine learning tasks. \n",
    "By defining our process as a pipeline, we gain several benefits:\n",
    "\n",
    "1. Reproducibility: The entire workflow can be easily recreated and rerun\n",
    "2. Automation: Steps are executed automatically in sequence\n",
    "3. Scalability: Pipeline can handle large-scale data processing and model training\n",
    "4. Versioning: Each run of the pipeline can be tracked and compared\n",
    "\n",
    "In this section, we'll define the steps of our pipeline, including data preprocessing, \n",
    "model training, and model registration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline parameters\n",
    "\n",
    "In this section, we set up essential parameters for our SageMaker pipeline. These parameters define:\n",
    "\n",
    "- The pipeline session for managing our workflow\n",
    "- AWS region and model naming for resource management\n",
    "- Compute resources for data preprocessing\n",
    "- Caching configuration to improve pipeline efficiency\n",
    "\n",
    "These settings help us optimize our pipeline's performance, ensure consistency across runs, and manage computational resources effectively. Adjusting these parameters allows us to fine-tune the pipeline for different scenarios or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession()\n",
    "\n",
    "# Define pipeline parameters\n",
    "region=sagemaker_session.boto_region_name\n",
    "model_name = \"llama3-qa-model\"\n",
    "instance_type_preprocessing = \"ml.m5.large\"\n",
    "instance_count = 1\n",
    "# Cache configuration to improve pipeline execution time\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessing step\n",
    "This section defines the preprocessing step of our SageMaker pipeline. Here's what we're doing:\n",
    "\n",
    "- Setting up a SKLearnProcessor to handle our data preprocessing\n",
    "- Configuring input and output paths for our data\n",
    "- Defining the preprocessing script location\n",
    "\n",
    "The preprocessing step is crucial for preparing our data before model training. It ensures our dataset is in the correct format and structure for the Llama 3 model fine-tuning process.\n",
    "\n",
    "By using SageMaker's built-in SKLearnProcessor, we leverage AWS-optimized containers for efficient and scalable data processing, streamlining our ML workflow.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    instance_type=instance_type_preprocessing,\n",
    "    instance_count=instance_count,\n",
    "    base_job_name=\"llama3-qa-preprocessing\",\n",
    "    role=role,\n",
    "    max_runtime_in_seconds=3600,  # Set a maximum runtime of 1 hour,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(source=train_s3_path, destination=\"/opt/ml/processing/input\"),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "    ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessQADataset\",\n",
    "    processor=preprocessing_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    \n",
    "    code=\"scripts/preprocessing/preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training step\n",
    "This section sets up the core training step of our SageMaker pipeline. Key aspects include:\n",
    "\n",
    "- Defining the training configuration using a YAML file\n",
    "- Setting up a HuggingFace estimator for training the Llama 3 model\n",
    "- Configuring compute resources, environment variables, and hyperparameters\n",
    "- Creating a TrainingStep that integrates with our pipeline\n",
    "\n",
    "The training step is where the actual fine-tuning of the Llama 3 model occurs. We're using SageMaker's integration with HuggingFace to simplify the process of training large language models.\n",
    "\n",
    "This configuration allows us to leverage distributed training techniques like FSDP (Fully Sharded Data Parallel) for efficient training of our large model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3_2_3B_fsdp_lora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Llama-3.2-3B-Instruct\"# Hugging Face model id\n",
    "max_seq_length:  512 #2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test\"   # path to where SageMaker saves test dataset\n",
    "#output_dir: \"/opt/ml/model\"            # path to where SageMaker will upload the model \n",
    "output_dir: \"/tmp/llama3\"            # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 10                   # number of training epochs\n",
    "per_device_train_batch_size: 16         # batch size per device during training\n",
    "per_device_eval_batch_size: 16          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: false                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the model yaml file to s3\n",
    "model_yaml = \"llama_3_2_3B_fsdp_lora.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name with timestamp\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "job_name = f'llama3-8B-exp1-{timestamp}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'training/train_fsdp_lora.py',      # train script\n",
    "    model_dir            = '/opt/ml/model',\n",
    "    source_dir           = 'scripts/',  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.48xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.16xlarge',  # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_2_3B_fsdp_lora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": os.environ['HUGGINGFACE_TOKEN'],       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    "    \n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=job_name,\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "        ),\n",
    "        \"config\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=train_config_s3_path,\n",
    "        ),\n",
    "        \"test\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model registration step\n",
    "This section focuses on registering our trained model in SageMaker. Key points include:\n",
    "\n",
    "- Setting up a HuggingFaceModel for deployment\n",
    "- Creating a CreateModelStep to integrate model creation into our pipeline\n",
    "- Configuring a RegisterModel step to add our model to the SageMaker Model Registry\n",
    "\n",
    "Model registration is crucial for version control and deployment management. It allows us to:\n",
    "- Track different versions of our fine-tuned Llama 3 model\n",
    "- Manage model approvals and transitions between stages (e.g., testing to production)\n",
    "- Simplify model deployment and updates in production environments\n",
    "\n",
    "By integrating this step into our pipeline, we ensure that each successful training run results in a properly registered and trackable model vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\",\n",
    "  region=region,\n",
    "  version=\"2.0\",\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model=HuggingFaceModel(\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py310\",\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model step\n",
    "llama_model_step = CreateModelStep(\n",
    "    name=\"CreateLlama3ModelStep\",\n",
    "    model=llm_model,\n",
    "    inputs=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    depends_on=[training_step],\n",
    ")\n",
    "    \n",
    "# Crete a RegisterModel step, which registers the model with Sagemaker Model Registry.\n",
    "model_package_group_name = \"Llama3Models\" \n",
    "step_register_model = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=llm_model,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.g5.12xlarge\"],\n",
    "    transform_instances=[\"ml.g5.12xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    depends_on=[training_step],\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Deployment\n",
    "## Why Deploy to Amazon Bedrock?\n",
    "\n",
    "Amazon Bedrock provides a serverless environment optimized for running large language models. \n",
    "By deploying our fine-tuned Llama 3 model to Bedrock, we gain:\n",
    "\n",
    "1. Scalability: Bedrock can handle varying inference loads efficiently\n",
    "2. Cost-effectiveness: Pay only for the compute resources used during inference\n",
    "3. Easy integration: Simplified API for model invocation in production environments\n",
    "4. Optimized performance: Bedrock is tuned for running large language models\n",
    "\n",
    "This section will walk through the process of importing our trained model into Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IAM roles and policies for Bedrock access\n",
    "This section focuses on setting up the necessary IAM (Identity and Access Management) roles and policies for Amazon Bedrock integration. We create two key roles:\n",
    "\n",
    "1. Lambda role\n",
    "   - Allows our Lambda function to interact with Bedrock and other AWS services\n",
    "   - Includes permissions for model import and S3 access\n",
    "\n",
    "2. Bedrock custom import role\n",
    "   - Enables Bedrock to access our trained model in S3\n",
    "   - Ensures secure and controlled access during the model import process\n",
    "\n",
    "Creating these roles is crucial for:\n",
    "- Maintaining the principle of least privilege\n",
    "- Enabling secure communication between different AWS services\n",
    "- Allowing our pipeline to programmatically import models into Bedrock\n",
    "\n",
    "By carefully defining these roles, we ensure our deployment process is both secure and functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lambda role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "# Now you can use it\n",
    "from iam_role_helper import create_lambda_execution_role\n",
    "\n",
    "# Get account information\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"LambdaBedrockExecutionRole\"\n",
    "\n",
    "# Define trust relationship\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define policies\n",
    "bedrock_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelImportJob\",\n",
    "                \"bedrock:ListModelImportJobs\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"iam:PassRole\"],\n",
    "            \"Resource\": f\"arn:aws:iam::{account_id}:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"bedrock.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{training_bucket}\",\n",
    "                f\"arn:aws:s3:::{training_bucket}/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Configure policies\n",
    "policies_config = {\n",
    "    'inline_policies': {\n",
    "        'BedrockAccessPolicy': bedrock_policy,\n",
    "        'S3AccessPolicy': s3_policy\n",
    "    },\n",
    "    'managed_policies': [\n",
    "        \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the role\n",
    "execution_role_arn = create_lambda_execution_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    policies_config=policies_config\n",
    ")\n",
    "\n",
    "print(f\"Execution Role ARN: {execution_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bedrock custom import role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"\n",
    "\n",
    "# Define policies\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "            \"Resource\": [f\"arn:aws:s3:::{training_bucket}\", f\"arn:aws:s3:::{training_bucket}/*\"],\n",
    "            \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "bedrock_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=permission_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Lambda function for model import to Bedrock\n",
    "This section prepares a Lambda function to automate the process of importing our trained model into Amazon Bedrock. Key aspects include:\n",
    "\n",
    "1. Creating a Lambda layer\n",
    "   - Adds necessary dependencies (like boto3) to our Lambda environment\n",
    "   - Ensures our function has access to required libraries\n",
    "\n",
    "2. Creating the Lambda function\n",
    "   - Defines a function that will handle the model import process\n",
    "   - Configures function parameters like memory, timeout, and execution role\n",
    "\n",
    "3. Integrating the Lambda function into our pipeline\n",
    "   - Creates a LambdaStep to include this function in our SageMaker pipeline\n",
    "   - Defines inputs (model URI, role ARN) and outputs (model ARN) for the step\n",
    "\n",
    "This Lambda function acts as a bridge between our SageMaker pipeline and Amazon Bedrock, automating the model deployment process. It allows us to programmatically import our fine-tuned Llama 3 model into Bedrock as soon as training is complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "# Create the boto3 layer first\n",
    "layer_arn = create_boto3_layer(lambda_client)\n",
    "print(f\"Layer ARN: {layer_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda function instance\n",
    "lambda_func = Lambda(\n",
    "    function_name=\"bedrock-model-import\",\n",
    "    execution_role_arn=execution_role_arn,\n",
    "    script=\"scripts/lambda/bedrock_model_import.py\",\n",
    "    handler='bedrock_model_import.lambda_handler',\n",
    "    timeout=900,  # 15 minutes, adjust as needed\n",
    "    memory_size=128,\n",
    "    runtime='python3.12',\n",
    "    layers=[layer_arn],  # Your boto3 layer ARN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the outputs\n",
    "lambda_outputs = [\n",
    "    LambdaOutput(output_name=\"model_arn\", output_type=LambdaOutputTypeEnum.String)\n",
    "]\n",
    "\n",
    "# Create the Lambda step\n",
    "lambda_step = LambdaStep(\n",
    "    name=\"BedrockModelImport\",\n",
    "    lambda_func=lambda_func,\n",
    "    inputs={\n",
    "        \"model_uri\": training_step.properties.ModelArtifacts.S3ModelArtifacts,  # Use the output from the training step\n",
    "        \"role_arn\": bedrock_role_arn,\n",
    "        \"model_name\": model_name\n",
    "    },\n",
    "    outputs=lambda_outputs,\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"1d\"),\n",
    "    depends_on=[step_register_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Execution\n",
    "### Executing the Pipeline: What to Expect\n",
    "\n",
    "Now that we've defined our pipeline, it's time to run it. This process will:\n",
    "\n",
    "1. Preprocess our data\n",
    "2. Train the Llama 3 model on our dataset\n",
    "3. Register the model in the SageMaker Model Registry\n",
    "4. Deploy the model to Amazon Bedrock\n",
    "\n",
    "Depending on the size of your dataset and the complexity of your model, \n",
    "this process may take several hours to complete. We'll monitor the progress \n",
    "and check the results of each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run the SageMaker Pipeline\n",
    "This section brings together all the previously defined steps to create and execute our complete SageMaker pipeline. Key points include:\n",
    "\n",
    "- Assembling the pipeline by combining preprocessing, training, model registration, and Lambda steps\n",
    "- Defining pipeline parameters and the execution role\n",
    "- Using error handling to manage potential issues during pipeline creation and execution\n",
    "- Initiating the pipeline execution\n",
    "\n",
    "The pipeline encapsulates our entire workflow, from data preprocessing to model deployment in Bedrock. By using a pipeline, we ensure:\n",
    "\n",
    "- Reproducibility of our ML workflow\n",
    "- Automated execution of all steps in sequence\n",
    "- Easy monitoring and management of the entire process\n",
    "\n",
    "Running this pipeline will kick off the end-to-end process of fine-tuning our Llama 3 model and deploying it to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline = Pipeline(\n",
    "        name=\"Llama3-QAPipeline\",\n",
    "        steps=[preprocessing_step, training_step,step_register_model,lambda_step ],\n",
    "        parameters=[role, model_name],\n",
    "        sagemaker_session=pipeline_session,\n",
    "    )\n",
    "    logging.info(\"Pipeline created successfully\")\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    logging.info(\"Pipeline upserted successfully\")\n",
    "\n",
    "    execution = pipeline.start()\n",
    "    logging.info(\"Pipeline started successfully\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    logging.error(f\"ValueError occurred: {str(ve)}\")\n",
    "    logging.error(f\"Error occurred in pipeline definition: {pipeline.definition()}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    logging.error(f\"Error type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor pipeline execution progress\n",
    "This section focuses on tracking the progress of our SageMaker pipeline execution. Key aspects include:\n",
    "\n",
    "- Using a custom function `monitor_pipeline_execution` to observe the pipeline's status\n",
    "- Extracting information about individual step outputs, particularly the preprocessing step\n",
    "- Retrieving the S3 URI for the test dataset, which will be used later for model evaluation\n",
    "\n",
    "Monitoring the pipeline execution is crucial because:\n",
    "- It allows us to track the progress of our end-to-end ML workflow in real-time\n",
    "- We can quickly identify and troubleshoot any issues that arise during execution\n",
    "- It provides valuable information about the location of output artifacts (like the test dataset)\n",
    "\n",
    "This monitoring step ensures we have visibility into our pipeline's performance and prepares us for the subsequent model evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pipeline execution status\n",
    "monitor_pipeline_execution(execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing step\n",
    "preprocessing_step_name = \"PreprocessQADataset\"  # Make sure this matches your step name\n",
    "steps = execution.list_steps()\n",
    "preprocessing_step_output = next((step for step in steps if step['StepName'] == preprocessing_step_name), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the processing job ARN\n",
    "processing_job_arn = preprocessing_step_output['Metadata']['ProcessingJob']['Arn']\n",
    "\n",
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Describe the processing job to get its details\n",
    "processing_job_details = sagemaker_client.describe_processing_job(ProcessingJobName=processing_job_arn.split('/')[-1])\n",
    "\n",
    "# Extract the S3 URI for the test output\n",
    "test_output_uri = None\n",
    "for output in processing_job_details['ProcessingOutputConfig']['Outputs']:\n",
    "    if output['OutputName'] == 'test':\n",
    "        test_output_uri = output['S3Output']['S3Uri']\n",
    "        break\n",
    "if test_output_uri:\n",
    "    print(f\"Extracted test output S3 URI: {test_output_uri}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Could not find test output S3 URI in the processing job details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "### Why Evaluate the Model?\n",
    "\n",
    "After training and deploying our model, it's crucial to evaluate its performance. \n",
    "This helps us:\n",
    "\n",
    "1. Understand how well the model has learned from our dataset\n",
    "2. Identify any areas where the model might be struggling\n",
    "3. Compare the performance of our fine-tuned model to the base Llama 3 model\n",
    "4. Make informed decisions about whether the model is ready for production use\n",
    "\n",
    "In this section, we'll run some sample inputs through our deployed model and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Availability in Amazon Bedrock\n",
    "\n",
    "After initiating the model import job, we need to verify that our model has been successfully imported and is available in Amazon Bedrock. This step is crucial before we can proceed with using the model for inference.\n",
    "\n",
    "The following function, `wait_for_model_availability`, periodically checks Bedrock for the presence of our imported model. It continues checking at regular intervals until either:\n",
    "\n",
    "1. The model is found and its details are returned, or\n",
    "2. The maximum number of attempts is reached without finding the model.\n",
    "\n",
    "This approach is necessary because the import process can take several minutes to complete, and the exact duration can vary depending on factors such as model size and current Bedrock workload.\n",
    "\n",
    "> ⚠️ **Note:** The default settings check for the model every 60 seconds for up to 30 attempts (30 minutes total). You may need to adjust these parameters based on your specific model and use case.\n",
    "\n",
    "Let's run this function to confirm our model's availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "model_name_filter = \"llama3-qa-model\"  # Replace with your model name\n",
    "model_info = wait_for_model_availability(model_name_filter,max_attempts=30,delay=60)\n",
    "\n",
    "if model_info:\n",
    "    model_arn=model_info[\"modelArn\"]\n",
    "    print(\"Model is now available in Bedrock.\")\n",
    "else:\n",
    "    print(\"Failed to find the model in Bedrock within the specified attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📌 Tip: While waiting for the import job to complete, you can take a short break. The import process typically takes between 10 to 30 minutes, depending on the model size and current Bedrock workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model outputs with expected results\n",
    "In this section, we'll evaluate our newly imported Bedrock model by comparing its outputs with expected results from our test dataset.\n",
    "\n",
    "> ⚠️ **Important Note:** After a successful import, Amazon Bedrock requires some time to prepare your custom model for inference. This process typically takes 10-15 minutes but may occasionally take longer.\n",
    "\n",
    "> 📌 **Tip:** If you encounter errors like \"Model is not in a valid state for invocation\", it means Bedrock is still preparing your model. Wait for a few minutes and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "S3_URI = test_output_uri\n",
    "results_df = run_model_evaluation(\n",
    "    model_id=model_arn,\n",
    "    s3_uri=S3_URI,\n",
    "    num_samples=10,\n",
    "    batch_size=5,\n",
    "    display_examples=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Resources\n",
    "\n",
    "After completing your experiments and evaluations, it's crucial to clean up the resources you've created to avoid ongoing charges. This section will guide you through the process of deleting all the resources used in this notebook.\n",
    "\n",
    "> ⚠️ **Warning:** The following steps will permanently delete resources. Make sure you've saved any important data or model artifacts before proceeding.\n",
    "\n",
    "### 1. Delete the Bedrock Custom Model\n",
    "\n",
    "First, let's remove the custom model from Amazon Bedrock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bedrock_custom_model(model_name):\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    try:\n",
    "        bedrock_client.delete_imported_model(modelIdentifier=model_name)\n",
    "        print(f\"Successfully deleted Bedrock custom model: {model_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        error_code = error.response['Error']['Code']\n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"Error deleting Bedrock custom model: The provided model name is invalid. Model Name: {model_name}\")\n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            print(f\"Error: The model '{model_name}' was not found in Bedrock.\")\n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            print(\"Error: You do not have permission to delete this model.\")\n",
    "        elif error_code == 'ConflictException':\n",
    "            print(\"Error: The model is currently in use or in a state that doesn't allow deletion.\")\n",
    "        else:\n",
    "            print(f\"Error deleting Bedrock custom model: {error}\")\n",
    "\n",
    "# Replace with your actual model name\n",
    "MODEL_NAME = \"llama3-qa-model\"\n",
    "delete_bedrock_custom_model(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Delete IAM Roles\n",
    "\n",
    "Now, let's remove the IAM roles we created specifically for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_iam_role(role_name):\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        # Delete inline policies\n",
    "        inline_policies = iam.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy in inline_policies:\n",
    "            iam.delete_role_policy(RoleName=role_name, PolicyName=policy)\n",
    "            \n",
    "        # Detach managed policies\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "            \n",
    "        # Delete permissions boundary if it exists\n",
    "        try:\n",
    "            iam.delete_role_permissions_boundary(RoleName=role_name)\n",
    "        except iam.exceptions.NoSuchEntityException:\n",
    "            pass\n",
    "        \n",
    "        # Finally delete the role\n",
    "        iam.delete_role(RoleName=role_name)\n",
    "        print(f\"Successfully deleted IAM role: {role_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting IAM role: {error}\")\n",
    "\n",
    "# Delete LambdaBedrockExecutionRole\n",
    "delete_iam_role(\"LambdaBedrockExecutionRole\")\n",
    "\n",
    "# Delete Sagemaker_Bedrock_import_role\n",
    "delete_iam_role(\"Sagemaker_Bedrock_import_role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Delete Lambda Functions\n",
    "\n",
    "If you created any Lambda functions, delete them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lambda_function(function_name):\n",
    "    lambda_client = boto3.client('lambda')\n",
    "    try:\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "        print(f\"Successfully deleted Lambda function: {function_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting Lambda function: {error}\")\n",
    "\n",
    "# Replace with your actual Lambda function name\n",
    "LAMBDA_FUNCTION_NAME = \"bedrock-model-import\"\n",
    "delete_lambda_function(LAMBDA_FUNCTION_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Delete Lambda Layers\n",
    "\n",
    "If you created any Lambda layers, remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lambda_layer(layer_name):\n",
    "    lambda_client = boto3.client('lambda')\n",
    "    try:\n",
    "        # List versions of the layer\n",
    "        versions = lambda_client.list_layer_versions(LayerName=layer_name)['LayerVersions']\n",
    "        \n",
    "        # Delete each version\n",
    "        for version in versions:\n",
    "            lambda_client.delete_layer_version(\n",
    "                LayerName=layer_name,\n",
    "                VersionNumber=version['Version']\n",
    "            )\n",
    "        print(f\"Successfully deleted all versions of Lambda layer: {layer_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting Lambda layer: {error}\")\n",
    "\n",
    "# Replace with your actual Lambda layer name\n",
    "LAYER_NAME = \"boto3-latest\"\n",
    "delete_lambda_layer(LAYER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📌 Tip: Always double-check your AWS Console to ensure all resources have been properly deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "After running these cleanup steps, it's a good practice to manually verify in the AWS Console that all resources have been successfully deleted. Pay special attention to:\n",
    "\n",
    "- Bedrock custom models\n",
    "- IAM roles (especially LambdaBedrockExecutionRole and Sagemaker_Bedrock_import_role)\n",
    "- Lambda functions and layers\n",
    "> 🚨 Important: If you encounter any issues or have resources that weren't covered in this cleanup process, please refer to the AWS documentation or contact AWS support for assistance in properly removing all resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've walked through the entire process of fine-tuning a Llama 3 model and deploying it to Amazon Bedrock. Let's recap the key steps and learnings:\n",
    "\n",
    "### Key Accomplishments\n",
    "\n",
    "1. **Data Preparation**: We preprocessed and prepared a dataset suitable for fine-tuning the Llama 3 model.\n",
    "\n",
    "2. **SageMaker Pipeline**: We constructed a SageMaker pipeline that encompassed:\n",
    "   - Data preprocessing\n",
    "   - Model training using Llama 3\n",
    "   - Model evaluation\n",
    "   - Model registration\n",
    "\n",
    "3. **Bedrock Deployment**: We successfully imported our fine-tuned model into Amazon Bedrock, making it ready for inference.\n",
    "\n",
    "4. **Custom Inference**: We demonstrated how to use the custom Bedrock model for inference, comparing its outputs with expected results.\n",
    "\n",
    "5. **Resource Management**: We created and managed various AWS resources, including IAM roles, Lambda functions, and S3 buckets.\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **Fine-tuning Large Language Models**: We gained hands-on experience in fine-tuning a state-of-the-art language model like Llama 3 for specific use cases.\n",
    "\n",
    "- **SageMaker Pipelines**: We leveraged SageMaker Pipelines to create a reproducible and scalable ML workflow.\n",
    "\n",
    "- **Bedrock Integration**: We learned how to bridge the gap between SageMaker and Bedrock, enabling us to use custom models in the Bedrock environment.\n",
    "\n",
    "- **AWS Service Orchestration**: This project demonstrated the seamless integration of multiple AWS services (SageMaker, Bedrock, Lambda, IAM, S3) to create an end-to-end ML solution.\n",
    "\n",
    "### Potential Next Steps\n",
    "\n",
    "1. **Model Optimization**: Experiment with different hyperparameters or training datasets to further improve model performance.\n",
    "\n",
    "2. **Scalability Testing**: Assess the model's performance under various loads to ensure it meets production requirements.\n",
    "\n",
    "3. **Monitoring and Logging**: Implement comprehensive monitoring and logging for the deployed model in Bedrock.\n",
    "\n",
    "4. **A/B Testing**: Compare the performance of your fine-tuned model against the base Llama 3 model or other variants.\n",
    "\n",
    "5. **Continuous Learning**: Explore ways to implement continuous learning or periodic re-training to keep the model up-to-date.\n",
    "\n",
    "> 💡 **Insight**: The combination of SageMaker's robust training capabilities and Bedrock's inference optimizations provides a powerful platform for deploying custom large language models.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "This notebook has demonstrated the power and flexibility of AWS's machine learning ecosystem. By leveraging SageMaker for training and Bedrock for deployment, we've created a custom language model that can be easily integrated into various applications.\n",
    "\n",
    "Remember to clean up your resources as shown in the previous section to avoid unnecessary costs. As you continue to explore and build with these technologies, always keep best practices in mind, especially regarding data security and model governance.\n",
    "\n",
    "Thank you for following along with this notebook. Happy modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
