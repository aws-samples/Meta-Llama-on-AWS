{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Llama 3.1 405B for Summarizing and Preparing Instruction Fine-Tuned Dataset\n",
    "\n",
    "In this notebook, we will walk you through the process of utilizing a larger language model (LLM) like Llama 3.1 405B, Meta AI's latest and most advanced model, to create a dataset for instruction fine-tuning. This dataset will be used to perform distillation by fine-tuning a smaller model, such as Llama 3 8B.\n",
    "\n",
    "By leveraging the capabilities of Llama 3.1 405B, we can generate high-quality, concise training data that enhances the performance of smaller models. This approach is particularly useful for tasks that require detailed and specific instructions.\n",
    "\n",
    "Before we begin, ensure that you have access to Llama 3.1 405B, which is now available on Amazon SageMaker Jumpstart. You can find the dataset we will be using [here](https://huggingface.co/datasets/deepmind/aqua_rat).\n",
    "\n",
    "You can run the notebook on an Amazon SageMaker Studio notebook, or a SageMaker notebook instance without manually setting your aws credentials.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Amazon SageMaker JumpStart\n",
    "\n",
    "![Alt text](imgs/jumpstart-overview-img1.png \"SageMaker JumpStart Overview\")\n",
    "\n",
    "**Amazon SageMaker JumpStart** is a powerful feature within Amazon SageMaker designed to help you quickly get started with LLMs by providing access to a wide range of pre-trained foundation models (FM). We'll be using this for deploying and fine tuning our models.\n",
    "\n",
    "Key Features\n",
    "- **Pre-trained Models**: SageMaker JumpStart provides a variety of pre-trained models from different model providers (Llama, Mistral, Cohere, Stablity) for different problem types, enabling you to start your machine learning projects without the need to build models from scratch.\n",
    "\n",
    "- **Training and Tuning**: With a few clicks, you can train and fine-tune these models to better fit your specific data and use case before deploying them.\n",
    "\n",
    "- **Solution Templates**: JumpStart offers solution templates that automatically set up the necessary infrastructure for common use cases, streamlining the deployment process.\n",
    "\n",
    "### Llama 3.1 405B Model\n",
    "\n",
    "Llama 3.1 405B is the largest model in the family of Llama 3.1 models. Llama 3.1 model family is a collection of pre-trained and instruction-tuned LLMs which already includes 8B and 70B parameter sizes. Llama 3.1 405B comes with new capabilities including multi-language support and a 128k context window. These models are stronger overall capabilities and are ideal for content creation, conversational AI, language understanding, research and development (R&D), and enterprise applications.\n",
    "\n",
    "\n",
    "### Llama 3 8B Model\n",
    "\n",
    "LLama 3 8B is an LLM with 8 billion parameters designed to deliver high performance across a variety of tasks while maintaining cost efficiency. This model is particularly advantageous for developers and organizations looking to implement advanced AI capabilities without the need for extensive computational resources. LLaMA 3 8B is optimized for dialogue and other interactive applications, demonstrating strong performance in benchmarks such as MMLU, AGIEval, and CommonSenseQA, where it outperforms many open-source models of similar size. Its ability to run on more affordable hardware highlights its potential for cost-effective deployment in real-time applications like chatbots and customer support systems.\n",
    "\n",
    "\n",
    "\n",
    "### Prequisites\n",
    " In order to follow along in this notebook, you'll need access to the following:\n",
    "\n",
    " - An AWS account with SageMaker endpoint capacity for an ml.p4de instance type. You can find more information about how to request a service limit increase [here](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html).\n",
    "\n",
    " - An [AWS Identity and Access Management (IAM)](https://aws.amazon.com/iam/) role to access SageMaker. To learn more about how IAM works with SageMaker, refer to [Identity and Access Management for Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam.html).\n",
    " \n",
    " - Access to SageMaker Studio or a SageMaker notebook instance or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we perform the following high level steps: \n",
    "\n",
    "1. We deploy a `Llama3-8b instruct` model and generate inferences on a `deepmind/aqua_rat` dataset.\n",
    "\n",
    "1. Deploy and leverage the capabilities of the new `Llama 3.1 405B Model` to generate labels and corresponding data to be used to do distillation by fine-tuning`Llama3-8b instruct`\n",
    "\n",
    "1. Test the fine-tuned `Llama3-8b instruct` model and test the model against the same questions to showcase the increase in response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4037/2991053635.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from botocore.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use this function to call inference on our deployed models\n",
    "def run_inference(predictor, example_payloads):\n",
    "    for payload in example_payloads:\n",
    "        response = predictor.predict(payload)\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        print(\"Input:\\n\", payload[\"inputs\"], end=\"\\n\\n\")\n",
    "        print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration and Preparation\n",
    "\n",
    "In this section, we will explore a dataset from the Hugging Face Hub using the HF Datasets library. Hugging Face provides a vast collection of datasets for various tasks in natural language processing (NLP), computer vision, and audio processing. This exploration will help us understand the structure, features, and contents of the dataset, enabling us to prepare it for training and evaluation in our machine learning models. The [deepmind/aqua_rat](https://huggingface.co/datasets/deepmind/aqua_rat) dataset is a large-scale collection of approximately 100,000 algebraic word problems, each accompanied by a detailed natural language rationale explaining the solution process. This dataset is designed to train and evaluate models that not only generate the correct answer but also provide a step-by-step explanation, making it ideal for tasks requiring mathematical reasoning and natural language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: deepmind/aqua_rat\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'options', 'rationale', 'correct'],\n",
      "        num_rows: 97467\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'options', 'rationale', 'correct'],\n",
      "        num_rows: 254\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'options', 'rationale', 'correct'],\n",
      "        num_rows: 254\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset Features:\n",
      "{'question': Value(dtype='string', id=None), 'options': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'rationale': Value(dtype='string', id=None), 'correct': Value(dtype='string', id=None)}\n",
      "\n",
      "Sample Examples:\n",
      "{'question': \"Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\", 'options': ['A)21', 'B)21.5', 'C)22', 'D)22.5', 'E)23'], 'rationale': 'If Q complete x kilometers, then P completes 1.15x kilometers.\\nx + 1.15x = 43\\n2.15x=43\\nx = 43/2.15 = 20\\nThen P will have have walked 1.15*20=23 km.\\nThe answer is E.', 'correct': 'E'}\n",
      "{'question': 'In the coordinate plane, points (x, 1) and (5, y) are on line k. If line k passes through the origin and has slope 1/5, then what are the values of x and y respectively?', 'options': ['A)4 and 1', 'B)1 and 5', 'C)5 and 1', 'D)3 and 5', 'E)5 and 3'], 'rationale': 'Line k passes through the origin and has slope 1/5 means that its equation is y=1/5*x.\\nThus: (x, 1)=(5, 1) and (5, y) = (5,1) -->x=5 and y=1\\nAnswer: C', 'correct': 'C'}\n",
      "{'question': 'For all numbers p and q, the operation @ is defined by p@q = p^2 - pq. If xy ≠ 0, then which of the following can be equal to zero?\\nI. x@y\\nII. (xy)@y\\nIII. x@(x + y)', 'options': ['A)II', 'B)I and II', 'C)I and III', 'D)II and III', 'E)All of the above'], 'rationale': 'p@q = p^2 - pq=p(p-q).... so p@q will be zero if p=q or p=0.. but a cannot be equal to 0.. as per Q, x and y can take any int value except 0...\\nnow lets look at the choices..\\nwhen x=y, it will be 0... so ok...\\nwhen we put xy=y, it is possible when x=1 and y any integer... so ok again\\nwhen we put x=x+y.... only possibility when y=0 and it is given x and y cannot be 0....so not possible\\nonly l and ll possible ans B....', 'correct': 'B'}\n",
      "\n",
      "Number of Examples in Each Split:\n",
      "train: 97467 examples\n",
      "test: 254 examples\n",
      "validation: 254 examples\n",
      "\n",
      "First 20 Questions:\n",
      "1: Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?\n",
      "2: In the coordinate plane, points (x, 1) and (5, y) are on line k. If line k passes through the origin and has slope 1/5, then what are the values of x and y respectively?\n",
      "3: For all numbers p and q, the operation @ is defined by p@q = p^2 - pq. If xy ≠ 0, then which of the following can be equal to zero?\n",
      "I. x@y\n",
      "II. (xy)@y\n",
      "III. x@(x + y)\n",
      "4: Carl is facing very difficult financial times and can only pay the interest on a $10,000 loan he has taken. The bank charges him a quarterly compound rate of 4%. What is the approximate interest he pays annually?\n",
      "5: The speed at which a man can row a boat in still water is 25 kmph. If he rows downstream, where the speed of current is 11 kmph, what time will he take to cover 80 metres?\n",
      "6: There are k-2 members in a certain band, including Jim and Ellen. Two members are to be selected to attend the Grammy awards ceremony. If there are 6 possible combinations in which Jim and Ellen are not selected, what is the value of k?\n",
      "7: If (x^2 + 4x - 11)/5 ≤ x + 1, then x could be represented by which of the following?\n",
      "8: Find the smallest number of five digits exactly divisible by 22,33,66 and 44.\n",
      "9: The entrance fee for a fair is $5 for persons under the age of 18, and 20% more for persons older. Each ride at the fair costs $0.50. If Joe goes with her 6 years old twin brothers, and they each took 3 rides in total. How much money does Joe end up spending at the fair?\n",
      "10: If X and Y are digits and 8XY is a 3-digit number that is divisible by 2, which of the following is a possible product of X and Y?\n",
      "11: If Tim had lunch at $50 and he gave 20% tip, how much did he spend?\n",
      "12: Rs. 825 becomes Rs. 956 in 3 years at a certain rate of simple interest.If the rate of interest is increased by 4% ,What amount will Rs. 825 become in 3 years ?\n",
      "13: q is a positive integer and multiple of 2; p = 4^q, what is the remainder when p is divided by 10?\n",
      "14: If q is the square of a positive integer, which of the following must be equal to the square of the next positive integer?\n",
      "15: Rs. 5600 is divided into three parts A, B and C. How much A is more than C if their ratio is 1/7:1/7:1/14?\n",
      "16: If a/b=3/4 and 8a+5b=22,then find the value of a.\n",
      "17: Given that k/l < 1, and both k and l are positive integers, which one of the following must be greater than 1?\n",
      "18: Mike took 5 mock tests before appearing for the GMAT. In each mock test he scored 10 points more than the previous mock test. If he scored 760 on the GMAT and his average score for the mocks and the GMAT was 716.67, what was the difference in the score of his last mock and his GMAT score?\n",
      "19: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of number is?\n",
      "20: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the necessary functions from the datasets library\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the AQUA-RAT dataset from the Hugging Face Hub\n",
    "dataset_name = \"deepmind/aqua_rat\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(dataset)\n",
    "\n",
    "# Display the dataset's features\n",
    "print(\"\\nDataset Features:\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# Display a few examples from the dataset\n",
    "print(\"\\nSample Examples:\")\n",
    "for i in range(3):\n",
    "    print(dataset['train'][i])\n",
    "\n",
    "# Display the number of examples in each split\n",
    "print(\"\\nNumber of Examples in Each Split:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"{split}: {len(dataset[split])} examples\")\n",
    "\n",
    "# Extract 20 questions from the dataset\n",
    "questions = dataset['train'].select(range(20))['question']\n",
    "\n",
    "# Display the first 20 questions\n",
    "print(\"\\nFirst 20 Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"{i+1}: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Llama 3 8B Instruct\n",
    "\n",
    "In this section, we will deploy the base, pre-trained LLama 3 8B model and test it against a subset of our dataset to evaluate its responses compared to the larger LLama 3.1 405B model. Initially, we expect the smaller model to produce lower-quality responses. By identifying these deficiencies, we can generate high-quality synthetic data using the 405B model and subsequently do distillation by fine-tuning the 8B model. This process aims to demonstrate the improvement in response quality after fine-tuning the 8B model with the generated dataset.\n",
    "\n",
    "> You'll need a `g5.12xlarge` instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-07-24 16:17:50,580] p4037 {model.py:242} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-07-24 16:17:50,585] p4037 {session.py:3961} INFO - Creating model with name: meta-textgeneration-llama-3-8b-instruct-2024-07-24-16-17-50-579\n",
      "[2024-07-24 16:17:51,531] p4037 {session.py:5725} INFO - Creating endpoint-config with name meta-textgeneration-llama-3-8b-instruct-2024-07-24-16-17-50-584\n",
      "[2024-07-24 16:17:51,852] p4037 {session.py:4571} INFO - Creating endpoint with name meta-textgeneration-llama-3-8b-instruct-2024-07-24-16-17-50-584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!Running inference with LLama 3 8B model:\n",
      "\n",
      "Input:\n",
      " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Output:\n",
      " Let's say that Friend Q's rate is x. Then Friend P's rate is 1.15x. The total distance is 43 km. So when they pass each other, they will have walked a total of 43 km. So x + 1.15x = 43. 2.15x = 43. x = 20. So Friend P's rate is 1.15 * 20 = 23. So when they pass each other, Friend P will have walked 23 km. The answer is 23.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Specify the role ARN directly\n",
    "role = get_execution_role(sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Select a model ID and version\n",
    "llama_3_8b_model_id = \"meta-textgeneration-llama-3-8b-instruct\" # Replace with your chosen model ID\n",
    "\n",
    "# If your selected model is gated, you will need to set accept_eula to True to accept the model end-user license agreement (EULA).\n",
    "accept_eula = False\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "llama_3_8b_model = JumpStartModel(model_id=llama_3_8b_model_id,role=role)\n",
    "llama_3_8b_predictor = llama_3_8b_model.deploy(accept_eula=accept_eula)\n",
    "\n",
    "# example_payloads = llama_3_8b_model.retrieve_all_examples() # uncomment if you want to preloaded examples instead\n",
    "\n",
    "question = questions[0]\n",
    "\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"details\": True,\n",
    "            \"stop\": \"<|eot_id|>\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Running inference with LLama 3 8B model:\\n\")\n",
    "run_inference(llama_3_8b_predictor, example_payloads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying LLama 3.1 405B Instruct\n",
    "\n",
    "In this section, we will deploy the LLama 3.1 405B model to compare its responses with those of the smaller LLama 3 8B model. This deployment will allow us to evaluate the performance differences and identify areas where the 8B model's responses can be improved. By analyzing the responses from the 405B model, we can generate high-quality data for distillation of the 8B model, enhancing its accuracy and effectiveness for domain-specific tasks.\n",
    "\n",
    "> You'll need a 'p5.48xlarge' instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "[2024-07-24 16:26:15,382] p4037 {utils.py:566} INFO - Model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' with wildcard version identifier '*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-07-24 16:26:15,384] p4037 {cache.py:619} WARNING - Using model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' with wildcard version identifier '*'. You can pin to version '1.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "[2024-07-24 16:26:15,387] p4037 {model.py:242} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "[2024-07-24 16:26:15,394] p4037 {session.py:3961} INFO - Creating model with name: llama-3-1-405b-instruct-fp8-2024-07-24-16-26-15-386\n",
      "[2024-07-24 16:26:16,090] p4037 {session.py:5725} INFO - Creating endpoint-config with name llama-3-1-405b-instruct-fp8-2024-07-24-16-26-15-390\n",
      "[2024-07-24 16:26:16,427] p4037 {session.py:4571} INFO - Creating endpoint with name llama-3-1-405b-instruct-fp8-2024-07-24-16-26-15-390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!Running inference with LLama 3.1 405B model:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a model ID and version\n",
    "llama_3_1_405b_model_id = \"meta-textgeneration-llama-3-1-405b-instruct-fp8\" # Replace with your chosen model ID\n",
    "\n",
    "# If your selected model is gated, you will need to set accept_eula to True to accept the model end-user license agreement (EULA).\n",
    "accept_eula = False\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "llama_3_1_405b_model = JumpStartModel(model_id=llama_3_1_405b_model_id,role=role)\n",
    "llama_3_1_405b_predictor = llama_3_1_405b_model.deploy(accept_eula=accept_eula)\n",
    "\n",
    "# example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "question = questions[1]\n",
    "\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"details\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Test the deployed endpoint\n",
    "print(\"Running inference with LLama 3.1 405B model:\\n\")\n",
    "run_inference(llama_3_1_405b_predictor, example_payloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 3.1 405B for Data Labeling/Generation\n",
    "\n",
    "In this section, we will leverage the LLama 3.1 405B model to generate high-quality synthetic data for distillation by fine-tuning the LLama 3 8B model. By using the 405B model to generate responses to domain-specific prompts, we can create a labeled dataset that will be used to fine-tune the 8B model, improving its accuracy and effectiveness in specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and select the first 2000 questions\n",
    "dataset = load_dataset('deepmind/aqua_rat', split='train')\n",
    "questions = dataset.select(range(2000))['question']\n",
    "\n",
    "# Function to run inference and generate synthetic data using SageMaker JumpStart\n",
    "def generate_synthetic_data(predictor, questions):\n",
    "    synthetic_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question\n",
    "        user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        payload = {\n",
    "            \"inputs\": user_message,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.0\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            # Send the message to the model\n",
    "            response = predictor.predict(payload)\n",
    "            # print(f\"Response: {response}\")  # Debugging statement to inspect the response structure\n",
    "            \n",
    "            # Directly handle the response without JSON parsing\n",
    "            if isinstance(response, list) and 'generated_text' in response[0]:\n",
    "                response_text = response[0]['generated_text'].strip()\n",
    "            else:\n",
    "                response_text = response['generated_text'].strip()\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                \"instruction\": question,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Reason: {e}\")\n",
    "            break \n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data using the SageMaker JumpStart deployed model\n",
    "synthetic_data = generate_synthetic_data(llama_3_1_405b_predictor, questions)\n",
    "\n",
    "# Save the synthetic data to a JSONL file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for entry in synthetic_data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Bedrock Example\n",
    "\n",
    "> Note: You'll probably need [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Can't invoke 'meta.llama3-1-8b-instruct-v1:0'. Reason: An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Bedrock client\n",
    "config = Config(read_timeout=5000)\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\", config=config)\n",
    "\n",
    "# Set the model ID, e.g., Llama 3.1 405b.\n",
    "model_id = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "\n",
    "# Load the dataset and select the first 20 questions\n",
    "dataset = load_dataset('deepmind/aqua_rat', split='train')\n",
    "questions = dataset.select(range(2000))['question']\n",
    "\n",
    "# Function to run inference and generate synthetic data using Bedrock\n",
    "def generate_synthetic_data(client, model_id, questions):\n",
    "    synthetic_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question\n",
    "        user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_message}],\n",
    "            }\n",
    "        ]\n",
    "        try:\n",
    "            # Send the message to the model, using a basic inference configuration.\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 1024,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"topP\": 0.9\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Extract the response text\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            synthetic_data.append({\n",
    "                \"instruction\": question,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "            break\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data using Bedrock\n",
    "synthetic_data = generate_synthetic_data(client, model_id, questions)\n",
    "\n",
    "# Save the synthetic data to a JSONL file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for entry in synthetic_data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Files to S3 for Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File template.json uploaded successfully to llama-405b-synthetic-training-data/template.json.\n",
      "File synthetic_data.jsonl uploaded successfully to llama-405b-synthetic-training-data/synthetic_data.jsonl.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = '<<INSERT_BUCKET_NAME>>'  # Create a new bucket or use an existing one\n",
    "subdirectory = 'llama-405b-synthetic-training-data'\n",
    "train_data_location = f\"s3://{bucket_name}/{subdirectory}\"\n",
    "\n",
    "files_to_upload = ['template.json','synthetic_data.jsonl']\n",
    "\n",
    "# Upload the files to the specified subdirectory\n",
    "for file_name in files_to_upload:\n",
    "    file_path = file_name  # File is in the same directory as the notebook\n",
    "    key_path = f\"{subdirectory}/{file_name}\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n",
    "    \n",
    "    # Upload the file\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, key_path)\n",
    "        print(f\"File {file_name} uploaded successfully to {key_path}.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distillation by Fine-tuning Llama 3 8B\n",
    "\n",
    "In this section, we will dive deep into the process of distillation by fine-tuning the LLama 3 8B model to enhance its performance for specific tasks. Fine-tuning involves training the pre-trained model on custom datasets to adapt it to particular domains or applications. This process can be resource-intensive, but using techniques such as LoRA (Low Rank Adaptation) and QLoRA (Quantized LoRA) can significantly reduce the required computational resources and costs. We will explore how to set up and execute a fine-tuning job using SageMaker.\n",
    "\n",
    "> You'll need a `g5.12xlarge` instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-19 13:54:25,263] p54 {session.py:978} INFO - Creating training-job with name: meta-textgeneration-llama-3-8b-instruct-2024-07-19-13-54-25-260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-19 13:54:25 Starting - Starting the training job...\n",
      "2024-07-19 13:54:25 Pending - Training job waiting for capacity...........................\n",
      "2024-07-19 13:59:21 Pending - Preparing the instances for training...\n",
      "2024-07-19 13:59:52 Downloading - Downloading input data...........................\n",
      "2024-07-19 14:04:18 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-07-19 14:04:20,635 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-07-19 14:04:20,671 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-19 14:04:20,680 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:04:20,682 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:04:29,784 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.20.3-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.38.0-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.5-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.20.3->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.0->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mhuggingface-hub is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=16e356293a60f4e8b79f6f5604907a3bef0067edea5219319c7fe9b799402ea0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.5 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.15.2 torch-2.2.0 transformers-4.38.0 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,567 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,567 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,621 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,667 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,714 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,725 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"1\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"1\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-8b-instruct-2024-07-19-13-54-25-260\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-8b-instruct-2024-07-19-13-54-25-260\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"1\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 1 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-07-19 14:05:29,756 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '1', '--micro_batch_size', '1', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '1', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-07-19 14:05:35,148] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-07-19 14:05:35,148] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-07-19 14:05:35,148] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-07-19 14:05:35,148] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14614.30it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 890.89it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2103 examples [00:00, 103431.30 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 22250.64 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 5234.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 5591.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 5458.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 3939.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 3871.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 3059.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 4498.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 4394.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 4434.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 4334.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 4300.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 4065.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2103 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 2550.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 2807.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 2797.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 1000/2103 [00:00<00:00, 2801.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 2665.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 2632.02 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 2722.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 2729.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 2716.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 2000/2103 [00:00<00:00, 2791.39 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 2710.75 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2103/2103 [00:00<00:00, 2773.75 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.26s/it]#015Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.25s/it]#015Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.33s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.25s/it]#015Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.14s/it]#015Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.35s/it]#015Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.10s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 8030.261248 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/135 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/135 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 540\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 136\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/135 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/135 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/135 [00:10<24:22, 10.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/135 [00:09<21:19,  9.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/135 [00:09<22:11,  9.94s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.2013858556747437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/135 [00:09<21:10,  9.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 2/135 [00:18<20:06,  9.07s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 2/135 [00:19<21:21,  9.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 2/135 [00:18<20:28,  9.23s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.0415804386138916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 2/135 [00:18<20:02,  9.04s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/135 [00:27<19:36,  8.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/135 [00:28<20:17,  9.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/135 [00:27<19:48,  9.00s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.0098515748977661\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/135 [00:26<19:34,  8.90s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.2258648872375488\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 4/135 [00:36<19:26,  8.90s/it]#015Training Epoch0:   3%|#033[34m▎         #033[0m| 4/135 [00:35<19:17,  8.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 4/135 [00:37<19:43,  9.04s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 4/135 [00:35<19:19,  8.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 5/135 [00:45<19:20,  8.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 5/135 [00:44<19:09,  8.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 5/135 [00:44<19:05,  8.81s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.8045049905776978\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 5/135 [00:44<19:04,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 6/135 [00:53<18:52,  8.78s/it]#015Training Epoch0:   4%|#033[34m▍         #033[0m| 6/135 [00:54<19:02,  8.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 6/135 [00:53<18:55,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.9891311526298523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 6/135 [00:53<18:52,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 7/135 [01:01<18:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 7/135 [01:02<18:43,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.0444092750549316\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 7/135 [01:01<18:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 7/135 [01:03<18:48,  8.82s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 8/135 [01:11<18:33,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.964569628238678\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 8/135 [01:10<18:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 8/135 [01:10<18:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 8/135 [01:12<18:36,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.0264739990234375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 9/135 [01:19<18:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 9/135 [01:19<18:21,  8.74s/it]#015Training Epoch0:   7%|#033[34m▋         #033[0m| 9/135 [01:19<18:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 9/135 [01:20<18:25,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 10/135 [01:28<18:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.0063016414642334\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 10/135 [01:28<18:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 10/135 [01:29<18:14,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 10/135 [01:28<18:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 11/135 [01:38<18:05,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 11/135 [01:36<18:04,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 11/135 [01:37<18:04,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.998724102973938\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 11/135 [01:36<18:04,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 12/135 [01:46<17:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 12/135 [01:45<17:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.9560663104057312\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 12/135 [01:45<17:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 12/135 [01:47<17:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 13/135 [01:55<17:47,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.1491397619247437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 13/135 [01:54<17:46,  8.74s/it]#015Training Epoch0:  10%|#033[34m▉         #033[0m| 13/135 [01:54<17:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 13/135 [01:54<17:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 14/135 [02:03<17:37,  8.74s/it]#015Training Epoch0:  10%|#033[34m█         #033[0m| 14/135 [02:04<17:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.7413103580474854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 14/135 [02:03<17:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 14/135 [02:03<17:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.9925273656845093\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 15/135 [02:11<17:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 15/135 [02:13<17:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 15/135 [02:11<17:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 15/135 [02:12<17:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.1399332284927368\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 16/135 [02:21<17:20,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 16/135 [02:20<17:20,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 16/135 [02:21<17:20,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 16/135 [02:20<17:20,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 17/135 [02:30<17:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.7995707392692566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 17/135 [02:29<17:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 17/135 [02:29<17:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 17/135 [02:29<17:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.8620465397834778\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 18/135 [02:38<17:02,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 18/135 [02:38<17:02,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 18/135 [02:39<17:02,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 18/135 [02:38<17:02,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 19/135 [02:46<16:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.784344494342804\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 19/135 [02:48<16:53,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 19/135 [02:46<16:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 19/135 [02:47<16:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 20/135 [02:55<16:44,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.7164490222930908\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 20/135 [02:55<16:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 20/135 [02:55<16:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 20/135 [02:56<16:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 21/135 [03:05<16:40,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.6975975036621094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 21/135 [03:04<16:40,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 21/135 [03:04<16:40,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 21/135 [03:04<16:40,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.7751501202583313\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 22/135 [03:14<16:30,  8.77s/it]#015Training Epoch0:  16%|#033[34m█▋        #033[0m| 22/135 [03:13<16:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 22/135 [03:13<16:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 22/135 [03:13<16:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.8464022278785706\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 23/135 [03:22<16:20,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 23/135 [03:21<16:20,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 23/135 [03:23<16:20,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 23/135 [03:21<16:20,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.8016583323478699\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 24/135 [03:30<16:11,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 24/135 [03:30<16:11,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 24/135 [03:31<16:11,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 24/135 [03:31<16:11,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 25/135 [03:40<16:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 25/135 [03:39<16:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 25/135 [03:39<16:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.7008528709411621\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 25/135 [03:39<16:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 26/135 [03:48<15:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 26/135 [03:49<15:52,  8.74s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 26/135 [03:48<15:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.6166541576385498\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 26/135 [03:48<15:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 27/135 [03:58<15:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.7837060689926147\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 27/135 [03:56<15:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 27/135 [03:56<15:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 27/135 [03:57<15:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.727531909942627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 28/135 [04:05<15:35,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 28/135 [04:05<15:35,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 28/135 [04:05<15:35,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 28/135 [04:06<15:35,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.7202706336975098\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 29/135 [04:14<15:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 29/135 [04:15<15:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 29/135 [04:14<15:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 29/135 [04:14<15:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 30/135 [04:24<15:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 30/135 [04:23<15:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.7947585582733154\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 30/135 [04:22<15:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 30/135 [04:23<15:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.7314786314964294\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 31/135 [04:31<15:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 31/135 [04:33<15:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 31/135 [04:32<15:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 31/135 [04:31<15:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 32/135 [04:41<15:04,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.5857634544372559\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 32/135 [04:40<15:04,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 32/135 [04:40<15:04,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 32/135 [04:42<15:04,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.6350923180580139\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 33/135 [04:49<14:54,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 33/135 [04:49<14:54,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 33/135 [04:49<14:54,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 33/135 [04:50<14:54,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.6652361750602722\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 34/135 [04:58<14:44,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 34/135 [04:58<14:44,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 34/135 [04:59<14:44,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 34/135 [04:58<14:44,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.6857945322990417\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 35/135 [05:06<14:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 35/135 [05:08<14:34,  8.74s/it]#015Training Epoch0:  26%|#033[34m██▌       #033[0m| 35/135 [05:07<14:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 35/135 [05:06<14:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.4904922544956207\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 36/135 [05:15<14:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 36/135 [05:15<14:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 36/135 [05:15<14:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 36/135 [05:16<14:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 37/135 [05:25<14:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 37/135 [05:24<14:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 37/135 [05:24<14:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.5354757905006409\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 37/135 [05:24<14:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 38/135 [05:33<14:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.5543065667152405\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 38/135 [05:32<14:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 38/135 [05:33<14:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 38/135 [05:34<14:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 39/135 [05:43<13:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.8303149342536926\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 39/135 [05:41<13:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 39/135 [05:42<13:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 39/135 [05:41<13:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 40/135 [05:50<13:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.8041111826896667\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 40/135 [05:50<13:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 40/135 [05:50<13:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 40/135 [05:51<13:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 41/135 [05:59<13:41,  8.73s/it]#015Training Epoch0:  30%|#033[34m███       #033[0m| 41/135 [06:00<13:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 41/135 [05:59<13:41,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.7055438756942749\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 41/135 [05:59<13:41,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 42/135 [06:09<13:32,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.9358335137367249\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 42/135 [06:07<13:32,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 42/135 [06:08<13:32,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 42/135 [06:07<13:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 43/135 [06:16<13:26,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 43/135 [06:18<13:26,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.5401359796524048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 43/135 [06:16<13:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 43/135 [06:17<13:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 44/135 [06:27<13:20,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 44/135 [06:25<13:20,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.6588045954704285\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 44/135 [06:25<13:20,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 44/135 [06:26<13:20,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 45/135 [06:35<13:10,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.6053938269615173\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 45/135 [06:34<13:10,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 45/135 [06:34<13:10,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 45/135 [06:34<13:10,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.6413427591323853\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 46/135 [06:43<13:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 46/135 [06:43<13:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 46/135 [06:44<13:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 46/135 [06:43<13:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.656688928604126\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 47/135 [06:51<12:50,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 47/135 [06:53<12:50,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 47/135 [06:52<12:50,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 47/135 [06:51<12:50,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 48/135 [07:01<12:41,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.8099720478057861\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 48/135 [07:00<12:41,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 48/135 [07:01<12:41,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 48/135 [07:00<12:41,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.6434630155563354\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 49/135 [07:09<12:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 49/135 [07:09<12:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 49/135 [07:09<12:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 49/135 [07:10<12:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.6161194443702698\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 50/135 [07:18<12:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 50/135 [07:18<12:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 50/135 [07:18<12:23,  8.74s/it]#015Training Epoch0:  37%|#033[34m███▋      #033[0m| 50/135 [07:19<12:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 51/135 [07:27<12:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 51/135 [07:26<12:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 51/135 [07:28<12:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.6775678396224976\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 51/135 [07:26<12:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 52/135 [07:36<12:05,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.5760949850082397\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 52/135 [07:35<12:05,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 52/135 [07:35<12:05,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 52/135 [07:35<12:05,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 0.6329063177108765\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 53/135 [07:44<11:59,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 53/135 [07:44<11:59,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 53/135 [07:45<11:59,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 53/135 [07:44<11:59,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 54/135 [07:54<11:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 54/135 [07:53<11:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 54/135 [07:53<11:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.8387918472290039\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 54/135 [07:53<11:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 55/135 [08:02<11:43,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 55/135 [08:03<11:43,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 55/135 [08:02<11:43,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.5931142568588257\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 55/135 [08:01<11:43,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.5597504377365112\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 56/135 [08:10<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 56/135 [08:12<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 56/135 [08:11<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 56/135 [08:10<11:33,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 57/135 [08:19<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 57/135 [08:20<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 0.62925785779953\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 57/135 [08:19<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 57/135 [08:19<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 0.5408278107643127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 58/135 [08:28<11:13,  8.75s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 58/135 [08:29<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 58/135 [08:28<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 58/135 [08:28<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.0217761993408203\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 59/135 [08:36<11:05,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 59/135 [08:38<11:05,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 59/135 [08:36<11:05,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 59/135 [08:37<11:05,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.5004748106002808\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 60/135 [08:45<10:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 60/135 [08:45<10:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 60/135 [08:46<10:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 60/135 [08:47<10:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 61/135 [08:54<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.0404438972473145\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 61/135 [08:54<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 61/135 [08:54<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 61/135 [08:55<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.7873325943946838\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 62/135 [09:03<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 62/135 [09:03<10:37,  8.74s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 62/135 [09:04<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 62/135 [09:03<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.7374187707901001\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 63/135 [09:12<10:31,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 63/135 [09:11<10:31,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 63/135 [09:12<10:31,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 63/135 [09:13<10:31,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 64/135 [09:22<10:22,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.570105791091919\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 64/135 [09:20<10:22,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 64/135 [09:20<10:22,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 64/135 [09:21<10:22,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 65/135 [09:30<10:13,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 65/135 [09:29<10:13,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.705792248249054\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 65/135 [09:29<10:13,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 65/135 [09:29<10:13,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 0.6969019770622253\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 66/135 [09:38<10:06,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 66/135 [09:38<10:06,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 66/135 [09:38<10:06,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 66/135 [09:39<10:06,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 67/135 [09:48<09:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 67/135 [09:47<09:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.49927714467048645\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 67/135 [09:47<09:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 67/135 [09:47<09:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 68/135 [09:56<09:47,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 68/135 [09:55<09:47,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 68/135 [09:57<09:47,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.6378130316734314\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 68/135 [09:55<09:47,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 69/135 [10:04<09:37,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 69/135 [10:04<09:37,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.5593183040618896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 69/135 [10:04<09:37,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 69/135 [10:05<09:37,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 70/135 [10:13<09:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.5004302263259888\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 70/135 [10:13<09:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 70/135 [10:13<09:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 70/135 [10:14<09:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 0.580974817276001\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 71/135 [10:21<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 71/135 [10:22<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 71/135 [10:23<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 71/135 [10:22<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 72/135 [10:32<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 72/135 [10:31<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.506155788898468\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 72/135 [10:30<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 72/135 [10:30<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 73/135 [10:39<09:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 73/135 [10:40<09:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 73/135 [10:39<09:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.5433278679847717\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 73/135 [10:39<09:02,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 74/135 [10:49<08:53,  8.75s/it]#015Training Epoch0:  55%|#033[34m█████▍    #033[0m| 74/135 [10:48<08:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.6628289818763733\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 74/135 [10:48<08:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 74/135 [10:48<08:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 75/135 [10:58<08:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 75/135 [10:57<08:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 75/135 [10:57<08:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 0.7175914645195007\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 75/135 [10:56<08:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 76/135 [11:06<08:38,  8.79s/it]#015Training Epoch0:  56%|#033[34m█████▋    #033[0m| 76/135 [11:05<08:38,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.7251070141792297\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 76/135 [11:05<08:38,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 76/135 [11:07<08:38,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 77/135 [11:14<08:28,  8.77s/it]#015Training Epoch0:  57%|#033[34m█████▋    #033[0m| 77/135 [11:15<08:28,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.8226479887962341\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 77/135 [11:14<08:28,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 77/135 [11:16<08:28,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 78/135 [11:24<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 0.41063225269317627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 78/135 [11:23<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 78/135 [11:23<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 78/135 [11:23<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 79/135 [11:33<08:10,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 79/135 [11:32<08:10,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 0.7862761616706848\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 79/135 [11:32<08:10,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▊    #033[0m| 79/135 [11:32<08:10,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 80/135 [11:41<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 80/135 [11:40<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 80/135 [11:42<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.5398159623146057\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 80/135 [11:40<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 81/135 [11:49<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 81/135 [11:49<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 0.5050135254859924\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 81/135 [11:49<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 81/135 [11:50<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 82/135 [11:59<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 82/135 [11:58<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 82/135 [11:58<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 0.7705278992652893\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 82/135 [11:58<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 83/135 [12:07<07:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 83/135 [12:07<07:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 83/135 [12:08<07:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.684785008430481\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 83/135 [12:06<07:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 84/135 [12:17<07:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 84/135 [12:15<07:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.6722327470779419\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 84/135 [12:15<07:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 84/135 [12:16<07:27,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 85/135 [12:25<07:18,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 85/135 [12:25<07:18,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 85/135 [12:24<07:18,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.5248802900314331\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 85/135 [12:24<07:18,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 86/135 [12:33<07:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 86/135 [12:33<07:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 0.7445923089981079\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 86/135 [12:33<07:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 86/135 [12:34<07:09,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 87/135 [12:42<07:01,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.47822779417037964\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 87/135 [12:43<07:01,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 87/135 [12:42<07:01,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 87/135 [12:42<07:01,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 88/135 [12:50<06:52,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 88/135 [12:52<06:52,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 88/135 [12:51<06:52,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.7151714563369751\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 88/135 [12:50<06:52,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 89/135 [12:59<06:43,  8.76s/it]#015Training Epoch0:  66%|#033[34m██████▌   #033[0m| 89/135 [13:01<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.5673708915710449\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 89/135 [12:59<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 89/135 [13:00<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 90/135 [13:08<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 90/135 [13:08<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 90/135 [13:09<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.6439829468727112\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 90/135 [13:08<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 91/135 [13:17<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 91/135 [13:17<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 91/135 [13:18<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.5865366458892822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 91/135 [13:17<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 92/135 [13:27<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 0.523607611656189\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 92/135 [13:25<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 92/135 [13:25<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 92/135 [13:26<06:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 93/135 [13:36<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.6330453753471375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 93/135 [13:34<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 93/135 [13:34<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 93/135 [13:35<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 94/135 [13:43<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.5348782539367676\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 94/135 [13:43<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 94/135 [13:43<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 94/135 [13:44<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.6349557638168335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 95/135 [13:53<05:50,  8.77s/it]#015Training Epoch0:  70%|#033[34m███████   #033[0m| 95/135 [13:52<05:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 95/135 [13:52<05:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 95/135 [13:52<05:50,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 96/135 [14:01<05:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.6922279596328735\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 96/135 [14:00<05:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 96/135 [14:00<05:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 96/135 [14:02<05:41,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 97/135 [14:11<05:32,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 97/135 [14:10<05:32,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 97/135 [14:09<05:32,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.6890630125999451\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 97/135 [14:09<05:32,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 98/135 [14:19<05:24,  8.78s/it]#015Training Epoch0:  73%|#033[34m███████▎  #033[0m| 98/135 [14:18<05:24,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 98/135 [14:18<05:24,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 0.6248250007629395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 98/135 [14:18<05:24,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 99/135 [14:27<05:16,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 99/135 [14:28<05:16,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 0.7862832546234131\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 99/135 [14:27<05:16,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 99/135 [14:27<05:16,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.7197193503379822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 100/135 [14:36<05:07,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 100/135 [14:36<05:07,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 100/135 [14:37<05:07,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 100/135 [14:36<05:07,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 101/135 [14:46<04:57,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 101/135 [14:45<04:58,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 101/135 [14:44<04:58,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 0.49827316403388977\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 101/135 [14:44<04:58,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 102/135 [14:54<04:48,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 102/135 [14:53<04:48,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 0.7015801668167114\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 102/135 [14:53<04:48,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 102/135 [14:53<04:48,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 103/135 [15:02<04:39,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 103/135 [15:02<04:39,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 0.6734302043914795\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 103/135 [15:02<04:39,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 103/135 [15:03<04:39,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 104/135 [15:11<04:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 104/135 [15:12<04:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 104/135 [15:11<04:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 0.49171093106269836\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 104/135 [15:10<04:31,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 0.5702556371688843\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 105/135 [15:19<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 105/135 [15:20<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 105/135 [15:21<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 105/135 [15:19<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.4703785479068756\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 106/135 [15:28<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 106/135 [15:28<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 106/135 [15:28<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▊  #033[0m| 106/135 [15:29<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.43331819772720337\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 107/135 [15:37<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 107/135 [15:38<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 107/135 [15:37<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 107/135 [15:37<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 108/135 [15:47<03:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.6119489669799805\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 108/135 [15:45<03:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 108/135 [15:46<03:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 108/135 [15:46<03:56,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 109/135 [15:56<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 109/135 [15:54<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 109/135 [15:55<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.5496556758880615\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 109/135 [15:54<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 110/135 [16:03<03:39,  8.77s/it]#015Training Epoch0:  81%|#033[34m████████▏ #033[0m| 110/135 [16:04<03:39,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 0.5327841639518738\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 110/135 [16:03<03:39,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 110/135 [16:03<03:39,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 111/135 [16:13<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.5822473168373108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 111/135 [16:12<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 111/135 [16:12<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 111/135 [16:12<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 112/135 [16:22<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 112/135 [16:21<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 112/135 [16:21<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 0.704568088054657\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 112/135 [16:20<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 113/135 [16:31<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 113/135 [16:30<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 113/135 [16:29<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 0.8939056992530823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 113/135 [16:29<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 114/135 [16:38<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 114/135 [16:38<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 114/135 [16:39<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 0.6623179316520691\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 114/135 [16:38<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 115/135 [16:47<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 0.47982460260391235\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 115/135 [16:47<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 115/135 [16:47<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 115/135 [16:48<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 116/135 [16:57<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 116/135 [16:56<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.566754937171936\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 116/135 [16:55<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 116/135 [16:56<02:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 117/135 [17:06<02:37,  8.74s/it]#015Training Epoch0:  87%|#033[34m████████▋ #033[0m| 117/135 [17:05<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 0.9092655777931213\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 117/135 [17:04<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 117/135 [17:04<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 118/135 [17:14<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.505027711391449\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 118/135 [17:13<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 118/135 [17:13<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 118/135 [17:13<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 119/135 [17:23<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 0.6998178362846375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 119/135 [17:22<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 119/135 [17:22<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 119/135 [17:22<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 120/135 [17:32<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.6137351989746094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 120/135 [17:30<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 120/135 [17:31<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 120/135 [17:30<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 121/135 [17:41<02:02,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.5052788853645325\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 121/135 [17:40<02:02,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 121/135 [17:39<02:02,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m████████▉ #033[0m| 121/135 [17:39<02:02,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 122/135 [17:49<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 122/135 [17:48<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 0.6771602630615234\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 122/135 [17:48<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 122/135 [17:48<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 0.8889145255088806\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 123/135 [17:57<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 123/135 [17:58<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 123/135 [17:57<01:45,  8.76s/it]#015Training Epoch0:  91%|#033[34m█████████ #033[0m| 123/135 [17:57<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 124/135 [18:07<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 0.4081791937351227\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 124/135 [18:05<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 124/135 [18:06<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 124/135 [18:06<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 125/135 [18:15<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 125/135 [18:14<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 0.42755353450775146\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 125/135 [18:14<01:27,  8.74s/it]#015Training Epoch0:  93%|#033[34m█████████▎#033[0m| 125/135 [18:16<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 126/135 [18:24<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 126/135 [18:23<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 126/135 [18:23<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.6426351070404053\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 126/135 [18:23<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 127/135 [18:33<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 127/135 [18:32<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 127/135 [18:32<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 0.5237849354743958\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 127/135 [18:32<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 128/135 [18:41<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 128/135 [18:41<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.586417555809021\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 128/135 [18:41<01:01,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 128/135 [18:42<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.6347320079803467\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 129/135 [18:49<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 129/135 [18:51<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 129/135 [18:50<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 129/135 [18:49<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 130/135 [18:58<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 0.767922580242157\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 130/135 [18:58<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 130/135 [18:59<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 130/135 [18:58<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 131/135 [19:07<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 0.4445285499095917\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 131/135 [19:07<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 131/135 [19:08<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 131/135 [19:07<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.7549232244491577\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 132/135 [19:16<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 132/135 [19:17<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 132/135 [19:16<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 132/135 [19:16<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▊#033[0m| 133/135 [19:26<00:17,  8.76s/it]#015Training Epoch0:  99%|#033[34m█████████▊#033[0m| 133/135 [19:25<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.6285684704780579\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▊#033[0m| 133/135 [19:24<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▊#033[0m| 133/135 [19:24<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 134/135 [19:35<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 134/135 [19:34<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 134/135 [19:33<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.6003308296203613\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 134/135 [19:33<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.8217244148254395\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:43<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.74s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:43<00:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 135/135 [19:42<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/34 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/34 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/34 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/34 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/34 [00:04<02:20,  4.24s/it]#015evaluating Epoch:   3%|#033[32m▎         #033[0m| 1/34 [00:04<02:20,  4.24s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/34 [00:04<02:19,  4.24s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 1/34 [00:04<02:19,  4.24s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/34 [00:07<02:05,  3.93s/it]#015evaluating Epoch:   6%|#033[32m▌         #033[0m| 2/34 [00:07<02:05,  3.93s/it]#015evaluating Epoch:   6%|#033[32m▌         #033[0m| 2/34 [00:07<02:05,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m▌         #033[0m| 2/34 [00:07<02:05,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 3/34 [00:11<01:58,  3.83s/it]#015evaluating Epoch:   9%|#033[32m▉         #033[0m| 3/34 [00:11<01:58,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 3/34 [00:11<01:58,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 3/34 [00:11<01:58,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 4/34 [00:15<01:53,  3.79s/it]#015evaluating Epoch:  12%|#033[32m█▏        #033[0m| 4/34 [00:15<01:53,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 4/34 [00:15<01:53,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 4/34 [00:15<01:53,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 5/34 [00:19<01:49,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 5/34 [00:19<01:49,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 5/34 [00:19<01:49,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 5/34 [00:19<01:49,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 6/34 [00:22<01:45,  3.76s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 6/34 [00:22<01:45,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 6/34 [00:22<01:45,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 6/34 [00:22<01:45,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 7/34 [00:26<01:41,  3.75s/it]#015evaluating Epoch:  21%|#033[32m██        #033[0m| 7/34 [00:26<01:41,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 7/34 [00:26<01:41,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 7/34 [00:26<01:41,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 8/34 [00:30<01:37,  3.75s/it]#015evaluating Epoch:  24%|#033[32m██▎       #033[0m| 8/34 [00:30<01:37,  3.75s/it]#015evaluating Epoch:  24%|#033[32m██▎       #033[0m| 8/34 [00:30<01:37,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▎       #033[0m| 8/34 [00:30<01:37,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 9/34 [00:34<01:33,  3.74s/it]#015evaluating Epoch:  26%|#033[32m██▋       #033[0m| 9/34 [00:34<01:33,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▋       #033[0m| 9/34 [00:34<01:33,  3.74s/it]#015evaluating Epoch:  26%|#033[32m██▋       #033[0m| 9/34 [00:34<01:33,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 10/34 [00:37<01:29,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 10/34 [00:37<01:29,  3.74s/it]#015evaluating Epoch:  29%|#033[32m██▉       #033[0m| 10/34 [00:37<01:29,  3.74s/it]#015evaluating Epoch:  29%|#033[32m██▉       #033[0m| 10/34 [00:37<01:29,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 11/34 [00:41<01:25,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 11/34 [00:41<01:25,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 11/34 [00:41<01:25,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 11/34 [00:41<01:25,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 12/34 [00:45<01:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 12/34 [00:45<01:22,  3.73s/it]#015evaluating Epoch:  35%|#033[32m███▌      #033[0m| 12/34 [00:45<01:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▌      #033[0m| 12/34 [00:45<01:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 13/34 [00:48<01:18,  3.72s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 13/34 [00:48<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 13/34 [00:48<01:18,  3.72s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 13/34 [00:48<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 14/34 [00:52<01:14,  3.72s/it]#015evaluating Epoch:  41%|#033[32m████      #033[0m| 14/34 [00:52<01:14,  3.72s/it]#015evaluating Epoch:  41%|#033[32m████      #033[0m| 14/34 [00:52<01:14,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 14/34 [00:52<01:14,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 15/34 [00:56<01:10,  3.71s/it]#015evaluating Epoch:  44%|#033[32m████▍     #033[0m| 15/34 [00:56<01:10,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 15/34 [00:56<01:10,  3.71s/it]#015evaluating Epoch:  44%|#033[32m████▍     #033[0m| 15/34 [00:56<01:10,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 16/34 [01:00<01:06,  3.71s/it]#015evaluating Epoch:  47%|#033[32m████▋     #033[0m| 16/34 [01:00<01:06,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 16/34 [01:00<01:06,  3.71s/it]#015evaluating Epoch:  47%|#033[32m████▋     #033[0m| 16/34 [01:00<01:06,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 17/34 [01:03<01:02,  3.70s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 17/34 [01:03<01:02,  3.70s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 17/34 [01:03<01:02,  3.70s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 17/34 [01:03<01:02,  3.70s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 18/34 [01:07<00:59,  3.71s/it]#015evaluating Epoch:  53%|#033[32m█████▎    #033[0m| 18/34 [01:07<00:59,  3.71s/it]#015evaluating Epoch:  53%|#033[32m█████▎    #033[0m| 18/34 [01:07<00:59,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 18/34 [01:07<00:59,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 19/34 [01:11<00:55,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 19/34 [01:11<00:55,  3.71s/it]#015evaluating Epoch:  56%|#033[32m█████▌    #033[0m| 19/34 [01:11<00:55,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 19/34 [01:11<00:55,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 20/34 [01:14<00:51,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 20/34 [01:14<00:51,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 20/34 [01:14<00:51,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 20/34 [01:14<00:51,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 21/34 [01:18<00:48,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 21/34 [01:18<00:48,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 21/34 [01:18<00:48,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 21/34 [01:18<00:48,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 22/34 [01:22<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 22/34 [01:22<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 22/34 [01:22<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▍   #033[0m| 22/34 [01:22<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 23/34 [01:26<00:40,  3.72s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 23/34 [01:26<00:40,  3.72s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 23/34 [01:26<00:40,  3.72s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 23/34 [01:26<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 24/34 [01:29<00:37,  3.72s/it]#015evaluating Epoch:  71%|#033[32m███████   #033[0m| 24/34 [01:29<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 24/34 [01:29<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 24/34 [01:29<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 25/34 [01:33<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 25/34 [01:33<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 25/34 [01:33<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▎  #033[0m| 25/34 [01:33<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 26/34 [01:37<00:29,  3.72s/it]#015evaluating Epoch:  76%|#033[32m███████▋  #033[0m| 26/34 [01:37<00:29,  3.72s/it]#015evaluating Epoch:  76%|#033[32m███████▋  #033[0m| 26/34 [01:37<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▋  #033[0m| 26/34 [01:37<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 27/34 [01:41<00:26,  3.74s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 27/34 [01:41<00:26,  3.74s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 27/34 [01:41<00:26,  3.74s/it]#015evaluating Epoch:  79%|#033[32m███████▉  #033[0m| 27/34 [01:41<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 28/34 [01:44<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 28/34 [01:44<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 28/34 [01:44<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 28/34 [01:44<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 29/34 [01:48<00:18,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 29/34 [01:48<00:18,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 29/34 [01:48<00:18,  3.75s/it]#015evaluating Epoch:  85%|#033[32m████████▌ #033[0m| 29/34 [01:48<00:18,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 30/34 [01:52<00:14,  3.74s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 30/34 [01:52<00:14,  3.74s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 30/34 [01:52<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 30/34 [01:52<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 31/34 [01:55<00:11,  3.74s/it]#015evaluating Epoch:  91%|#033[32m█████████ #033[0m| 31/34 [01:55<00:11,  3.74s/it]#015evaluating Epoch:  91%|#033[32m█████████ #033[0m| 31/34 [01:55<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 31/34 [01:55<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 32/34 [01:59<00:07,  3.73s/it]#015evaluating Epoch:  94%|#033[32m█████████▍#033[0m| 32/34 [01:59<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 32/34 [01:59<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m█████████▍#033[0m| 32/34 [01:59<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 33/34 [02:03<00:03,  3.73s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 33/34 [02:03<00:03,  3.73s/it]#015evaluating Epoch:  97%|#033[32m█████████▋#033[0m| 33/34 [02:03<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 33/34 [02:03<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.72s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.74s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 34/34 [02:07<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(1.8579, device='cuda:0') eval_epoch_loss=tensor(0.6194, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 0.6194344758987427\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=2.0130, train_epoch_loss=0.6996, epcoh time 1182.5596492949999s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 2.0129501819610596\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 0.699601411819458\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 1.8578771352767944\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 0.6194344758987427\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 1182.5596492949999\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.676680067999996\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.84it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\n",
      "2024-07-19 14:29:19 Uploading - Uploading generated training model\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:29:14,412 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:29:14,412 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-07-19 14:29:14,413 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-07-19 14:30:03 Completed - Training job completed\n",
      "Training seconds: 1810\n",
      "Billable seconds: 1810\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-8b-instruct\", \"*\"\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"false\"},  # Please change {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.12xlarge\",  # For Llama-3-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(\n",
    "    instruction_tuned=\"True\", epoch=\"2\", max_input_length=\"1024\", chat_dataset=\"False\"\n",
    ")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the LLama 3 8B Fine-tuned Model \n",
    "\n",
    "In this section, we will evaluate the performance of the fine-tuned LLaMA 3 8B model to determine how well it has adapted to the specific tasks for which it was trained. Testing involves comparing the model's responses to a set of predefined questions or tasks against the baseline performance of the original, pre-trained model. This process helps us understand the improvements achieved through distillation by fine-tuning and identify any remaining areas for enhancement. By systematically examining the model's outputs, we can ensure that the fine-tuning process has effectively tailored the model to meet our specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-07-19 14:34:50,076] p54 {model.py:201} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-07-19 14:34:50,165] p54 {session.py:3872} INFO - Creating model with name: meta-textgeneration-llama-3-8b-instruct-2024-07-19-14-34-50-079\n",
      "[2024-07-19 14:34:50,838] p54 {session.py:5632} INFO - Creating endpoint-config with name meta-textgeneration-llama-3-8b-instruct-2024-07-19-14-34-50-076\n",
      "[2024-07-19 14:34:51,179] p54 {session.py:4478} INFO - Creating endpoint with name meta-textgeneration-llama-3-8b-instruct-2024-07-19-14-34-50-076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with fine-tuned LLaMA 3 8B model...\n",
      "\n",
      "Running inference with LLaMA 3 8B model...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Question</th>\n",
       "        <th>Correct Answer</th>\n",
       "        <th>Fine-tuned LLaMA 3 8B Output</th>\n",
       "        <th>LLaMA 3 8B Output</th>\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "        <td>Two friends plan to walk along a 43-km trail, starting at opposite ends of the trail at the same time. If Friend P's rate is 15% faster than Friend Q's, how many kilometers will Friend P have walked when they pass each other?</td>\n",
       "        <td>23</td>\n",
       "        <td>Let's say Friend Q's rate is x km/hr. Then Friend P's rate is x + 15% of x = 1.15x km/hr. Since they walk for the same amount of time, the distance they cover is the same. So, x * t = 1.15x * t, where t is the time they walk. So, x = 1.15x. So, x = 0. Therefore, x = 0. So, the distance covered by Friend P when they pass each other is 0. The answer is 0.</td>\n",
       "        <td>Let's say Friend Q's rate is x. Then Friend P's rate is 1.15x. The total distance is 43 km. So the time it takes for them to meet is 43 / (x + 1.15x) = 43 / 2.15x. The distance Friend P will have walked is 1.15x * (43 / 2.15x) = 20.5. The answer is 20.5.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>In the coordinate plane, points (x, 1) and (5, y) are on line k. If line k passes through the origin and has slope 1/5, then what are the values of x and y respectively?</td>\n",
       "        <td>5 and 1</td>\n",
       "        <td>## Step 1:  Since the line passes through the origin, we can write the equation of the line as y = (1/5)x.  Step 2:  We are given that the point (x, 1) is on the line, so we can substitute x for x and 1 for y in the equation of the line to get 1 = (1/5)x.  Step 3:  Solving for x, we get x = 5.  Step 4:  We are also given that the point (5, y) is on the line, so we can substitute 5 for x and y for y in the equation of the line to get y = (1/5)(5).  Step 5:  Solving for y, we get y = 1.  Step 6:  Therefore, the values of x and y are 5 and 1, respectively.</td>\n",
       "        <td>The slope of the line is 1/5, so the slope between the points (0,0) and (x,1) is 1/5.  This means that\n",
       "\n",
       "$$\\frac{1-0}{x-0}=\\frac{1}{5}\\Rightarrow x=5.$$Similarly, the slope between the points (5,y) and (0,0) is 1/5, so\n",
       "\n",
       "$$\\frac{y-0}{5-0}=\\frac{1}{5}\\Rightarrow y=1.$$Therefore, the values of x and y are 5 and 1, respectively.\n",
       "Final Answer: The final answer is 5 and 1. I hope it is correct.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>For all numbers p and q, the operation @ is defined by p@q = p^2 - pq. If xy ≠ 0, then which of the following can be equal to zero?\n",
       "I. x@y\n",
       "II. (xy)@y\n",
       "III. x@(x + y)</td>\n",
       "        <td>I and II</td>\n",
       "        <td>## Step 1:  Define the operation @ for p and q as p@q = p^2 - pq.\\n\\n## Step 2:  Given that xy ≠ 0, we need to find which of the options can be equal to zero.\\n\\n## Step 3:  Option I: x@y = x^2 - xy = x(x - y). Since xy ≠ 0, x ≠ 0 or y ≠ 0. If x = 0, then x@y = 0. If y = 0, then x@y = x^2 ≠ 0. Therefore, x@y can be equal to zero.\\n\\n## Step 4:  Option II: (xy)@y = (xy)^2 - xy(y) = x^2y^2 - xy^2 = x^2y(y - 1). Since xy ≠ 0, y ≠ 0 or y ≠ 1. If y = 1, then (xy)@y = x^2y - xy = x^2 - x = x(x - 1). If y ≠ 1, then (xy)@y = 0. Therefore, (xy)@y can be equal to zero.\\n\\n## Step 5:  Option III: x@(x + y) = x^2 - x(x + y) = x^2 - x^2 - xy = -xy. Since xy ≠ 0, -xy ≠ 0. Therefore, x@(x + y) cannot be equal to zero.\\n\\nThe final answer is: I and II.</td>\n",
       "        <td>A nice algebraic puzzle!\n",
       "\n",
       "Let's analyze each option:\n",
       "\n",
       "I. x@y = x^2 - xy = x(x - y) = 0\n",
       "\n",
       "This can be equal to zero if x = 0 or x = y. However, we are given that xy ≠ 0, so x cannot be zero. Therefore, x@y cannot be equal to zero.\n",
       "\n",
       "II. (xy)@y = (xy)^2 - (xy)y = x^2y^2 - xy^2 = x^2y(y - 1) = 0\n",
       "\n",
       "This can be equal to zero if x = 0, y = 1, or y = 1. However, we are given that xy ≠ 0, so y cannot be 1. Therefore, (xy)@y cannot be equal to zero.\n",
       "\n",
       "III. x@(x + y) = x^2 - x(x + y) = x^2 - x^2 - xy = -xy\n",
       "\n",
       "This can be equal to zero if xy = 0, which is allowed since xy ≠ 0 is not a given condition.\n",
       "\n",
       "Therefore, the correct answer is III. x@(x + y).</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>Carl is facing very difficult financial times and can only pay the interest on a $10,000 loan he has taken. The bank charges him a quarterly compound rate of 4%. What is the approximate interest he pays annually?</td>\n",
       "        <td>$1600</td>\n",
       "        <td>The interest rate is 4% per quarter. So the interest rate per year is 4 * 4 = 16%. 16% of 10,000 is 1600. The answer is 1600.</td>\n",
       "        <td>The interest rate is 4% per quarter. So the interest rate per year is 4 * 4 = 16%. 16% of 10,000 is 1600. The answer is 1600.</td>\n",
       "    </tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract 4 questions, options, and their correct answers from the dataset\n",
    "num_questions = 4\n",
    "questions = dataset['train'].select(range(num_questions))['question']\n",
    "options = dataset['train'].select(range(num_questions))['options']\n",
    "correct_answers = dataset['train'].select(range(num_questions))['correct']\n",
    "\n",
    "# Map the correct answer letter to the actual answer\n",
    "def get_correct_answer(options, correct_letter):\n",
    "    for option in options:\n",
    "        if option.startswith(correct_letter):\n",
    "            return option.split(')', 1)[1].strip()\n",
    "    return None\n",
    "\n",
    "actual_correct_answers = [get_correct_answer(opt, correct) for opt, correct in zip(options, correct_answers)]\n",
    "\n",
    "# Define the inference parameters\n",
    "params = {\n",
    "    \"max_new_tokens\": 512,  # Increase this value to allow longer responses\n",
    "    \"top_p\": 0.9,  # Adjust to introduce variability\n",
    "    \"temperature\": 0.0,  # Adjust to introduce variability\n",
    "    \"details\": True,\n",
    "    \"stop\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Define the example payloads list\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    for question in questions\n",
    "]\n",
    "\n",
    "# Function to run inference and collect results\n",
    "def run_inference(predictor, example_payloads):\n",
    "    results = []\n",
    "    for payload in example_payloads:\n",
    "        response = predictor.predict(payload)\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        generated_text = response[\"generated_text\"].strip()\n",
    "        \n",
    "        # Check if the response is truncated\n",
    "        if generated_text.endswith(\"...\"):\n",
    "            generated_text += \" [TRUNCATED]\"\n",
    "        \n",
    "        results.append(generated_text)\n",
    "    return results\n",
    "\n",
    "# Run inference with both models\n",
    "print(\"Running inference with fine-tuned LLama 3 8B model...\\n\")\n",
    "results_fine_tuned_8b = run_inference(finetuned_predictor, example_payloads)\n",
    "\n",
    "print(\"Running inference with LLama 3 8B model...\\n\")\n",
    "results_8b = run_inference(llama_3_8b_predictor, example_payloads)\n",
    "\n",
    "\n",
    "# Create a table of the outputs using HTML\n",
    "table_html = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Question</th>\n",
    "        <th>Correct Answer</th>\n",
    "        <th>Fine-tuned LLama 3 8B Output</th>\n",
    "        <th>LLama 3 8B Output</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i in range(4):\n",
    "    table_html += f\"\"\"\n",
    "    <tr>\n",
    "        <td>{questions[i]}</td>\n",
    "        <td>{actual_correct_answers[i]}</td>\n",
    "        <td>{results_fine_tuned_8b[i]}</td>\n",
    "        <td>{results_8b[i]}</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "table_html += \"</table>\"\n",
    "\n",
    "# Display the table using HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully demonstrated the process of distillation by fine-tuning and evaluating the LLama 3 8B model using Amazon SageMaker JumpStart. By leveraging the advanced capabilities of the LLama 3.1 405B model, we generated high-quality synthetic data that served as a foundation for fine-tuning the smaller 8B model. This approach allowed us to enhance the performance of the Llama 3 8B model, tailoring it to specific domain tasks and improving its accuracy and effectiveness.\n",
    "\n",
    "### Key Steps Accomplished:\n",
    "1. **Dataset Exploration**: We explored a sample dataset to understand its structure and contents, preparing it for use in model training and evaluation.\n",
    "2. **Data Generation with LLama 3.1 405B**: Utilizing the LLama 3.1 405B model, we generated synthetic data that provided high-quality responses to domain-specific prompts.\n",
    "3. **Distillation by Fine-Tuning LLama 3 8B**: We fine-tuned the LLaMA 3 8B model using the synthetic data, adapting it to better handle specific tasks and improving its overall performance.\n",
    "4. **Model Testing**: We tested the fine-tuned model against a set of evaluation questions, comparing its responses to those of the pre-trained model and assessing the improvements achieved through distillation by fine-tuning.\n",
    "\n",
    "### Results and Insights:\n",
    "- **Enhanced Performance**: The fine-tuned LLama 3 8B model demonstrated significant improvements in generating accurate and contextually relevant responses, showcasing the effectiveness of the fine-tuning process.\n",
    "- **Cost-Effective Adaptation**: By fine-tuning the smaller 8B model with data generated from the larger 405B model, we achieved high performance without the need for extensive computational resources, highlighting a cost-effective approach to model adaptation.\n",
    "- **Scalability and Flexibility**: The workflow outlined in this notebook can be scaled and adapted to various domains and tasks, providing a flexible framework for enhancing the capabilities of language models.\n",
    "\n",
    "### Future Work:\n",
    "- **Further Fine-Tuning**: Additional fine-tuning with more diverse and extensive datasets can further improve the model's performance and adaptability to different domains.\n",
    "- **Real-World Applications**: Deploying the fine-tuned model in real-world applications such as customer support, content generation, and domain-specific research can provide valuable insights and practical benefits.\n",
    "- **Continuous Evaluation**: Ongoing evaluation and monitoring of the model's performance will ensure that it remains effective and relevant as new data and requirements emerge.\n",
    "\n",
    "In conclusion, this notebook has provided a comprehensive guide to generate synthetic data using Llama 3.1 405B and use the generated data for distillation by fine-tuning and evaluating the LLama 3 8B model, demonstrating the potential of using advanced language models to address specific domain needs. By the steps outlined, practitioners can enhance their models' performance, achieve cost-effective adaptations, and unlock new possibilities in natural language processing and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-24 16:16:47,497] p3513 {session.py:4608} INFO - Deleting endpoint configuration with name: meta-textgeneration-llama-3-8b-instruct-2024-07-24-16-03-13-388\n",
      "[2024-07-24 16:16:47,638] p3513 {session.py:4598} INFO - Deleting endpoint with name: meta-textgeneration-llama-3-8b-instruct-2024-07-24-16-03-13-388\n"
     ]
    }
   ],
   "source": [
    "# llama_3_8b_predictor.delete_predictor()\n",
    "\n",
    "# llama_3_1_405b_predictor.delete_predictor()\n",
    "\n",
    "# finetuned_predictor.delete_predictor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
