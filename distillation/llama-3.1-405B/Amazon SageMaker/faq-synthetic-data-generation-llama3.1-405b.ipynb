{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Llama 3.1 405B for Summarizing and Preparing Instruction Fine-Tuned Dataset\n",
    "\n",
    "In this notebook, we will walk you through the process of utilizing a larger language model (LLM) like Llama 3.1 405B, Meta AI's latest and most advanced model, to create a dataset for instruction fine-tuning. This dataset will be used to perform distillation by fine-tuning a smaller model, such as Llama 3 8B.\n",
    "\n",
    "By leveraging the capabilities of Llama 3.1 405B, we can generate high-quality, concise training data that enhances the performance of smaller models. This approach is particularly useful for tasks that require detailed and specific instructions.\n",
    "\n",
    "Before we begin, ensure that you have access to Llama 3.1 405B, which is now available on Amazon SageMaker Jumpstart. You can find the dataset we will be using [here](https://huggingface.co/datasets/deepmind/aqua_rat).\n",
    "\n",
    "You can run the notebook on an Amazon SageMaker Studio notebook, or a SageMaker notebook instance without manually setting your aws credentials.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Amazon SageMaker JumpStart\n",
    "\n",
    "![Alt text](imgs/jumpstart-overview-img1.png \"SageMaker JumpStart Overview\")\n",
    "\n",
    "**Amazon SageMaker JumpStart** is a powerful feature within Amazon SageMaker designed to help you quickly get started with LLMs by providing access to a wide range of pre-trained foundation models (FM). We'll be using this for deploying and fine tuning our models.\n",
    "\n",
    "Key Features\n",
    "- **Pre-trained Models**: SageMaker JumpStart provides a variety of pre-trained models from different model providers (Llama, Mistral, Cohere, Stablity) for different problem types, enabling you to start your machine learning projects without the need to build models from scratch.\n",
    "\n",
    "- **Training and Tuning**: With a few clicks, you can train and fine-tune these models to better fit your specific data and use case before deploying them.\n",
    "\n",
    "- **Solution Templates**: JumpStart offers solution templates that automatically set up the necessary infrastructure for common use cases, streamlining the deployment process.\n",
    "\n",
    "### Llama 3.1 405B Model\n",
    "\n",
    "Llama 3.1 405B is the largest model in the family of Llama 3.1 models. Llama 3.1 model family is a collection of pre-trained and instruction-tuned LLMs which already includes 8B and 70B parameter sizes. Llama 3.1 405B comes with new capabilities including multi-language support and a 128k context window. These models are stronger overall capabilities and are ideal for content creation, conversational AI, language understanding, research and development (R&D), and enterprise applications.\n",
    "\n",
    "\n",
    "### Llama 3 8B Model\n",
    "\n",
    "LLama 3 8B is an LLM with 8 billion parameters designed to deliver high performance across a variety of tasks while maintaining cost efficiency. This model is particularly advantageous for developers and organizations looking to implement advanced AI capabilities without the need for extensive computational resources. LLaMA 3 8B is optimized for dialogue and other interactive applications, demonstrating strong performance in benchmarks such as MMLU, AGIEval, and CommonSenseQA, where it outperforms many open-source models of similar size. Its ability to run on more affordable hardware highlights its potential for cost-effective deployment in real-time applications like chatbots and customer support systems.\n",
    "\n",
    "\n",
    "\n",
    "### Prequisites\n",
    " In order to follow along in this notebook, you'll need access to the following:\n",
    "\n",
    " - An AWS account with SageMaker endpoint capacity for an ml.p4de instance type. You can find more information about how to request a service limit increase [here](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html).\n",
    "\n",
    " - An [AWS Identity and Access Management (IAM)](https://aws.amazon.com/iam/) role to access SageMaker. To learn more about how IAM works with SageMaker, refer to [Identity and Access Management for Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam.html).\n",
    " \n",
    " - Access to SageMaker Studio or a SageMaker notebook instance or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we perform the following high level steps: \n",
    "\n",
    "1. We deploy a `Llama3-8b instruct` model and generate inferences on a `deepmind/aqua_rat` dataset.\n",
    "\n",
    "1. Deploy and leverage the capabilities of the new `Llama 3.1 405B Model` to generate labels and corresponding data to be used to do distillation by fine-tuning`Llama3-8b instruct`\n",
    "\n",
    "1. Test the fine-tuned `Llama3-8b instruct` model and test the model against the same questions to showcase the increase in response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8434/2991053635.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    " \n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from botocore.config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use this function to call inference on our deployed models\n",
    "def run_inference(predictor, example_payloads):\n",
    "    for payload in example_payloads:\n",
    "        response = predictor.predict(payload)\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        print(\"Input:\\n\", payload[\"inputs\"], end=\"\\n\\n\")\n",
    "        print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration and Preparation\n",
    "\n",
    "In this section, we will explore a dataset from the Hugging Face Hub using the HF Datasets library. Hugging Face provides a vast collection of datasets for various tasks in natural language processing (NLP), computer vision, and audio processing. This exploration will help us understand the structure, features, and contents of the dataset, enabling us to prepare it for training and evaluation in our machine learning models. The [deepmind/aqua_rat](https://huggingface.co/datasets/deepmind/aqua_rat) dataset is a large-scale collection of approximately 100,000 algebraic word problems, each accompanied by a detailed natural language rationale explaining the solution process. This dataset is designed to train and evaluate models that not only generate the correct answer but also provide a step-by-step explanation, making it ideal for tasks requiring mathematical reasoning and natural language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, multidict, fsspec, frozenlist, async-timeout, aiohappyeyeballs, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "Successfully installed aiohappyeyeballs-2.3.4 aiohttp-3.10.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.24.5 multidict-6.0.5 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-06 06:59:03,859] p8434 {config.py:58} INFO - PyTorch version 2.2.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Examples in Each Split After Splitting:\n",
      "train: 43 examples\n",
      "validation: 5 examples\n",
      "test: 13 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = \"CShorten/CDC-COVID-FAQ\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Split the train dataset into train and test splits\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "train_test_split = dataset['train'].train_test_split(test_size=1 - split_ratio)\n",
    "\n",
    "# Optionally split the train set further into train and validation sets\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Create a new dataset dictionary with the new splits\n",
    "dataset = DatasetDict({\n",
    "    'train': train_validation_split['train'],\n",
    "    'validation': train_validation_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the number of examples in each split after splitting\n",
    "print(\"\\nNumber of Examples in Each Split After Splitting:\")\n",
    "for split in dataset.keys():\n",
    "    if dataset[split] is not None:\n",
    "        print(f\"{split}: {len(dataset[split])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CShorten/CDC-COVID-FAQ\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'category', 'question', 'answer'],\n",
      "        num_rows: 61\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset Features:\n",
      "{'Unnamed: 0': Value(dtype='int64', id=None), 'category': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None)}\n",
      "\n",
      "Sample Examples:\n",
      "{'Unnamed: 0': 0, 'category': 'COVID-19 Risk', 'question': 'Who is at risk for infection with SARS-CoV-2, the virus that causes COVID-19?', 'answer': 'Currently, those at greatest risk of infection are persons who have had prolonged, unprotected close contact (i.e., within 6 feet for 15 minutes or longer) with a patient with confirmed SARS-CoV-2 infection, regardless of whether the patient has symptoms. Persons frequently in congregate settings (e.g., homeless shelters, assisted living facilities, college or university dormitories) are at increased risk of acquiring infection because of the increased likelihood of close contact. Those who live in or have recently been to areas with sustained transmission may also be at higher risk of infection. All persons can reduce the risk to themselves and others by wearing a mask, practicing physical distancing, washing their hands often, and taking other prevention measures. For more information, see Risk Assessment and Your Health.'}\n",
      "{'Unnamed: 0': 1, 'category': 'COVID-19 Risk', 'question': 'Who is at risk for severe COVID-19?', 'answer': 'Among adults, the risk for severe illness from COVID-19 increases with age, with older adults at highest risk. Severe illness means that the person with COVID-19 may require hospitalization, intensive care, or a ventilator to help them breathe, or they may even die. People of any age with certain underlying medical conditions are also at increased risk for severe illness from SARS-CoV-2 infection.'}\n",
      "{'Unnamed: 0': 2, 'category': 'COVID-19 Risk', 'question': 'What is multisystem inflammatory syndrome in children (MIS-C) and who is at risk?', 'answer': 'CDC continues to investigate multisystem inflammatory syndrome in children (MIS-C) associated with COVID-19. Children and adolescents with MIS-C have presented with a persistent fever and a variety of signs and symptoms including involvement of multiple organs (e.g., cardiac, gastrointestinal, renal, hematologic, dermatologic, neurologic) and elevated inflammatory markers. CDC is collaborating with domestic and international partners to better understand this new syndrome, including how common it is and its risk factors. For more information, including a full case definition and how to report MIS-C to your health department, visit MIS-C Information for Healthcare Providers.'}\n",
      "\n",
      "Number of Examples in Each Split:\n",
      "train: 61 examples\n",
      "\n",
      "Number of Examples in Each Split After Splitting:\n",
      "train: 43 examples\n",
      "validation: 5 examples\n",
      "test: 13 examples\n",
      "\n",
      "First 20 Questions:\n",
      "1: A healthcare provider at our facility was recently diagnosed with COVID-19. What time period and criteria do we use to determine the patients, visitors, and other healthcare personnel (HCP) who might have been exposed to this individual while he/she was potentially infectious?\n",
      "2: What is multisystem inflammatory syndrome in children (MIS-C) and who is at risk?\n",
      "3: What should I do if I suspect a potential case of reinfection?\n",
      "4: Which procedures are considered aerosol generating procedures in healthcare settings?\n",
      "5: What detergents are used for routine environmental cleaning in healthcare settings?\n",
      "6: Are empiric antibiotics recommended for patients suspected of having COVID-19?\n",
      "7: What are no-touch devices or NTDs?\n",
      "8: Are there ways to audit the cleaning process?\n",
      "9: My hospital is experiencing a shortage of isolation gowns. To preserve our supply, can we stop using gowns for the care of patients with methicillin-resistant Staphylococcus aureus (MRSA) and other endemic multidrug-resistant oragnisms (MDROs), and Clostridioides difficile?\n",
      "10: What further evidence is needed to be reassured that persistent or recurrent shedding of SARS-CoV-2 RNA after recovery does not represent the presence of infectious virus?\n",
      "11: What should visitors use for source control (masks or respirators) when visiting healthcare facilities?\n",
      "12: How are environmental surfaces involved in the transmission of infections?\n",
      "13: Who is at risk for infection with SARS-CoV-2, the virus that causes COVID-19?\n",
      "14: If a previously infected person has clinically recovered but later experiences symptoms consistent with COVID-19, should the person be isolated again and tested for SARS-CoV-2?\n",
      "15: Do nonsteroidal anti-inflammatory drugs (NSAIDs) worsen the course of disease for people with COVID-19?\n",
      "16: What personal protective equipment (PPE) should be worn by individuals transporting patients with suspected or confirmed SARS-CoV-2 infection within a healthcare facility? For example, what PPE should be worn when transporting the patient to radiology for imaging that cannot be performed in the patient room?\n",
      "17: Are clinically recovered persons infectiouso to others if they test persistently or recurrently positive for SARS-CoV-2 RNA?\n",
      "18: What information is available about the use of electrostatic sprayers or foggers for the disinfection of rooms and surfaces in healthcare environments?\n",
      "19: Why does CDC continue to recommend respiratory protection equivalent or higher to the level provided by an N95 disposable filtering facepiece respirator for care of patients with known or suspected COVID-19?\n",
      "20: Can self-tests (also known as over-the-counter or home tests) be used when determining when healthcare personnel (HCP) with SARS-CoV-2 infection or higher-risk exposures may return to work?\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions from the datasets library with test split\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the deepmind/aqua_rat dataset from the Hugging Face Hub\n",
    "dataset_name = \"CShorten/CDC-COVID-FAQ\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(dataset)\n",
    "\n",
    "# Display the dataset's features\n",
    "print(\"\\nDataset Features:\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# Display a few examples from the dataset\n",
    "print(\"\\nSample Examples:\")\n",
    "for i in range(3):\n",
    "    print(dataset['train'][i])\n",
    "\n",
    "# Display the number of examples in each split\n",
    "print(\"\\nNumber of Examples in Each Split:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"{split}: {len(dataset[split])} examples\")\n",
    "\n",
    "# Split the train dataset into train and test splits\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "train_test_split = dataset['train'].train_test_split(test_size=1 - split_ratio)\n",
    "\n",
    "# Optionally split the train set further into train and validation sets\n",
    "train_validation_split = train_test_split['train'].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Create a new dataset dictionary with the new splits\n",
    "dataset = DatasetDict({\n",
    "    'train': train_validation_split['train'],\n",
    "    'validation': train_validation_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Display the number of examples in each split after splitting\n",
    "print(\"\\nNumber of Examples in Each Split After Splitting:\")\n",
    "for split in dataset.keys():\n",
    "    if dataset[split] is not None:\n",
    "        print(f\"{split}: {len(dataset[split])} examples\")\n",
    "\n",
    "# Extract 20 questions from the train split\n",
    "questions = dataset['train'].select(range(20))['question']\n",
    "\n",
    "# Display the first 20 questions\n",
    "print(\"\\nFirst 20 Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"{i+1}: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Llama 3 8B Instruct\n",
    "\n",
    "In this section, we will deploy the base, pre-trained LLama 3 8B model and test it against a subset of our dataset to evaluate its responses compared to the larger LLama 3.1 405B model. Initially, we expect the smaller model to produce lower-quality responses. By identifying these deficiencies, we can generate high-quality synthetic data using the 405B model and subsequently do distillation by fine-tuning the 8B model. This process aims to demonstrate the improvement in response quality after fine-tuning the 8B model with the generated dataset.\n",
    "\n",
    "> You'll need a `g5.12xlarge` instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-06 06:59:17,806] p8434 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-08-06 06:59:17,964] p8434 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-08-06 06:59:18,430] p8434 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Model 'meta-textgeneration-llama-3-8b-instruct' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "[2024-08-06 06:59:18,987] p8434 {utils.py:566} INFO - Model 'meta-textgeneration-llama-3-8b-instruct' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.3.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-08-06 06:59:18,990] p8434 {cache.py:619} WARNING - Using model 'meta-textgeneration-llama-3-8b-instruct' with wildcard version identifier '*'. You can pin to version '2.3.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-08-06 06:59:18,993] p8434 {model.py:237} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-08-06 06:59:19,001] p8434 {session.py:3961} INFO - Creating model with name: meta-textgeneration-llama-3-8b-instruct-2024-08-06-06-59-18-992\n",
      "[2024-08-06 06:59:19,700] p8434 {session.py:5725} INFO - Creating endpoint-config with name meta-textgeneration-llama-3-8b-instruct-2024-08-06-06-59-18-998\n",
      "[2024-08-06 06:59:20,193] p8434 {session.py:4571} INFO - Creating endpoint with name meta-textgeneration-llama-3-8b-instruct-2024-08-06-06-59-18-998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!Running inference with LLama 3 8B model:\n",
      "\n",
      "Input:\n",
      " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can employees choose to wear respirators when not required by the employer?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Output:\n",
      " In general, employees are not required to wear respirators unless their employer has a legitimate reason to require them to do so. However, in some cases, employees may choose to wear respirators even if not required by their employer. Here are some scenarios:\n",
      "\n",
      "1. **Employee's personal choice**: An employee may choose to wear a respirator as a personal precaution, even if not required by their employer, if they are concerned about their health or the health of others in the workplace.\n",
      "2. **Employee's medical condition**: An employee with a medical condition, such as a respiratory disease, may choose to wear a respirator as a precautionary measure to protect themselves from exposure to airborne contaminants or allergens.\n",
      "3. **Special circumstances**: In situations where an employee is working in an area with a high risk of exposure to hazardous substances, such as a construction site or a laboratory, they may choose to wear a respirator as an added precaution, even if not required by their employer.\n",
      "4. **Union or collective bargaining agreement**: In some cases, a union or collective bargaining agreement may require or allow employees to wear respirators as a condition of employment.\n",
      "\n",
      "However, there are some limitations and considerations to keep in mind:\n",
      "\n",
      "1. **Employer's right to set safety standards**: Employers have the right to set safety standards and requirements for their workplace, and employees are generally required to comply with those standards.\n",
      "2. **Cost and feasibility**: Employers may not be required to provide respirators for employees who choose to wear them, especially if the employer has not deemed them necessary for the specific work environment.\n",
      "3. **Fit testing and training**: If an employee chooses to wear a respirator, they may need to undergo fit testing and training to ensure the respirator is properly fitted and used correctly.\n",
      "4. **Potential conflicts with employer policies**: Wearing a respirator without employer approval may lead to conflicts with employer policies or procedures, which could result in disciplinary action.\n",
      "\n",
      "In summary, while employees may choose to wear respirators in certain situations, it's essential to consider the employer's safety standards, policies, and procedures before doing so. It's also important for employees to ensure they are properly trained and equipped to use respirators safely and effectively.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Specify the role ARN directly\n",
    "role = get_execution_role(sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Select a model ID and version\n",
    "llama_3_8b_model_id = \"meta-textgeneration-llama-3-8b-instruct\" # Replace with your chosen model ID\n",
    "\n",
    "# If your selected model is gated, you will need to set accept_eula to True to accept the model end-user license agreement (EULA).\n",
    "accept_eula = True\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "llama_3_8b_model = JumpStartModel(model_id=llama_3_8b_model_id,role=role)\n",
    "llama_3_8b_predictor = llama_3_8b_model.deploy(accept_eula=accept_eula)\n",
    "\n",
    "# example_payloads = llama_3_8b_model.retrieve_all_examples() # uncomment if you want to preloaded examples instead\n",
    "\n",
    "question = questions[0]\n",
    "\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"details\": True,\n",
    "            \"stop\": \"<|eot_id|>\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Running inference with LLama 3 8B model:\\n\")\n",
    "run_inference(llama_3_8b_predictor, example_payloads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying LLama 3.1 405B Instruct\n",
    "\n",
    "In this section, we will deploy the LLama 3.1 405B model to compare its responses with those of the smaller LLama 3 8B model. This deployment will allow us to evaluate the performance differences and identify areas where the 8B model's responses can be improved. By analyzing the responses from the 405B model, we can generate high-quality data for distillation of the 8B model, enhancing its accuracy and effectiveness for domain-specific tasks.\n",
    "\n",
    "> You'll need a 'p5.48xlarge' instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "[2024-08-06 07:06:38,786] p8434 {utils.py:566} INFO - Model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-west-2.s3.us-west-2.amazonaws.com/fmhMetadata/eula/llama3_1Eula.txt for terms of use.\n",
      "Using model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' with wildcard version identifier '*'. You can pin to version '2.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "[2024-08-06 07:06:38,789] p8434 {cache.py:619} WARNING - Using model 'meta-textgeneration-llama-3-1-405b-instruct-fp8' with wildcard version identifier '*'. You can pin to version '2.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "[2024-08-06 07:06:38,791] p8434 {model.py:237} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.p5.48xlarge.\n",
      "[2024-08-06 07:06:38,797] p8434 {session.py:3961} INFO - Creating model with name: llama-3-1-405b-instruct-fp8-2024-08-06-07-06-38-791\n",
      "[2024-08-06 07:06:39,512] p8434 {session.py:5725} INFO - Creating endpoint-config with name llama-3-1-405b-instruct-fp8-2024-08-06-07-06-38-795\n",
      "[2024-08-06 07:06:39,810] p8434 {session.py:4571} INFO - Creating endpoint with name llama-3-1-405b-instruct-fp8-2024-08-06-07-06-38-795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------!Running inference with LLama 3.1 405B model:\n",
      "\n",
      "Input:\n",
      " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can employees choose to wear respirators when not required by the employer?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Output:\n",
      " Yes, employees can choose to wear respirators even when not required by the employer, but there are some conditions and considerations that apply. Here are the details:\n",
      "\n",
      "**OSHA Regulations:**\n",
      "\n",
      "The Occupational Safety and Health Administration (OSHA) allows employees to wear respirators voluntarily, as long as the employer permits it and the respirator use does not interfere with the employee's work duties or create a hazard. (29 CFR 1910.134(c)(2))\n",
      "\n",
      "**Voluntary Use of Respirators:**\n",
      "\n",
      "When an employee chooses to wear a respirator voluntarily, the employer is not required to provide the respirator or pay for it. However, the employer must still ensure that the employee is properly trained on the use, maintenance, and limitations of the respirator. (29 CFR 1910.134(c)(2))\n",
      "\n",
      "**Requirements for Voluntary Use:**\n",
      "\n",
      "To allow voluntary use of respirators, the employer must:\n",
      "\n",
      "1. Determine that the respirator use will not create a hazard or interfere with the employee's work duties.\n",
      "2. Ensure the employee is properly trained on the use, maintenance, and limitations of the respirator.\n",
      "3. Provide the employee with a copy of Appendix D of the OSHA Respiratory Protection Standard (29 CFR 1910.134), which provides information on the voluntary use of respirators.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "Employers are not required to provide medical evaluations or fit testing for employees who choose to wear respirators voluntarily. However, if the employer requires the use of respirators as part of the job duties, then medical evaluations and fit testing are mandatory.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "Employers should also consider the following:\n",
      "\n",
      "1. Ensure that the respirator is properly maintained and cleaned.\n",
      "2. Ensure that the employee understands the limitations of the respirator and the potential risks associated with its use.\n",
      "3. Consider any potential impact on the employee's work duties or interactions with other employees.\n",
      "\n",
      "In summary, employees can choose to wear respirators voluntarily, but employers must ensure that the use of respirators does not create a hazard or interfere with work duties, and provide proper training and information to the employee.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a model ID and version\n",
    "llama_3_1_405b_model_id = \"meta-textgeneration-llama-3-1-405b-instruct-fp8\" # Replace with your chosen model ID\n",
    "\n",
    "# If your selected model is gated, you will need to set accept_eula to True to accept the model end-user license agreement (EULA).\n",
    "accept_eula = True\n",
    "\n",
    "# Deploy the model to a SageMaker endpoint\n",
    "llama_3_1_405b_model = JumpStartModel(model_id=llama_3_1_405b_model_id,role=role)\n",
    "llama_3_1_405b_predictor = llama_3_1_405b_model.deploy(accept_eula=accept_eula)\n",
    "\n",
    "# example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "question = questions[0]\n",
    "\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"details\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Test the deployed endpoint\n",
    "print(\"Running inference with LLama 3.1 405B model:\\n\")\n",
    "run_inference(llama_3_1_405b_predictor, example_payloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 3.1 405B for Data Labeling/Generation\n",
    "\n",
    "In this section, we will leverage the LLama 3.1 405B model to generate high-quality synthetic data for distillation by fine-tuning the LLama 3 8B model. By using the 405B model to generate responses to domain-specific prompts, we can create a labeled dataset that will be used to fine-tune the 8B model, improving its accuracy and effectiveness in specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset and select the training data\n",
    "dataset = load_dataset('CShorten/CDC-COVID-FAQ', split='train')\n",
    "questions = dataset.select(range(48))['question']\n",
    "\n",
    "# Function to run inference and generate synthetic data using SageMaker JumpStart\n",
    "def generate_synthetic_data(predictor, questions):\n",
    "    synthetic_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question\n",
    "        user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        payload = {\n",
    "            \"inputs\": user_message,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.0\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            # Send the message to the model\n",
    "            response = predictor.predict(payload)\n",
    "            # print(f\"Response: {response}\")  # Debugging statement to inspect the response structure\n",
    "            \n",
    "            # Directly handle the response without JSON parsing\n",
    "            if isinstance(response, list) and 'generated_text' in response[0]:\n",
    "                response_text = response[0]['generated_text'].strip()\n",
    "            else:\n",
    "                response_text = response['generated_text'].strip()\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                \"instruction\": question,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Reason: {e}\")\n",
    "            break \n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data using the SageMaker JumpStart deployed model\n",
    "synthetic_data = generate_synthetic_data(llama_3_1_405b_predictor, questions)\n",
    "\n",
    "# Save the synthetic data to a JSONL file\n",
    "with open('synthetic_data_gen_faq_file.jsonl', 'w') as f:\n",
    "    for entry in synthetic_data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Bedrock Example\n",
    "\n",
    "> Note: You'll probably need [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "config = Config(read_timeout=5000)\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\", config=config)\n",
    "\n",
    "# Set the model ID, e.g., Llama 3.1 405b.\n",
    "model_id = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "\n",
    "# Load the dataset and select the first 20 questions\n",
    "dataset = load_dataset('deepmind/aqua_rat', split='train')\n",
    "questions = dataset.select(range(2000))['question']\n",
    "\n",
    "# Function to run inference and generate synthetic data using Bedrock\n",
    "def generate_synthetic_data(client, model_id, questions):\n",
    "    synthetic_data = []\n",
    "    for question in questions:\n",
    "        # Add Chain of Thought Reasoning prompt to the question\n",
    "        user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_message}],\n",
    "            }\n",
    "        ]\n",
    "        try:\n",
    "            # Send the message to the model, using a basic inference configuration.\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 1024,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"topP\": 0.9\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Extract the response text\n",
    "            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            synthetic_data.append({\n",
    "                \"instruction\": question,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "            break\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate synthetic data using Bedrock\n",
    "synthetic_data = generate_synthetic_data(client, model_id, questions)\n",
    "\n",
    "# Save the synthetic data to a JSONL file\n",
    "with open('synthetic_data.jsonl', 'w') as f:\n",
    "    for entry in synthetic_data:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Files to S3 for Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File template.json uploaded successfully to llama-405b-synthetic-training-data/template.json.\n",
      "File synthetic_data_gen_faq_file.jsonl uploaded successfully to llama-405b-synthetic-training-data/synthetic_data_gen_faq_file.jsonl.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'synthetic-data-gen-workshop'  # Create a new bucket or use an existing one\n",
    "subdirectory = 'llama-405b-synthetic-training-data'\n",
    "train_data_location = f\"s3://{bucket_name}/{subdirectory}\"\n",
    "\n",
    "files_to_upload = ['template.json','synthetic_data_gen_faq_file.jsonl']\n",
    "\n",
    "# Upload the files to the specified subdirectory\n",
    "for file_name in files_to_upload:\n",
    "    file_path = file_name  # File is in the same directory as the notebook\n",
    "    key_path = f\"{subdirectory}/{file_name}\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"No such file or directory: '{file_path}'\")\n",
    "    \n",
    "    # Upload the file\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, key_path)\n",
    "        print(f\"File {file_name} uploaded successfully to {key_path}.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distillation by Fine-tuning Llama 3 8B\n",
    "\n",
    "In this section, we will dive deep into the process of distillation by fine-tuning the LLama 3 8B model to enhance its performance for specific tasks. Fine-tuning involves training the pre-trained model on custom datasets to adapt it to particular domains or applications. This process can be resource-intensive, but using techniques such as LoRA (Low Rank Adaptation) and QLoRA (Quantized LoRA) can significantly reduce the required computational resources and costs. We will explore how to set up and execute a fine-tuning job using SageMaker.\n",
    "\n",
    "> You'll need a `g5.12xlarge` instance for endpoint usage to deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-06 07:39:47,952] p8434 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-08-06 07:39:48,289] p8434 {session.py:1036} INFO - Creating training-job with name: meta-textgeneration-llama-3-8b-instruct-2024-08-06-07-39-47-904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:39:48 Starting - Starting the training job...\n",
      "2024-08-06 07:40:05 Pending - Training job waiting for capacity...\n",
      "2024-08-06 07:40:30 Pending - Preparing the instances for training...\n",
      "2024-08-06 07:41:07 Downloading - Downloading input data...........................\n",
      "2024-08-06 07:45:40 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-08-06 07:45:43,369 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-08-06 07:45:43,405 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-06 07:45:43,415 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-08-06 07:45:43,417 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-08-06 07:45:52,562 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.33.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.24.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.43.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.5-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.24.2->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.43.1->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=b1aec7734af1085461b467f3b2931541bcef5d13cc0479e65419f8efa0a2aed3\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.33.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 huggingface-hub-0.24.2 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.5 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.19.1 torch-2.2.0 transformers-4.43.1 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,214 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,214 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,270 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,316 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,363 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,373 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"2\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"1\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-8b-instruct-2024-08-06-07-39-47-904\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"2\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"2\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-8b-instruct-2024-08-06-07-39-47-904\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"2\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=2\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 2 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-08-06 07:46:54,404 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '1', '--micro_batch_size', '1', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '2', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-08-06 07:46:58,991] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-08-06 07:46:58,991] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-08-06 07:46:58,991] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-08-06 07:46:58,991] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:'/opt/ml/additonals3data/__models_info__.json' file could not be found. Returning huggingface-model-id=info_not_present\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 542.74it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2346 examples [00:00, 90484.59 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 2166/2346 [00:00<00:00, 21523.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 21323.18 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:'/opt/ml/additonals3data/__models_info__.json' file could not be found. Returning huggingface-model-id=info_not_present\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:'/opt/ml/additonals3data/__models_info__.json' file could not be found. Returning huggingface-model-id=info_not_present\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:'/opt/ml/additonals3data/__models_info__.json' file could not be found. Returning huggingface-model-id=info_not_present\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 4899.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 5506.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 3666.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 3720.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 3971.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 4868.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 4459.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 4453.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 5013.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 4360.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 4340.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 4870.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2346 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 2517.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 2682.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 2664.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 1000/2346 [00:00<00:00, 2523.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 2548.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 2641.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2495.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2502.56 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 2600.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 2000/2346 [00:00<00:00, 2569.91 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2561.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2585.96 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2532.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2555.85 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2507.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2346/2346 [00:00<00:00, 2515.44 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:12,  6.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.12s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 8030.261248 Million params\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 642\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 161\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/161 [00:09<25:52,  9.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/161 [00:09<24:40,  9.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/161 [00:09<24:30,  9.19s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.8426673412322998\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/161 [00:09<24:40,  9.26s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 2/161 [00:18<23:47,  8.98s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.7835540175437927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 2/161 [00:17<23:17,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 2/161 [00:17<23:17,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 2/161 [00:17<23:13,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/161 [00:26<22:44,  8.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/161 [00:26<23:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.8477538824081421\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/161 [00:26<22:44,  8.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 3/161 [00:26<22:42,  8.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 4/161 [00:35<22:35,  8.64s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 4/161 [00:34<22:26,  8.57s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.8424466848373413\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 4/161 [00:34<22:26,  8.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 4/161 [00:34<22:24,  8.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 5/161 [00:43<22:10,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 5/161 [00:43<22:11,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.874064028263092\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 5/161 [00:43<22:11,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 5/161 [00:43<22:17,  8.57s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.9597590565681458\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 6/161 [00:52<22:03,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 6/161 [00:51<21:59,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 6/161 [00:51<21:59,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 6/161 [00:51<21:58,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 7/161 [01:00<21:49,  8.50s/it]#015Training Epoch0:   4%|#033[34m▍         #033[0m| 7/161 [01:00<21:52,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 7/161 [01:00<21:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.8320353031158447\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 7/161 [01:00<21:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 8/161 [01:08<21:39,  8.49s/it]#015Training Epoch0:   5%|#033[34m▍         #033[0m| 8/161 [01:08<21:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 8/161 [01:08<21:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.8499082326889038\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 8/161 [01:08<21:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 9/161 [01:17<21:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 9/161 [01:16<21:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.6569344997406006\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 9/161 [01:17<21:30,  8.49s/it]#015Training Epoch0:   6%|#033[34m▌         #033[0m| 9/161 [01:17<21:31,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 10/161 [01:25<21:21,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 10/161 [01:25<21:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.7584294676780701\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 10/161 [01:25<21:20,  8.48s/it]#015Training Epoch0:   6%|#033[34m▌         #033[0m| 10/161 [01:25<21:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 11/161 [01:34<21:17,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 11/161 [01:34<21:17,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.6472251415252686\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 11/161 [01:34<21:17,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 11/161 [01:34<21:17,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 12/161 [01:42<21:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 12/161 [01:42<21:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 12/161 [01:42<21:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.010809063911438\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 12/161 [01:42<21:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 13/161 [01:51<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 13/161 [01:51<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.888209879398346\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 13/161 [01:50<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 13/161 [01:51<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 14/161 [01:59<20:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 14/161 [01:59<20:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9011095762252808\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 14/161 [01:59<20:47,  8.49s/it]#015Training Epoch0:   9%|#033[34m▊         #033[0m| 14/161 [01:59<20:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 15/161 [02:07<20:38,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 15/161 [02:08<20:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 15/161 [02:07<20:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.9326222538948059\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 15/161 [02:07<20:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 16/161 [02:16<20:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 16/161 [02:16<20:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 16/161 [02:16<20:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.9641438126564026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 16/161 [02:16<20:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 17/161 [02:25<20:21,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.8697429299354553\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 17/161 [02:24<20:21,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 17/161 [02:24<20:21,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 17/161 [02:24<20:21,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 18/161 [02:33<20:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.8054950833320618\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 18/161 [02:33<20:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 18/161 [02:33<20:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 18/161 [02:33<20:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 19/161 [02:41<20:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 19/161 [02:42<20:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 19/161 [02:41<20:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.9678282141685486\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 19/161 [02:41<20:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 20/161 [02:50<19:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 20/161 [02:50<19:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.4945826828479767\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 20/161 [02:50<19:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 20/161 [02:50<19:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 21/161 [02:59<19:47,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 21/161 [02:58<19:47,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.6870276927947998\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 21/161 [02:58<19:47,  8.48s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 21/161 [02:58<19:47,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 22/161 [03:07<19:43,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 22/161 [03:07<19:43,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.9101470708847046\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 22/161 [03:07<19:43,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 22/161 [03:07<19:43,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 23/161 [03:16<19:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 23/161 [03:15<19:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.8334218859672546\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 23/161 [03:15<19:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 23/161 [03:15<19:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 24/161 [03:24<19:23,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 24/161 [03:24<19:23,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.9055265784263611\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 24/161 [03:24<19:23,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 24/161 [03:24<19:23,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 25/161 [03:33<19:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 25/161 [03:32<19:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.1489007472991943\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 25/161 [03:32<19:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 25/161 [03:32<19:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 26/161 [03:41<19:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 26/161 [03:41<19:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.885148823261261\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 26/161 [03:41<19:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 26/161 [03:41<19:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 27/161 [03:50<18:55,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 27/161 [03:49<18:55,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.7978449463844299\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 27/161 [03:49<18:55,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 27/161 [03:49<18:55,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 28/161 [03:58<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 28/161 [03:58<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 28/161 [03:58<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.7068122625350952\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 28/161 [03:58<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 29/161 [04:07<18:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.7984932065010071\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 29/161 [04:06<18:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 29/161 [04:06<18:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 29/161 [04:06<18:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.7675579190254211\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 30/161 [04:15<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 30/161 [04:15<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 30/161 [04:15<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 30/161 [04:15<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 31/161 [04:23<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 31/161 [04:23<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 31/161 [04:24<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.670762300491333\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 31/161 [04:23<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 32/161 [04:32<18:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 32/161 [04:32<18:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 32/161 [04:32<18:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.6350326538085938\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 32/161 [04:32<18:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 33/161 [04:40<18:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.805723249912262\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 33/161 [04:40<18:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 33/161 [04:40<18:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 33/161 [04:41<18:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 34/161 [04:49<17:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 34/161 [04:49<17:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.5753465890884399\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 34/161 [04:49<17:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 34/161 [04:49<17:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 35/161 [04:58<17:47,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 35/161 [04:57<17:47,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 35/161 [04:57<17:47,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.7021757960319519\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 35/161 [04:57<17:47,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 36/161 [05:06<17:39,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.6261337399482727\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 36/161 [05:06<17:39,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 36/161 [05:05<17:39,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 36/161 [05:06<17:39,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 37/161 [05:14<17:30,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 37/161 [05:14<17:30,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 37/161 [05:14<17:30,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.5513234734535217\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 37/161 [05:14<17:30,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.6767269372940063\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 38/161 [05:23<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 38/161 [05:22<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 38/161 [05:22<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 38/161 [05:22<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.12124502658844\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 39/161 [05:31<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 39/161 [05:31<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 39/161 [05:31<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 39/161 [05:31<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 40/161 [05:39<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 40/161 [05:40<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 40/161 [05:39<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.5755788087844849\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 40/161 [05:39<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 41/161 [05:48<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 41/161 [05:48<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 41/161 [05:48<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.707655131816864\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 41/161 [05:48<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 42/161 [05:56<16:47,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 42/161 [05:57<16:48,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 42/161 [05:56<16:48,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.43892398476600647\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 42/161 [05:56<16:48,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 43/161 [06:05<16:44,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 43/161 [06:05<16:44,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.48471930623054504\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 43/161 [06:05<16:44,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 43/161 [06:05<16:44,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 44/161 [06:13<16:34,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 44/161 [06:14<16:34,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.6652041077613831\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 44/161 [06:13<16:34,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 44/161 [06:13<16:34,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 45/161 [06:22<16:25,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 45/161 [06:22<16:25,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 45/161 [06:22<16:25,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.589117169380188\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 45/161 [06:22<16:25,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 46/161 [06:31<16:16,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 46/161 [06:30<16:16,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 46/161 [06:30<16:16,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.49606388807296753\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 46/161 [06:30<16:16,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 47/161 [06:39<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 47/161 [06:39<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.5619221925735474\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 47/161 [06:39<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 47/161 [06:39<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 48/161 [06:47<15:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 48/161 [06:47<15:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 48/161 [06:48<15:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.537975013256073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 48/161 [06:47<15:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 49/161 [06:56<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 49/161 [06:56<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 49/161 [06:56<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.80000239610672\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 49/161 [06:56<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 50/161 [07:04<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.6387186646461487\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 50/161 [07:04<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 50/161 [07:05<15:41,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 50/161 [07:04<15:41,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.5632636547088623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 51/161 [07:13<15:32,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 51/161 [07:13<15:32,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 51/161 [07:13<15:32,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 51/161 [07:13<15:32,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 52/161 [07:21<15:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 52/161 [07:21<15:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.5336774587631226\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 52/161 [07:21<15:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 52/161 [07:22<15:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 53/161 [07:30<15:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 53/161 [07:30<15:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 0.9330553412437439\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 53/161 [07:30<15:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 53/161 [07:30<15:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 54/161 [07:39<15:10,  8.51s/it]#015Training Epoch0:  34%|#033[34m███▎      #033[0m| 54/161 [07:38<15:10,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.673075795173645\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 54/161 [07:38<15:11,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 54/161 [07:38<15:11,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 55/161 [07:47<15:01,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 55/161 [07:47<15:01,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 55/161 [07:47<15:01,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.5596635341644287\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 55/161 [07:47<15:01,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 56/161 [07:56<14:52,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.6865232586860657\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 56/161 [07:55<14:52,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 56/161 [07:55<14:52,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 56/161 [07:55<14:52,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 57/161 [08:04<14:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 0.6577386856079102\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 57/161 [08:04<14:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 57/161 [08:04<14:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 57/161 [08:04<14:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 58/161 [08:13<14:34,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 0.5301759839057922\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 58/161 [08:12<14:34,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 58/161 [08:12<14:34,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 58/161 [08:12<14:34,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 59/161 [08:21<14:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 59/161 [08:21<14:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 59/161 [08:21<14:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.6620139479637146\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 59/161 [08:21<14:25,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 60/161 [08:30<14:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 60/161 [08:29<14:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 60/161 [08:29<14:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.3931618928909302\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 60/161 [08:29<14:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 61/161 [08:38<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.48408734798431396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 61/161 [08:38<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 61/161 [08:38<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 61/161 [08:38<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 62/161 [08:46<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 62/161 [08:47<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.5955437421798706\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 62/161 [08:46<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 62/161 [08:46<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 63/161 [08:55<13:53,  8.50s/it]#015Training Epoch0:  39%|#033[34m███▉      #033[0m| 63/161 [08:55<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 63/161 [08:55<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.6218549609184265\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 63/161 [08:55<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 64/161 [09:03<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 64/161 [09:03<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.578116774559021\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 64/161 [09:03<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 64/161 [09:04<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 65/161 [09:12<13:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.6488133668899536\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 65/161 [09:12<13:38,  8.52s/it]#015Training Epoch0:  40%|#033[34m████      #033[0m| 65/161 [09:12<13:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 65/161 [09:12<13:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 66/161 [09:20<13:27,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 66/161 [09:20<13:27,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 66/161 [09:21<13:27,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 0.6158605217933655\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 66/161 [09:20<13:27,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 67/161 [09:29<13:18,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 67/161 [09:29<13:18,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.5820394158363342\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 67/161 [09:29<13:18,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 67/161 [09:29<13:18,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.5742499828338623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 68/161 [09:37<13:09,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 68/161 [09:37<13:09,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 68/161 [09:37<13:09,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 68/161 [09:38<13:09,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 69/161 [09:46<13:00,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 69/161 [09:46<13:00,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 69/161 [09:46<13:00,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.6641550064086914\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 69/161 [09:46<13:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 70/161 [09:54<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 70/161 [09:54<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.4329191744327545\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 70/161 [09:54<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 70/161 [09:55<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 71/161 [10:03<12:43,  8.48s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 71/161 [10:03<12:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 0.5103737711906433\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 71/161 [10:03<12:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 71/161 [10:03<12:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.5035320520401001\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 72/161 [10:11<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 72/161 [10:11<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 72/161 [10:12<12:34,  8.48s/it]#015Training Epoch0:  45%|#033[34m████▍     #033[0m| 72/161 [10:11<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 73/161 [10:20<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 73/161 [10:20<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.831996500492096\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 73/161 [10:20<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 73/161 [10:19<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 74/161 [10:28<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.7365372180938721\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 74/161 [10:28<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 74/161 [10:29<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 74/161 [10:28<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 75/161 [10:37<12:10,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 0.5354872345924377\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 75/161 [10:37<12:10,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 75/161 [10:37<12:10,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 75/161 [10:37<12:10,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 76/161 [10:45<12:01,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.6959278583526611\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 76/161 [10:45<12:01,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 76/161 [10:46<12:01,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 76/161 [10:45<12:01,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 77/161 [10:53<11:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.5237969756126404\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 77/161 [10:54<11:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 77/161 [10:54<11:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 77/161 [10:54<11:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 78/161 [11:02<11:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 78/161 [11:02<11:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 78/161 [11:02<11:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 0.5503201484680176\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 78/161 [11:02<11:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 0.5889741778373718\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 79/161 [11:10<11:34,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 79/161 [11:10<11:34,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 79/161 [11:11<11:34,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 79/161 [11:10<11:34,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.6806627511978149\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 80/161 [11:19<11:26,  8.47s/it]#015Training Epoch0:  50%|#033[34m████▉     #033[0m| 80/161 [11:19<11:26,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 80/161 [11:19<11:26,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m████▉     #033[0m| 80/161 [11:19<11:26,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 81/161 [11:27<11:17,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 81/161 [11:28<11:17,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 81/161 [11:27<11:17,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 0.6312364935874939\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m█████     #033[0m| 81/161 [11:27<11:17,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 82/161 [11:36<11:09,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 82/161 [11:36<11:09,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 82/161 [11:36<11:09,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 0.6241379976272583\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 82/161 [11:36<11:09,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 83/161 [11:44<11:03,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 83/161 [11:44<11:03,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.5748746991157532\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 83/161 [11:44<11:03,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 83/161 [11:45<11:03,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.898794412612915\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 84/161 [11:53<10:54,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 84/161 [11:53<10:54,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 84/161 [11:53<10:54,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 84/161 [11:53<10:54,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 85/161 [12:02<10:48,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.4332500994205475\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 85/161 [12:02<10:48,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 85/161 [12:01<10:48,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 85/161 [12:02<10:48,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 86/161 [12:10<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 86/161 [12:10<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 86/161 [12:11<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 0.4986829161643982\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 86/161 [12:10<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 87/161 [12:19<10:30,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 87/161 [12:19<10:30,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 87/161 [12:19<10:30,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.6121850609779358\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 87/161 [12:19<10:30,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 88/161 [12:27<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 88/161 [12:27<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.6345522999763489\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 88/161 [12:27<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 88/161 [12:27<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 89/161 [12:36<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 89/161 [12:35<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.4146037995815277\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 89/161 [12:36<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▌    #033[0m| 89/161 [12:36<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 90/161 [12:44<10:02,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.7235304117202759\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 90/161 [12:44<10:02,  8.48s/it]#015Training Epoch0:  56%|#033[34m█████▌    #033[0m| 90/161 [12:44<10:02,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 90/161 [12:44<10:02,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 91/161 [12:52<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 91/161 [12:52<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.4571821987628937\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 91/161 [12:53<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 91/161 [12:52<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 92/161 [13:01<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 92/161 [13:01<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 92/161 [13:01<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 0.7149783372879028\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 92/161 [13:01<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.480251282453537\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 93/161 [13:09<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 93/161 [13:10<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 93/161 [13:09<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 93/161 [13:09<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 94/161 [13:18<09:30,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.49555516242980957\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 94/161 [13:18<09:30,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 94/161 [13:18<09:30,  8.51s/it]#015Training Epoch0:  58%|#033[34m█████▊    #033[0m| 94/161 [13:18<09:30,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 95/161 [13:26<09:21,  8.50s/it]#015Training Epoch0:  59%|#033[34m█████▉    #033[0m| 95/161 [13:26<09:21,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 95/161 [13:27<09:21,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.4975086450576782\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 95/161 [13:26<09:21,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 96/161 [13:35<09:12,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 96/161 [13:35<09:12,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 96/161 [13:35<09:12,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.4868031144142151\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m█████▉    #033[0m| 96/161 [13:35<09:12,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 97/161 [13:43<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.7797001004219055\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 97/161 [13:44<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 97/161 [13:44<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 97/161 [13:44<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 98/161 [13:52<08:55,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 98/161 [13:52<08:55,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 98/161 [13:52<08:55,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 0.7898432016372681\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████    #033[0m| 98/161 [13:52<08:55,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 99/161 [14:01<08:46,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 99/161 [14:00<08:46,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 99/161 [14:00<08:46,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 0.427860289812088\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  61%|#033[34m██████▏   #033[0m| 99/161 [14:00<08:46,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 100/161 [14:09<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 100/161 [14:09<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 1.0093066692352295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 100/161 [14:09<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 100/161 [14:09<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 101/161 [14:18<08:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 0.49322310090065\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 101/161 [14:17<08:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 101/161 [14:17<08:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 101/161 [14:17<08:29,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 102/161 [14:26<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 102/161 [14:26<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 102/161 [14:26<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 0.5045825839042664\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 102/161 [14:26<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 103/161 [14:35<08:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 103/161 [14:34<08:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 103/161 [14:34<08:12,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 0.4568736255168915\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▍   #033[0m| 103/161 [14:34<08:12,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 104/161 [14:43<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 104/161 [14:43<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 104/161 [14:43<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 0.5541738867759705\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 104/161 [14:43<08:03,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 105/161 [14:52<07:57,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 105/161 [14:51<07:57,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 105/161 [14:51<07:57,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 0.4895540177822113\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 105/161 [14:51<07:57,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 106/161 [15:01<07:49,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 106/161 [15:00<07:49,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 106/161 [15:00<07:49,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.7030522227287292\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 106/161 [15:00<07:49,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▋   #033[0m| 107/161 [15:09<07:40,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▋   #033[0m| 107/161 [15:09<07:40,  8.52s/it]#015Training Epoch0:  66%|#033[34m██████▋   #033[0m| 107/161 [15:08<07:40,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.6260695457458496\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▋   #033[0m| 107/161 [15:09<07:40,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 108/161 [15:17<07:32,  8.54s/it]#015Training Epoch0:  67%|#033[34m██████▋   #033[0m| 108/161 [15:17<07:32,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 108/161 [15:18<07:32,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.5336444973945618\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 108/161 [15:17<07:32,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 109/161 [15:26<07:22,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 109/161 [15:26<07:22,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 109/161 [15:25<07:22,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.5974108576774597\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 109/161 [15:26<07:22,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 110/161 [15:34<07:13,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 0.7100328207015991\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 110/161 [15:34<07:13,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 110/161 [15:34<07:13,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 110/161 [15:34<07:13,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 111/161 [15:42<07:04,  8.48s/it]#015Training Epoch0:  69%|#033[34m██████▉   #033[0m| 111/161 [15:43<07:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.37977614998817444\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 111/161 [15:42<07:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 111/161 [15:42<07:04,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 112/161 [15:51<06:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 112/161 [15:51<06:55,  8.48s/it]#015Training Epoch0:  70%|#033[34m██████▉   #033[0m| 112/161 [15:51<06:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 0.43609827756881714\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m██████▉   #033[0m| 112/161 [15:51<06:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 113/161 [15:59<06:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 113/161 [16:00<06:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 0.553026020526886\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 113/161 [15:59<06:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 113/161 [15:59<06:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 114/161 [16:08<06:38,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 0.6272653341293335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 114/161 [16:08<06:38,  8.47s/it]#015Training Epoch0:  71%|#033[34m███████   #033[0m| 114/161 [16:08<06:38,  8.47s/it]#015Training Epoch0:  71%|#033[34m███████   #033[0m| 114/161 [16:08<06:38,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 115/161 [16:16<06:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 115/161 [16:17<06:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 0.7102250456809998\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 115/161 [16:16<06:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 115/161 [16:16<06:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.6855011582374573\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 116/161 [16:25<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 116/161 [16:25<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 116/161 [16:25<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  72%|#033[34m███████▏  #033[0m| 116/161 [16:25<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 117/161 [16:33<06:14,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 0.627480149269104\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 117/161 [16:33<06:14,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 117/161 [16:33<06:14,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 117/161 [16:34<06:14,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 118/161 [16:42<06:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.5149011611938477\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 118/161 [16:42<06:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 118/161 [16:42<06:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 118/161 [16:42<06:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 0.4939928650856018\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 119/161 [16:50<05:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 119/161 [16:50<05:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 119/161 [16:50<05:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 119/161 [16:51<05:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.6830321550369263\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 120/161 [16:59<05:47,  8.48s/it]#015Training Epoch0:  75%|#033[34m███████▍  #033[0m| 120/161 [16:59<05:47,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 120/161 [16:59<05:47,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 120/161 [16:59<05:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 121/161 [17:08<05:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 121/161 [17:07<05:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 121/161 [17:07<05:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.675156831741333\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▌  #033[0m| 121/161 [17:07<05:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 122/161 [17:16<05:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 122/161 [17:16<05:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 122/161 [17:16<05:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 0.5854880809783936\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 122/161 [17:16<05:30,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 123/161 [17:24<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 0.5832202434539795\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 123/161 [17:24<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 123/161 [17:24<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 123/161 [17:25<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 124/161 [17:33<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 124/161 [17:33<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 124/161 [17:33<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 0.5637618899345398\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 124/161 [17:33<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 125/161 [17:42<05:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 125/161 [17:41<05:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 0.4600462317466736\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 125/161 [17:41<05:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 125/161 [17:41<05:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 126/161 [17:50<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 126/161 [17:50<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.38448306918144226\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 126/161 [17:50<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 126/161 [17:50<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 0.5702818632125854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 127/161 [17:58<04:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 127/161 [17:59<04:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 127/161 [17:58<04:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 127/161 [17:58<04:49,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 128/161 [18:07<04:41,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.7366440296173096\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 128/161 [18:07<04:41,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 128/161 [18:07<04:41,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m███████▉  #033[0m| 128/161 [18:07<04:41,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 129/161 [18:16<04:33,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.50815349817276\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 129/161 [18:15<04:33,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 129/161 [18:15<04:33,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 129/161 [18:15<04:33,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 130/161 [18:24<04:24,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 130/161 [18:24<04:24,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 0.558501124382019\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 130/161 [18:24<04:24,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████  #033[0m| 130/161 [18:24<04:24,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 131/161 [18:33<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 131/161 [18:32<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 131/161 [18:32<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 0.7151052355766296\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 131/161 [18:32<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 132/161 [18:41<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 132/161 [18:41<04:06,  8.50s/it]#015Training Epoch0:  82%|#033[34m████████▏ #033[0m| 132/161 [18:41<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.5368754267692566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 132/161 [18:41<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 133/161 [18:50<03:57,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 133/161 [18:49<03:57,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.674221396446228\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 133/161 [18:49<03:57,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 133/161 [18:49<03:57,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 134/161 [18:58<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 134/161 [18:58<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.527887761592865\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 134/161 [18:58<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m████████▎ #033[0m| 134/161 [18:58<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 135/161 [19:07<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.5969197154045105\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 135/161 [19:06<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 135/161 [19:06<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 135/161 [19:06<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 136/161 [19:15<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 136/161 [19:15<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 136/161 [19:15<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 0.5925961136817932\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▍ #033[0m| 136/161 [19:15<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 137/161 [19:24<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 137/161 [19:23<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 137/161 [19:23<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 0.4303130805492401\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 137/161 [19:23<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 138/161 [19:32<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 138/161 [19:32<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 0.6895633339881897\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 138/161 [19:32<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 138/161 [19:32<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 139/161 [19:40<03:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 139/161 [19:40<03:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 0.5723516941070557\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 139/161 [19:40<03:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▋ #033[0m| 139/161 [19:41<03:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 140/161 [19:49<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 0.6271363496780396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 140/161 [19:49<02:59,  8.53s/it]#015Training Epoch0:  87%|#033[34m████████▋ #033[0m| 140/161 [19:49<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 140/161 [19:49<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 141/161 [19:57<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 141/161 [19:58<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 0.7949405908584595\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 141/161 [19:57<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 141/161 [19:57<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 0.6551342010498047\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 142/161 [20:06<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 142/161 [20:06<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 142/161 [20:06<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 142/161 [20:06<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 143/161 [20:15<02:32,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 143/161 [20:14<02:32,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 143/161 [20:14<02:32,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.7564888596534729\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 143/161 [20:14<02:32,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 144/161 [20:23<02:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 144/161 [20:23<02:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.5940571427345276\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 144/161 [20:23<02:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 144/161 [20:23<02:24,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 0.4508703947067261\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 145/161 [20:31<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 145/161 [20:31<02:15,  8.48s/it]#015Training Epoch0:  90%|#033[34m█████████ #033[0m| 145/161 [20:31<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 145/161 [20:32<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 146/161 [20:40<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 146/161 [20:40<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 0.5589672923088074\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 146/161 [20:40<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 146/161 [20:40<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 0.47998514771461487\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 147/161 [20:48<01:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 147/161 [20:48<01:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 147/161 [20:48<01:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████▏#033[0m| 147/161 [20:49<01:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 148/161 [20:57<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 0.4062255024909973\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 148/161 [20:57<01:50,  8.51s/it]#015Training Epoch0:  92%|#033[34m█████████▏#033[0m| 148/161 [20:57<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 148/161 [20:57<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 149/161 [21:06<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 149/161 [21:05<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.6287630200386047\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 149/161 [21:05<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 149/161 [21:05<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 150/161 [21:14<01:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 150/161 [21:14<01:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 150/161 [21:14<01:33,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 0.45635682344436646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 150/161 [21:14<01:33,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 151/161 [21:22<01:25,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 151/161 [21:23<01:25,  8.54s/it]#015Training Epoch0:  94%|#033[34m█████████▍#033[0m| 151/161 [21:22<01:25,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.5287202000617981\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 151/161 [21:22<01:25,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 152/161 [21:31<01:16,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 152/161 [21:31<01:16,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 152/161 [21:31<01:16,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 0.4686703085899353\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m█████████▍#033[0m| 152/161 [21:31<01:16,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 153/161 [21:40<01:08,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 0.667184591293335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 153/161 [21:39<01:08,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 153/161 [21:39<01:08,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▌#033[0m| 153/161 [21:39<01:08,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 154/161 [21:48<00:59,  8.49s/it]#015Training Epoch0:  96%|#033[34m█████████▌#033[0m| 154/161 [21:48<00:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 154/161 [21:48<00:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 0.7046951055526733\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 154/161 [21:48<00:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 0.55029296875\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 155/161 [21:56<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 155/161 [21:56<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 155/161 [21:56<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 155/161 [21:57<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 0.5106409788131714\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 156/161 [22:05<00:42,  8.48s/it]#015Training Epoch0:  97%|#033[34m█████████▋#033[0m| 156/161 [22:05<00:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 156/161 [22:05<00:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 156/161 [22:05<00:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 157/161 [22:13<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 0.6204579472541809\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 157/161 [22:13<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 157/161 [22:14<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 157/161 [22:13<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 158/161 [22:22<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 158/161 [22:22<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 0.5978630781173706\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 158/161 [22:22<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 158/161 [22:22<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 159/161 [22:31<00:16,  8.48s/it]#015Training Epoch0:  99%|#033[34m█████████▉#033[0m| 159/161 [22:30<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 159/161 [22:30<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.41638609766960144\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 159/161 [22:30<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 160/161 [22:39<00:08,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 160/161 [22:39<00:08,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.45128723978996277\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 160/161 [22:39<00:08,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 160/161 [22:39<00:08,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 0.6967154145240784\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.50s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:47<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:48<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 161/161 [22:48<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]#015evaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.83s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.76s/it]#015evaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:02,  3.73s/it]#015evaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:02,  3.73s/it]#015evaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:02,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:02,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]#015evaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.71s/it]#015evaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:43,  3.71s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:43,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:43,  3.71s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:43,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.71s/it]#015evaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.72s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]#015evaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.71s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.71s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.71s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]#015evaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]#015evaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]#015evaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]#015evaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.71s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.71s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]#015evaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.72s/it]#015evaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.72s/it]#015evaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(1.8273, device='cuda:0') eval_epoch_loss=tensor(0.6028, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 0.6028321385383606\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=1.9373, train_epoch_loss=0.6613, epcoh time 1368.032955163s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/161 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.5732079744338989\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/161 [00:07<21:17,  7.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/161 [00:07<21:18,  7.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/161 [00:07<21:17,  7.99s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/161 [00:07<21:18,  7.99s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.38290485739707947\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 2/161 [00:16<21:55,  8.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 2/161 [00:16<21:55,  8.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 2/161 [00:16<21:55,  8.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 2/161 [00:16<21:55,  8.28s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 3/161 [00:24<22:01,  8.37s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.45866528153419495\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 3/161 [00:24<22:01,  8.37s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 3/161 [00:24<22:01,  8.37s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 3/161 [00:24<22:02,  8.37s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 4/161 [00:33<21:59,  8.40s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.45762261748313904\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 4/161 [00:33<21:59,  8.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 4/161 [00:33<21:59,  8.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 4/161 [00:33<21:59,  8.41s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 5/161 [00:41<21:53,  8.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 5/161 [00:41<21:53,  8.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 5/161 [00:41<21:53,  8.42s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.45936572551727295\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 5/161 [00:41<21:53,  8.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 6/161 [00:50<21:48,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 6/161 [00:50<21:48,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 6/161 [00:50<21:48,  8.44s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.6316553950309753\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 6/161 [00:50<21:48,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 7/161 [00:58<21:40,  8.44s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.5012080669403076\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 7/161 [00:58<21:40,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 7/161 [00:58<21:40,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 7/161 [00:58<21:40,  8.44s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 8/161 [01:07<21:33,  8.45s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.49232053756713867\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 8/161 [01:07<21:33,  8.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 8/161 [01:07<21:33,  8.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▍         #033[0m| 8/161 [01:07<21:33,  8.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 9/161 [01:15<21:26,  8.46s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.4168863892555237\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 9/161 [01:15<21:26,  8.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 9/161 [01:15<21:26,  8.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 9/161 [01:15<21:26,  8.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 10/161 [01:24<21:18,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.4541541635990143\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 10/161 [01:24<21:18,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 10/161 [01:24<21:18,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m▌         #033[0m| 10/161 [01:24<21:18,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.33831968903541565\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 11/161 [01:32<21:15,  8.51s/it]#015Training Epoch1:   7%|#033[34m▋         #033[0m| 11/161 [01:32<21:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 11/161 [01:32<21:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 11/161 [01:32<21:16,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 12/161 [01:41<21:05,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.6532207131385803\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 12/161 [01:41<21:05,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 12/161 [01:41<21:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 12/161 [01:41<21:05,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 13/161 [01:49<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 13/161 [01:49<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 13/161 [01:49<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.5363851189613342\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 13/161 [01:49<20:56,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▊         #033[0m| 14/161 [01:58<20:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.5478698015213013\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▊         #033[0m| 14/161 [01:58<20:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▊         #033[0m| 14/161 [01:58<20:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▊         #033[0m| 14/161 [01:58<20:46,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.6629491448402405\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 15/161 [02:06<20:36,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 15/161 [02:06<20:36,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 15/161 [02:06<20:36,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 15/161 [02:06<20:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 16/161 [02:15<20:28,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.6711488366127014\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 16/161 [02:15<20:28,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 16/161 [02:15<20:28,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 16/161 [02:15<20:28,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 17/161 [02:23<20:20,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.5770627856254578\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 17/161 [02:23<20:20,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 17/161 [02:23<20:20,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 17/161 [02:23<20:20,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 18/161 [02:32<20:11,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 18/161 [02:32<20:11,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.5353144407272339\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 18/161 [02:32<20:11,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 18/161 [02:32<20:11,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 19/161 [02:40<20:03,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 19/161 [02:40<20:03,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.7077451944351196\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 19/161 [02:40<20:03,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 19/161 [02:40<20:03,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 20/161 [02:49<19:54,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 20/161 [02:49<19:54,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.2901914119720459\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 20/161 [02:49<19:54,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 20/161 [02:49<19:54,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 21/161 [02:57<19:51,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 21/161 [02:57<19:51,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.5044692158699036\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 21/161 [02:57<19:51,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 21/161 [02:57<19:51,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▎        #033[0m| 22/161 [03:06<19:45,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▎        #033[0m| 22/161 [03:06<19:45,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.7018018960952759\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▎        #033[0m| 22/161 [03:06<19:45,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▎        #033[0m| 22/161 [03:06<19:45,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 23/161 [03:14<19:34,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 23/161 [03:14<19:34,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.6027995944023132\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 23/161 [03:14<19:34,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 23/161 [03:14<19:34,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 24/161 [03:23<19:23,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 24/161 [03:23<19:24,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 24/161 [03:23<19:24,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.6598610281944275\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 24/161 [03:23<19:24,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 25/161 [03:31<19:13,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 25/161 [03:31<19:14,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 25/161 [03:31<19:13,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9388218522071838\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 25/161 [03:31<19:14,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 26/161 [03:40<19:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.7168974280357361\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 26/161 [03:40<19:05,  8.48s/it]#015Training Epoch1:  16%|#033[34m█▌        #033[0m| 26/161 [03:40<19:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▌        #033[0m| 26/161 [03:40<19:05,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.6063866019248962\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 27/161 [03:48<18:56,  8.48s/it]#015Training Epoch1:  17%|#033[34m█▋        #033[0m| 27/161 [03:48<18:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 27/161 [03:48<18:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 27/161 [03:48<18:56,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.5265394449234009\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 28/161 [03:56<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 28/161 [03:56<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 28/161 [03:56<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m█▋        #033[0m| 28/161 [03:56<18:46,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.6516929268836975\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 29/161 [04:05<18:38,  8.47s/it]#015Training Epoch1:  18%|#033[34m█▊        #033[0m| 29/161 [04:05<18:37,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 29/161 [04:05<18:38,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 29/161 [04:05<18:38,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 30/161 [04:13<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 30/161 [04:13<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.5915778875350952\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 30/161 [04:13<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 30/161 [04:13<18:29,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 31/161 [04:22<18:20,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 31/161 [04:22<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 31/161 [04:22<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.5311822295188904\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▉        #033[0m| 31/161 [04:22<18:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 32/161 [04:30<18:16,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.5140538811683655\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 32/161 [04:30<18:16,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 32/161 [04:30<18:16,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 32/161 [04:30<18:16,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m██        #033[0m| 33/161 [04:39<18:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.6710342168807983\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m██        #033[0m| 33/161 [04:39<18:07,  8.50s/it]#015Training Epoch1:  20%|#033[34m██        #033[0m| 33/161 [04:39<18:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m██        #033[0m| 33/161 [04:39<18:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 34/161 [04:47<17:57,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 34/161 [04:47<17:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.44398483633995056\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 34/161 [04:47<17:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 34/161 [04:47<17:58,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 35/161 [04:56<17:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 35/161 [04:56<17:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.6093916296958923\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 35/161 [04:56<17:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 35/161 [04:56<17:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 36/161 [05:04<17:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 36/161 [05:04<17:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.5173135995864868\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 36/161 [05:04<17:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 36/161 [05:04<17:39,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 37/161 [05:13<17:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 37/161 [05:13<17:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 37/161 [05:13<17:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.4386536180973053\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 37/161 [05:13<17:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▎       #033[0m| 38/161 [05:21<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.58616042137146\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▎       #033[0m| 38/161 [05:21<17:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▎       #033[0m| 38/161 [05:21<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▎       #033[0m| 38/161 [05:21<17:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 39/161 [05:30<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 39/161 [05:30<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 39/161 [05:30<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.9827848076820374\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 39/161 [05:30<17:13,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 40/161 [05:38<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 40/161 [05:38<17:04,  8.47s/it]#015Training Epoch1:  25%|#033[34m██▍       #033[0m| 40/161 [05:38<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.4606616199016571\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▍       #033[0m| 40/161 [05:38<17:04,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 41/161 [05:47<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.5858052372932434\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 41/161 [05:47<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 41/161 [05:47<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 41/161 [05:47<16:56,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 42/161 [05:55<16:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.366272509098053\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 42/161 [05:55<16:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 42/161 [05:55<16:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 42/161 [05:55<16:48,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 43/161 [06:04<16:48,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 43/161 [06:04<16:48,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 43/161 [06:04<16:48,  8.55s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.3852711021900177\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 43/161 [06:04<16:48,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 44/161 [06:12<16:37,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 44/161 [06:12<16:37,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 44/161 [06:12<16:37,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.530086100101471\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 44/161 [06:12<16:37,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 45/161 [06:21<16:27,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 45/161 [06:21<16:27,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.5045459270477295\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 45/161 [06:21<16:27,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  28%|#033[34m██▊       #033[0m| 45/161 [06:21<16:27,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 46/161 [06:29<16:17,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 46/161 [06:29<16:17,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 46/161 [06:29<16:17,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.3843640089035034\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 46/161 [06:29<16:17,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 47/161 [06:38<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 47/161 [06:38<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.44946497678756714\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 47/161 [06:38<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▉       #033[0m| 47/161 [06:38<16:07,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 48/161 [06:46<15:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 48/161 [06:46<15:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.43115347623825073\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 48/161 [06:46<15:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 48/161 [06:46<15:58,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 49/161 [06:55<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 49/161 [06:55<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 49/161 [06:55<15:50,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.6924876570701599\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m███       #033[0m| 49/161 [06:55<15:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 50/161 [07:03<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 50/161 [07:03<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 50/161 [07:03<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.5652779340744019\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 50/161 [07:03<15:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.4516390860080719\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 51/161 [07:12<15:31,  8.47s/it]#015Training Epoch1:  32%|#033[34m███▏      #033[0m| 51/161 [07:12<15:31,  8.47s/it]#015Training Epoch1:  32%|#033[34m███▏      #033[0m| 51/161 [07:12<15:31,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 51/161 [07:12<15:31,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 52/161 [07:20<15:22,  8.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 52/161 [07:20<15:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 52/161 [07:20<15:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.430629700422287\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 52/161 [07:20<15:22,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 53/161 [07:29<15:14,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 53/161 [07:29<15:14,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 53/161 [07:29<15:14,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 0.8258793950080872\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 53/161 [07:29<15:14,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▎      #033[0m| 54/161 [07:37<15:09,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▎      #033[0m| 54/161 [07:37<15:09,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▎      #033[0m| 54/161 [07:37<15:09,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.6050220131874084\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▎      #033[0m| 54/161 [07:37<15:09,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 55/161 [07:46<14:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 55/161 [07:46<14:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 55/161 [07:46<14:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.4707013666629791\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 55/161 [07:46<14:59,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 56/161 [07:54<14:50,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.6179217100143433\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 56/161 [07:54<14:50,  8.48s/it]#015Training Epoch1:  35%|#033[34m███▍      #033[0m| 56/161 [07:54<14:50,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▍      #033[0m| 56/161 [07:54<14:50,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 57/161 [08:03<14:41,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 57/161 [08:03<14:41,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 57/161 [08:03<14:41,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 0.564931333065033\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 57/161 [08:03<14:41,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 58/161 [08:11<14:32,  8.47s/it]#015Training Epoch1:  36%|#033[34m███▌      #033[0m| 58/161 [08:11<14:32,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 58/161 [08:11<14:32,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 0.46728554368019104\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▌      #033[0m| 58/161 [08:11<14:32,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 59/161 [08:19<14:23,  8.47s/it]#015Training Epoch1:  37%|#033[34m███▋      #033[0m| 59/161 [08:19<14:23,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.5488542318344116\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 59/161 [08:19<14:23,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 59/161 [08:19<14:23,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 60/161 [08:28<14:15,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 60/161 [08:28<14:15,  8.47s/it]#015Training Epoch1:  37%|#033[34m███▋      #033[0m| 60/161 [08:28<14:15,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.337358683347702\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 60/161 [08:28<14:15,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.43607664108276367\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 61/161 [08:36<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 61/161 [08:36<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 61/161 [08:36<14:07,  8.47s/it]#015Training Epoch1:  38%|#033[34m███▊      #033[0m| 61/161 [08:36<14:07,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▊      #033[0m| 62/161 [08:45<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▊      #033[0m| 62/161 [08:45<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▊      #033[0m| 62/161 [08:45<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.5160497426986694\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▊      #033[0m| 62/161 [08:45<13:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 63/161 [08:53<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 63/161 [08:53<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.5541811585426331\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 63/161 [08:53<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  39%|#033[34m███▉      #033[0m| 63/161 [08:53<13:53,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 64/161 [09:02<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 64/161 [09:02<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.49524998664855957\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 64/161 [09:02<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 64/161 [09:02<13:43,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 65/161 [09:11<13:40,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 65/161 [09:11<13:41,  8.55s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.5955079793930054\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 65/161 [09:11<13:41,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m████      #033[0m| 65/161 [09:11<13:41,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 66/161 [09:19<13:30,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 0.5428398847579956\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 66/161 [09:19<13:30,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 66/161 [09:19<13:30,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 66/161 [09:19<13:30,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.5117685198783875\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 67/161 [09:28<13:19,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 67/161 [09:28<13:19,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 67/161 [09:28<13:19,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 67/161 [09:28<13:19,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 68/161 [09:36<13:10,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 68/161 [09:36<13:10,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 68/161 [09:36<13:10,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.45221054553985596\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 68/161 [09:36<13:10,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 69/161 [09:44<13:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.5852599143981934\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 69/161 [09:44<13:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 69/161 [09:44<13:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 69/161 [09:44<13:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 70/161 [09:53<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 70/161 [09:53<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.3646635413169861\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 70/161 [09:53<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 70/161 [09:53<12:51,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 71/161 [10:01<12:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 71/161 [10:01<12:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 0.437662810087204\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 71/161 [10:01<12:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 71/161 [10:01<12:42,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 72/161 [10:10<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 72/161 [10:10<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.4394354224205017\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 72/161 [10:10<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▍     #033[0m| 72/161 [10:10<12:34,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 73/161 [10:18<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 73/161 [10:18<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 73/161 [10:18<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.7465634942054749\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 73/161 [10:18<12:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 74/161 [10:27<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 74/161 [10:27<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 74/161 [10:27<12:20,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.6679834127426147\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 74/161 [10:27<12:20,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 75/161 [10:35<12:11,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 75/161 [10:35<12:11,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 75/161 [10:35<12:11,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 0.44635599851608276\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 75/161 [10:35<12:11,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 76/161 [10:44<12:02,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.6295767426490784\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 76/161 [10:44<12:02,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 76/161 [10:44<12:02,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 76/161 [10:44<12:02,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 77/161 [10:52<11:53,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.410989373922348\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 77/161 [10:52<11:53,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 77/161 [10:52<11:53,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 77/161 [10:52<11:53,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 78/161 [11:01<11:43,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 0.5033447742462158\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 78/161 [11:01<11:44,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 78/161 [11:01<11:44,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 78/161 [11:01<11:44,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 79/161 [11:09<11:35,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 79/161 [11:09<11:35,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 0.5318101644515991\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 79/161 [11:09<11:35,  8.48s/it]#015Training Epoch1:  49%|#033[34m████▉     #033[0m| 79/161 [11:09<11:35,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 80/161 [11:18<11:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.6136483550071716\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 80/161 [11:18<11:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 80/161 [11:18<11:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m████▉     #033[0m| 80/161 [11:18<11:26,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 0.5647509098052979\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 81/161 [11:26<11:18,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 81/161 [11:26<11:18,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m█████     #033[0m| 81/161 [11:26<11:18,  8.48s/it]#015Training Epoch1:  50%|#033[34m█████     #033[0m| 81/161 [11:26<11:18,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 0.5617504715919495\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 82/161 [11:35<11:09,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 82/161 [11:35<11:09,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 82/161 [11:35<11:09,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 82/161 [11:35<11:09,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 83/161 [11:43<11:00,  8.47s/it]#015Training Epoch1:  52%|#033[34m█████▏    #033[0m| 83/161 [11:43<11:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.5254802107810974\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 83/161 [11:43<11:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 83/161 [11:43<11:00,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 84/161 [11:52<10:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 84/161 [11:52<10:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.8296326398849487\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 84/161 [11:52<10:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 84/161 [11:52<10:52,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 85/161 [12:00<10:46,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.37929654121398926\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 85/161 [12:00<10:46,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 85/161 [12:00<10:46,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 85/161 [12:00<10:46,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 86/161 [12:09<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 0.44207924604415894\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 86/161 [12:09<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 86/161 [12:09<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 86/161 [12:09<10:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 87/161 [12:17<10:29,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.5370790362358093\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 87/161 [12:17<10:29,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 87/161 [12:17<10:29,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 87/161 [12:17<10:30,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.5567032694816589\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 88/161 [12:26<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 88/161 [12:26<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 88/161 [12:26<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 88/161 [12:26<10:20,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 89/161 [12:34<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 89/161 [12:34<10:11,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.3737133741378784\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 89/161 [12:34<10:11,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▌    #033[0m| 89/161 [12:34<10:11,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 90/161 [12:43<10:02,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 90/161 [12:43<10:02,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 90/161 [12:43<10:02,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.6507673859596252\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 90/161 [12:43<10:02,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 91/161 [12:51<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 91/161 [12:51<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.419098436832428\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 91/161 [12:51<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 91/161 [12:51<09:53,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 92/161 [13:00<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 92/161 [13:00<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 0.6317570805549622\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 92/161 [13:00<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 92/161 [13:00<09:45,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 93/161 [13:08<09:36,  8.48s/it]#015Training Epoch1:  58%|#033[34m█████▊    #033[0m| 93/161 [13:08<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 93/161 [13:08<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.4418592154979706\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 93/161 [13:08<09:36,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 94/161 [13:17<09:27,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 94/161 [13:17<09:27,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 94/161 [13:17<09:27,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.40385493636131287\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 94/161 [13:17<09:27,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 95/161 [13:25<09:19,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.4375804662704468\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 95/161 [13:25<09:19,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 95/161 [13:25<09:19,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 95/161 [13:25<09:19,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 96/161 [13:34<09:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 96/161 [13:34<09:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 96/161 [13:34<09:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.42336589097976685\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m█████▉    #033[0m| 96/161 [13:34<09:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 97/161 [13:42<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.6675779223442078\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 97/161 [13:42<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 97/161 [13:42<09:05,  8.52s/it]#015Training Epoch1:  60%|#033[34m██████    #033[0m| 97/161 [13:42<09:05,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 98/161 [13:51<08:55,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 98/161 [13:51<08:55,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 0.7215988039970398\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 98/161 [13:51<08:55,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████    #033[0m| 98/161 [13:51<08:55,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████▏   #033[0m| 99/161 [13:59<08:46,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████▏   #033[0m| 99/161 [13:59<08:46,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 0.3727269172668457\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████▏   #033[0m| 99/161 [13:59<08:46,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  61%|#033[34m██████▏   #033[0m| 99/161 [13:59<08:46,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 100/161 [14:08<08:37,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.9275045990943909\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 100/161 [14:08<08:37,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 100/161 [14:08<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 100/161 [14:08<08:37,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 101/161 [14:16<08:29,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 101/161 [14:16<08:29,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 0.42879047989845276\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 101/161 [14:16<08:29,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 101/161 [14:16<08:29,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 102/161 [14:25<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 102/161 [14:25<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 0.43965041637420654\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 102/161 [14:25<08:20,  8.48s/it]#015Training Epoch1:  63%|#033[34m██████▎   #033[0m| 102/161 [14:25<08:20,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 103/161 [14:33<08:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 0.4045100808143616\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 103/161 [14:33<08:11,  8.48s/it]#015Training Epoch1:  64%|#033[34m██████▍   #033[0m| 103/161 [14:33<08:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▍   #033[0m| 103/161 [14:33<08:11,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 104/161 [14:42<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 104/161 [14:42<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 104/161 [14:42<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 0.47692739963531494\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 104/161 [14:42<08:03,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 105/161 [14:50<07:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 0.4324009418487549\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 105/161 [14:50<07:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 105/161 [14:50<07:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▌   #033[0m| 105/161 [14:50<07:55,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 106/161 [14:59<07:48,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 106/161 [14:59<07:48,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 106/161 [14:59<07:48,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.6538701057434082\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 106/161 [14:59<07:48,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▋   #033[0m| 107/161 [15:07<07:40,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▋   #033[0m| 107/161 [15:07<07:40,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.578780472278595\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▋   #033[0m| 107/161 [15:07<07:40,  8.53s/it]#015Training Epoch1:  66%|#033[34m██████▋   #033[0m| 107/161 [15:07<07:40,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 108/161 [15:16<07:33,  8.55s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.44236519932746887\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 108/161 [15:16<07:33,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 108/161 [15:16<07:33,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 108/161 [15:16<07:33,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 109/161 [15:24<07:23,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 109/161 [15:24<07:23,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.5555351972579956\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 109/161 [15:24<07:23,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 109/161 [15:24<07:23,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 110/161 [15:33<07:13,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 110/161 [15:33<07:13,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 110/161 [15:33<07:13,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 0.6664849519729614\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 110/161 [15:33<07:13,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 111/161 [15:41<07:04,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.32625076174736023\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 111/161 [15:41<07:04,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 111/161 [15:41<07:04,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 111/161 [15:41<07:04,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 112/161 [15:50<06:55,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 112/161 [15:50<06:55,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 112/161 [15:50<06:55,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 0.38718950748443604\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m██████▉   #033[0m| 112/161 [15:50<06:55,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 113/161 [15:58<06:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 0.48583540320396423\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 113/161 [15:58<06:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 113/161 [15:58<06:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 113/161 [15:58<06:47,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 114/161 [16:07<06:38,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 114/161 [16:07<06:38,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 114/161 [16:07<06:38,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 0.5714856386184692\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████   #033[0m| 114/161 [16:07<06:38,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 115/161 [16:15<06:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 115/161 [16:15<06:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 115/161 [16:15<06:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 0.6568148136138916\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 115/161 [16:15<06:29,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 116/161 [16:24<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.6226803660392761\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 116/161 [16:24<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 116/161 [16:24<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  72%|#033[34m███████▏  #033[0m| 116/161 [16:24<06:21,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 0.5853443741798401\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 117/161 [16:32<06:12,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 117/161 [16:32<06:12,  8.47s/it]#015Training Epoch1:  73%|#033[34m███████▎  #033[0m| 117/161 [16:32<06:12,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 117/161 [16:32<06:12,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 118/161 [16:41<06:05,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 118/161 [16:41<06:05,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 118/161 [16:41<06:05,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.46241557598114014\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 118/161 [16:41<06:05,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 119/161 [16:49<05:57,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 119/161 [16:49<05:57,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 119/161 [16:49<05:57,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 0.45204681158065796\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 119/161 [16:49<05:57,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.5958907008171082\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 120/161 [16:58<05:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 120/161 [16:58<05:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 120/161 [16:58<05:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 120/161 [16:58<05:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.6263411641120911\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 121/161 [17:06<05:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 121/161 [17:06<05:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▌  #033[0m| 121/161 [17:06<05:39,  8.49s/it]#015Training Epoch1:  75%|#033[34m███████▌  #033[0m| 121/161 [17:06<05:39,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 0.531852126121521\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 122/161 [17:15<05:30,  8.48s/it]#015Training Epoch1:  76%|#033[34m███████▌  #033[0m| 122/161 [17:15<05:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 122/161 [17:15<05:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 122/161 [17:15<05:30,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▋  #033[0m| 123/161 [17:23<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▋  #033[0m| 123/161 [17:23<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▋  #033[0m| 123/161 [17:23<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 0.5145044922828674\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▋  #033[0m| 123/161 [17:23<05:22,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 124/161 [17:31<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 124/161 [17:31<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 124/161 [17:31<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 0.5004755854606628\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 124/161 [17:31<05:13,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 125/161 [17:40<05:05,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 0.4022786021232605\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 125/161 [17:40<05:05,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 125/161 [17:40<05:05,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 125/161 [17:40<05:05,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 126/161 [17:49<04:57,  8.51s/it]#015Training Epoch1:  78%|#033[34m███████▊  #033[0m| 126/161 [17:49<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.35430359840393066\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 126/161 [17:49<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 126/161 [17:49<04:57,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 127/161 [17:57<04:48,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 127/161 [17:57<04:48,  8.50s/it]#015Training Epoch1:  79%|#033[34m███████▉  #033[0m| 127/161 [17:57<04:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 0.48005080223083496\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 127/161 [17:57<04:48,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 128/161 [18:05<04:40,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 128/161 [18:05<04:40,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.693096935749054\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 128/161 [18:05<04:40,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m███████▉  #033[0m| 128/161 [18:05<04:40,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 129/161 [18:14<04:33,  8.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 129/161 [18:14<04:33,  8.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 129/161 [18:14<04:33,  8.56s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.45561403036117554\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 129/161 [18:14<04:33,  8.56s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 130/161 [18:23<04:24,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 130/161 [18:23<04:24,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 130/161 [18:23<04:24,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 0.5023433566093445\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████  #033[0m| 130/161 [18:23<04:24,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 131/161 [18:31<04:15,  8.51s/it]#015Training Epoch1:  81%|#033[34m████████▏ #033[0m| 131/161 [18:31<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 131/161 [18:31<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 0.6451188325881958\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 131/161 [18:31<04:15,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 132/161 [18:40<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 132/161 [18:40<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.5040174722671509\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 132/161 [18:40<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 132/161 [18:40<04:06,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 133/161 [18:48<03:57,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.6118199825286865\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 133/161 [18:48<03:57,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 133/161 [18:48<03:57,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 133/161 [18:48<03:57,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 134/161 [18:57<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.4740484058856964\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 134/161 [18:57<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 134/161 [18:57<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m████████▎ #033[0m| 134/161 [18:57<03:49,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 135/161 [19:05<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 135/161 [19:05<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.5531214475631714\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 135/161 [19:05<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 135/161 [19:05<03:40,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 136/161 [19:13<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 0.5493483543395996\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 136/161 [19:13<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▍ #033[0m| 136/161 [19:13<03:31,  8.48s/it]#015Training Epoch1:  84%|#033[34m████████▍ #033[0m| 136/161 [19:13<03:31,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 137/161 [19:22<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 0.387361079454422\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 137/161 [19:22<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 137/161 [19:22<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 137/161 [19:22<03:24,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 138/161 [19:31<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 138/161 [19:31<03:15,  8.50s/it]#015Training Epoch1:  86%|#033[34m████████▌ #033[0m| 138/161 [19:31<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 0.6092650294303894\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 138/161 [19:31<03:15,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▋ #033[0m| 139/161 [19:39<03:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▋ #033[0m| 139/161 [19:39<03:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 0.5328724384307861\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▋ #033[0m| 139/161 [19:39<03:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▋ #033[0m| 139/161 [19:39<03:07,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 140/161 [19:48<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 140/161 [19:48<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 0.5634253621101379\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 140/161 [19:48<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 140/161 [19:48<02:59,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 141/161 [19:56<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 141/161 [19:56<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 0.7405396103858948\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 141/161 [19:56<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 141/161 [19:56<02:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 142/161 [20:05<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 142/161 [20:05<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 0.5922216773033142\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 142/161 [20:05<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 142/161 [20:05<02:41,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 143/161 [20:13<02:32,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.724678635597229\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 143/161 [20:13<02:32,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 143/161 [20:13<02:32,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 143/161 [20:13<02:32,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.5285524129867554\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 144/161 [20:22<02:24,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 144/161 [20:22<02:24,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 144/161 [20:22<02:24,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 144/161 [20:22<02:24,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 0.4054712951183319\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 145/161 [20:30<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 145/161 [20:30<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 145/161 [20:30<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 145/161 [20:30<02:15,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 146/161 [20:38<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 146/161 [20:38<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 0.5237683057785034\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 146/161 [20:38<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 146/161 [20:38<02:07,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████▏#033[0m| 147/161 [20:47<01:58,  8.47s/it]#015Training Epoch1:  91%|#033[34m█████████▏#033[0m| 147/161 [20:47<01:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████▏#033[0m| 147/161 [20:47<01:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 0.4321499168872833\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████▏#033[0m| 147/161 [20:47<01:58,  8.47s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 148/161 [20:56<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 148/161 [20:56<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 148/161 [20:56<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 0.3425142467021942\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 148/161 [20:56<01:50,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 149/161 [21:04<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 149/161 [21:04<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 149/161 [21:04<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.5781853795051575\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 149/161 [21:04<01:42,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 150/161 [21:13<01:33,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 150/161 [21:13<01:33,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 0.4146285355091095\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 150/161 [21:13<01:33,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 150/161 [21:13<01:33,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 151/161 [21:21<01:25,  8.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 151/161 [21:21<01:25,  8.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 151/161 [21:21<01:25,  8.57s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.4961469769477844\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 151/161 [21:21<01:25,  8.57s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 152/161 [21:30<01:16,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 152/161 [21:30<01:16,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 152/161 [21:30<01:16,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 0.4346645474433899\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m█████████▍#033[0m| 152/161 [21:30<01:16,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 153/161 [21:38<01:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 0.6111215353012085\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 153/161 [21:38<01:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 153/161 [21:38<01:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▌#033[0m| 153/161 [21:38<01:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 0.6585093140602112\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 154/161 [21:47<00:59,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 154/161 [21:47<00:59,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 154/161 [21:47<00:59,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 154/161 [21:47<00:59,  8.50s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 0.5004852414131165\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 155/161 [21:55<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 155/161 [21:55<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 155/161 [21:55<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 155/161 [21:55<00:50,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 156/161 [22:04<00:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 156/161 [22:04<00:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 156/161 [22:04<00:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 0.4358452260494232\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 156/161 [22:04<00:42,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 157/161 [22:12<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 0.5821792483329773\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 157/161 [22:12<00:33,  8.48s/it]#015Training Epoch1:  98%|#033[34m█████████▊#033[0m| 157/161 [22:12<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 157/161 [22:12<00:33,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 0.5364530682563782\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 158/161 [22:21<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 158/161 [22:21<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 158/161 [22:21<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 158/161 [22:21<00:25,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 159/161 [22:29<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 159/161 [22:29<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.37856265902519226\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 159/161 [22:29<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 159/161 [22:29<00:16,  8.48s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 160/161 [22:38<00:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 160/161 [22:38<00:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 160/161 [22:38<00:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.41422703862190247\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 160/161 [22:38<00:08,  8.51s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.50s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.49s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 0.6351872682571411\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.50s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 161/161 [22:46<00:00,  8.49s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]#015evaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/41 [00:04<02:48,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.91s/it]#015evaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]#015evaluating Epoch:   5%|#033[32m▍         #033[0m| 2/41 [00:07<02:32,  3.92s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.82s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.82s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.82s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 3/41 [00:11<02:25,  3.82s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]#015evaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]#015evaluating Epoch:  10%|#033[32m▉         #033[0m| 4/41 [00:15<02:19,  3.78s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.75s/it]#015evaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 5/41 [00:19<02:15,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:10,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:10,  3.74s/it]#015evaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▍        #033[0m| 6/41 [00:22<02:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 7/41 [00:26<02:06,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:03,  3.73s/it]#015evaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 8/41 [00:30<02:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 9/41 [00:33<01:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]#015evaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 10/41 [00:37<01:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]#015evaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 11/41 [00:41<01:51,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.72s/it]#015evaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.72s/it]#015evaluating Epoch:  29%|#033[32m██▉       #033[0m| 12/41 [00:45<01:47,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:44,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 13/41 [00:48<01:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]#015evaluating Epoch:  34%|#033[32m███▍      #033[0m| 14/41 [00:52<01:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]#015evaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 15/41 [00:56<01:36,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 16/41 [00:59<01:32,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.72s/it]#015evaluating Epoch:  41%|#033[32m████▏     #033[0m| 17/41 [01:03<01:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  44%|#033[32m████▍     #033[0m| 18/41 [01:07<01:25,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.71s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 19/41 [01:11<01:21,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.71s/it]#015evaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.71s/it]#015evaluating Epoch:  49%|#033[32m████▉     #033[0m| 20/41 [01:14<01:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]#015evaluating Epoch:  51%|#033[32m█████     #033[0m| 21/41 [01:18<01:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.72s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 22/41 [01:22<01:10,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]#015evaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  56%|#033[32m█████▌    #033[0m| 23/41 [01:25<01:06,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.71s/it]#015evaluating Epoch:  59%|#033[32m█████▊    #033[0m| 24/41 [01:29<01:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 25/41 [01:33<00:59,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]#015evaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]#015evaluating Epoch:  63%|#033[32m██████▎   #033[0m| 26/41 [01:37<00:55,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 27/41 [01:40<00:52,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 28/41 [01:44<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]#015evaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]#015evaluating Epoch:  71%|#033[32m███████   #033[0m| 29/41 [01:48<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 30/41 [01:51<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]#015evaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 31/41 [01:55<00:37,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 32/41 [01:59<00:33,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 33/41 [02:03<00:29,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 34/41 [02:06<00:25,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▌ #033[0m| 35/41 [02:10<00:22,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 36/41 [02:14<00:18,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 37/41 [02:17<00:14,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 38/41 [02:21<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]#015evaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▌#033[0m| 39/41 [02:25<00:07,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.71s/it]#015evaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.71s/it]#015evaluating Epoch:  98%|#033[32m█████████▊#033[0m| 40/41 [02:29<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 41/41 [02:32<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(1.7824, device='cuda:0') eval_epoch_loss=tensor(0.5779, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 0.5779499411582947\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=1.7335, train_epoch_loss=0.5501, epcoh time 1367.0307009030003s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 1.835396409034729\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 0.6057159900665283\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 1.8048336505889893\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 0.5903910398483276\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 1367.5318280330002\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.7088236050000205\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  9.79it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.89it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.72it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.03it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.00it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-08-06 08:39:33,066 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-08-06 08:39:33,066 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-08-06 08:39:33,066 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-08-06 08:39:53 Uploading - Uploading generated training model\n",
      "2024-08-06 08:40:36 Completed - Training job completed\n",
      "Training seconds: 3568\n",
      "Billable seconds: 3568\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-8b-instruct\", \"*\"\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.12xlarge\",  # For Llama-3-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(\n",
    "    instruction_tuned=\"True\", epoch=\"2\", max_input_length=\"1024\", chat_dataset=\"False\"\n",
    ")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the LLama 3 8B Fine-tuned Model \n",
    "\n",
    "In this section, we will evaluate the performance of the fine-tuned LLaMA 3 8B model to determine how well it has adapted to the specific tasks for which it was trained. Testing involves comparing the model's responses to a set of predefined questions or tasks against the baseline performance of the original, pre-trained model. This process helps us understand the improvements achieved through distillation by fine-tuning and identify any remaining areas for enhancement. By systematically examining the model's outputs, we can ensure that the fine-tuning process has effectively tailored the model to meet our specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-08-06 08:45:45,418] p8434 {model.py:237} INFO - No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "[2024-08-06 08:45:45,508] p8434 {session.py:3961} INFO - Creating model with name: meta-textgeneration-llama-3-8b-instruct-2024-08-06-08-45-45-423\n",
      "[2024-08-06 08:45:46,175] p8434 {session.py:5725} INFO - Creating endpoint-config with name meta-textgeneration-llama-3-8b-instruct-2024-08-06-08-45-45-417\n",
      "[2024-08-06 08:45:46,450] p8434 {session.py:4571} INFO - Creating endpoint with name meta-textgeneration-llama-3-8b-instruct-2024-08-06-08-45-45-417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with fine-tuned LLama 3 8B model...\n",
      "\n",
      "Running inference with LLama 3 8B model...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Question</th>\n",
       "        <th>Dataset Answer</th>\n",
       "        <th>Fine-tuned LLama 3 8B Output</th>\n",
       "        <th>LLama 3 8B Output</th>\n",
       "    </tr>\n",
       "\n",
       "    <tr>\n",
       "        <td>If I have patients with asthma, do I need to make any changes to their daily asthma preventive management regimens to reduce their risk of getting sick with COVID-19?</td>\n",
       "        <td>People with moderate to severe asthma, particularly if not well controlled, might be at higher risk of getting very sick from COVID-19. Based on what we currently know about COVID-19, the selection of therapeutic options through guideline-recommended treatment of asthma has not been affected. National asthma guidelines are available. Continuation of inhaled corticosteroids is particularly important for patients already using these medications because there is no evidence of increased risk of COVID-19 morbidity with use of inhaled corticosteroids and an abundance of data showing reduced risk of asthma exacerbation with maintenance of asthma controller therapy. Patients with asthma but without symptoms or a diagnosis of COVID-19 should continue any required nebulizer treatments.</td>\n",
       "        <td>Yes, patients with asthma should take steps to reduce their risk of getting sick with COVID-19. Here are some recommendations for modifying their daily asthma preventive management regimens:\n",
       "\n",
       "1. **Continue regular asthma medications**: Patients with asthma should continue taking their prescribed medications as directed by their healthcare provider. This includes inhaled corticosteroids, long-acting beta agonists, and other medications to control their asthma symptoms.\n",
       "2. **Adjust medication dosing**: If a patient's asthma symptoms worsen or they experience increased respiratory symptoms, such as cough, wheezing, or shortness of breath, they should contact their healthcare provider to adjust their medication dosing or add additional medications as needed.\n",
       "3. **Use inhalers correctly**: Patients should use their inhalers correctly, including:\n",
       "\t* Using a spacer device with metered-dose inhalers (MDIs) to help deliver medication deeper into the lungs.\n",
       "\t* Inhaling slowly and deeply through the mouthpiece of the inhaler.\n",
       "\t* Holding their breath for 1-2 seconds after inhaling.\n",
       "\t* Exhaling slowly and naturally.\n",
       "4. **Monitor peak flow**: Patients should monitor their peak flow readings regularly to track their lung function and adjust their medication as needed.\n",
       "5. **Avoid triggers**: Patients should avoid triggers that can exacerbate asthma symptoms, such as:\n",
       "\t* Allergens (e.g., pollen, dust mites, pet dander).\n",
       "\t* Irritants (e.g., tobacco smoke, strong odors, chemicals).\n",
       "\t* Respiratory viruses (e.g., rhinovirus, coronavirus).\n",
       "6. **Stay hydrated**: Drinking plenty of fluids can help thin mucus and make it easier to breathe.\n",
       "7. **Get vaccinated**: Patients with asthma should receive the COVID-19 vaccine, as recommended by the Centers for Disease Control and Prevention (CDC) and the American Lung Association.\n",
       "8. **Wear a mask**: Patients with asthma should wear a mask when in public or around others to reduce the risk of exposure to respiratory viruses.\n",
       "9. **Stay informed**: Patients should stay informed about COVID-19 and follow local health guidelines and recommendations.\n",
       "10. **Contact healthcare provider**: Patients should contact their healthcare provider if they experience any changes in their asthma symptoms, such as increased wheezing, coughing, or shortness of breath, or if they have concerns about their asthma management during the pandemic.\n",
       "\n",
       "By following these guidelines, patients with asthma can reduce their risk of getting sick with COVID-19 and manage their asthma symptoms effectively</td>\n",
       "        <td>Yes, patients with asthma should take steps to reduce their risk of getting sick with COVID-19, and their daily asthma preventive management regimens may need to be adjusted accordingly. Here are some recommendations:\n",
       "\n",
       "1. **Continue regular asthma medications**: Patients with asthma should continue taking their prescribed medications as directed, including inhaled corticosteroids, long-acting beta2-agonists, and other medications as prescribed by their healthcare provider.\n",
       "2. **Adjust medication dosing**: If a patient's asthma symptoms worsen or they experience increased symptoms, such as wheezing, coughing, or shortness of breath, they may need to adjust their medication dosing. Consult with their healthcare provider for guidance.\n",
       "3. **Use inhalers correctly**: Ensure patients use their inhalers correctly, as instructed by their healthcare provider, to ensure optimal medication delivery and effectiveness.\n",
       "4. **Monitor peak flow readings**: Patients with asthma should continue to monitor their peak flow readings regularly to track their lung function and adjust their medication as needed.\n",
       "5. **Avoid triggers**: Encourage patients to avoid triggers that can exacerbate asthma symptoms, such as:\n",
       "\t* Allergens (e.g., dust mites, mold, pet dander)\n",
       "\t* Irritants (e.g., tobacco smoke, strong odors, pollution)\n",
       "\t* Respiratory viruses (e.g., rhinovirus, coronavirus)\n",
       "6. **Stay hydrated**: Encourage patients to drink plenty of fluids to help keep their airways moist and prevent dehydration.\n",
       "7. **Get vaccinated**: Encourage patients to get vaccinated against seasonal flu and other preventable illnesses, such as pneumonia, to reduce their risk of developing a secondary infection.\n",
       "8. **Wear a mask**: Encourage patients to wear a mask when in public or in crowded areas to reduce the transmission of respiratory viruses.\n",
       "9. **Practice good hygiene**: Encourage patients to practice good hygiene, such as:\n",
       "\t* Washing their hands frequently with soap and water\n",
       "\t* Avoiding close contact with people who are sick\n",
       "\t* Covering their mouth and nose when coughing or sneezing\n",
       "10. **Consult with healthcare provider**: If a patient's asthma symptoms worsen or they experience any concerns, they should consult with their healthcare provider for guidance on adjusting their asthma management plan.\n",
       "\n",
       "By following these recommendations, patients with asthma can reduce their risk of getting sick with COVID-19 and manage their asthma symptoms effectively.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>Can viral culture be used to demonstrate that a person who had persistently or recurrently detectable viral RNA is not infectious to others?</td>\n",
       "        <td>Yes. However, viral culture is not widely performed for SARS-CoV-2. It must be conducted in Biosafety Level 3 (BSL-3) laboratories using BSL-3 practices by experienced virologists and culture results can take a week or more. Therefore, while persons whose specimens do not yield live virus are considered no longer infectious, the complexity of such testing and the time required to complete it mean that culture cannot be used routinely to guide management of infected persons.</td>\n",
       "        <td>A very specific and technical question!\n",
       "\n",
       "In the context of infectious diseases, viral culture is a laboratory test used to detect the presence of a virus in a sample, such as a swab from the throat or nose. The test involves inoculating a sample into a cell culture, which is a mixture of cells and nutrients that support the growth of the virus. If the virus is present in the sample, it will replicate in the cell culture, and the presence of the virus can be detected through various methods, such as observing the cells under a microscope or using molecular biology techniques like PCR (polymerase chain reaction).\n",
       "\n",
       "Now, regarding your question:\n",
       "\n",
       "To determine whether a person who has persistently or recurrently detectable viral RNA is infectious to others, we need to consider the following:\n",
       "\n",
       "1. **Viral load**: The presence of viral RNA does not necessarily mean the person is infectious. The amount of virus present (viral load) can be an important factor. If the viral load is very low, the person may not be infectious, even if the virus is detectable.\n",
       "2. **Viral replication**: Even if the person has detectable viral RNA, if the virus is not replicating or is not producing infectious particles, they may not be infectious to others.\n",
       "3. **Immune response**: The person's immune system may be able to control the infection, preventing the virus from replicating and producing infectious particles, even if the virus is detectable.\n",
       "4. **Viral mutations**: The virus may have mutated to a non-infectious form, which would not be transmitted to others.\n",
       "\n",
       "To address your question, a combination of tests and considerations is necessary:\n",
       "\n",
       "* **Viral culture**: Performing a viral culture on the person's sample can help determine if the virus is replicating and producing infectious particles.\n",
       "* **Quantitative PCR (qPCR)**: Measuring the viral load using qPCR can provide information on the amount of virus present, which can help determine if the person is likely to be infectious.\n",
       "* **Serology**: Testing for antibodies against the virus can indicate if the person has developed an immune response, which may indicate they are not infectious.\n",
       "* **Clinical presentation**: The person's symptoms and clinical presentation can also provide clues about their infectiousness. For example, if they are asymptomatic or have mild symptoms, they may be less likely to be infectious.\n",
       "\n",
       "In summary, while the presence of detectable viral RNA does not necessarily mean a person is infectious, a combination of tests and considerations can help determine their infectiousness</td>\n",
       "        <td>Yes, viral culture can be used to demonstrate that a person who had persistently or recurrently detectable viral RNA is not infectious to others. Here's how:\n",
       "\n",
       "1. **Viral culture is a gold standard for detecting infectious virus**: Viral culture involves isolating the virus from a patient's sample (e.g., blood, respiratory secretions, or stool) and growing it in a laboratory setting. If the virus grows in culture, it indicates that the virus is viable and infectious.\n",
       "2. **Recurrent detection of viral RNA does not necessarily mean infectious virus**: While detecting viral RNA (e.g., using PCR or other molecular methods) can indicate the presence of the virus, it does not necessarily mean that the virus is infectious. The virus may be present in a latent or dormant state, not capable of replicating or causing disease.\n",
       "3. **Viral culture can distinguish between infectious and non-infectious virus**: By performing viral culture on the same sample, researchers can determine whether the virus is infectious or not. If the virus does not grow in culture, it suggests that the virus is not infectious, even if it is still detectable by molecular methods.\n",
       "4. **Studies have shown that viral culture can be used to determine infectiousness**: Several studies have demonstrated that viral culture can be used to determine infectiousness in individuals with persistent or recurrent viral RNA detection. For example, a study on HIV-infected individuals found that those with detectable viral RNA but non-cultivable virus were not infectious to others.\n",
       "\n",
       "Some examples of viral cultures used to demonstrate non-infectiousness include:\n",
       "\n",
       "* In HIV, viral culture can be used to determine whether an individual is infectious or not, even if they have detectable viral RNA.\n",
       "* In hepatitis B, viral culture can be used to determine whether an individual is infectious or not, even if they have detectable viral DNA.\n",
       "* In influenza, viral culture can be used to determine whether an individual is infectious or not, even if they have detectable viral RNA.\n",
       "\n",
       "In summary, viral culture can be used to demonstrate that a person who had persistently or recurrently detectable viral RNA is not infectious to others, as it can distinguish between infectious and non-infectious virus.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>How do you test a patient for infection wtih SARS-COV-2?</td>\n",
       "        <td>Clinicians are able to access laboratory testing through state and local public health laboratories, as well as commercial and clinical laboratories across the country. The Association of Public Health Laboratories provides a list of states and territories with laboratories that are using COVID-19 viral tests. For more information, see Testing in U.S. Clinicians should direct testing questions to their state health departments. Commercial reference laboratories are also able to offer a larger volume of testing for SARS-CoV-2. CDC has guidance for who should be tested, but decisions about testing are at the discretion of state and local health departments and/or individual clinicians. Healthcare providers should report positive  results to their local/state health department CDC does not directly collect these data directly.</td>\n",
       "        <td>Testing for SARS-CoV-2, the virus that causes COVID-19, is a crucial step in diagnosing and managing the infection. Here are the common methods used to test for SARS-CoV-2:\n",
       "\n",
       "**1. Reverse Transcription Polymerase Chain Reaction (RT-PCR):**\n",
       "This is the most widely used diagnostic test for SARS-CoV-2. RT-PCR detects the genetic material (RNA) of the virus in a patient's sample. A nasal or throat swab, or a sample from a lower respiratory tract (e.g., bronchoalveolar lavage fluid) is collected and then analyzed using a specialized machine that amplifies the viral RNA. The test is highly sensitive and specific, but it may not detect the virus in the early stages of infection.\n",
       "\n",
       "**2. Rapid Antigen Test (RAT):**\n",
       "This is a rapid, point-of-care test that detects the presence of SARS-CoV-2 antigens in a patient's sample. The test uses a nasal or throat swab and provides results within 15-30 minutes. While less sensitive than RT-PCR, RATs are useful for rapid screening and can help identify patients who are likely to have a SARS-CoV-2 infection.\n",
       "\n",
       "**3. Serology (Antibody) Tests:**\n",
       "These tests detect the presence of antibodies against SARS-CoV-2 in a patient's blood or serum. Antibody tests can be used to diagnose past infection, as they typically appear several days after the onset of symptoms. There are two types of antibody tests:\n",
       "\t* IgM (Immunoglobulin M) tests detect the presence of IgM antibodies, which are produced early in the course of infection.\n",
       "\t* IgG (Immunoglobulin G) tests detect the presence of IgG antibodies, which are produced later in the course of infection and provide long-term immunity.\n",
       "\n",
       "**4. Viral Culture:**\n",
       "This method involves growing the virus in a laboratory using a cell culture. While not commonly used for routine diagnosis, viral culture can be used to confirm the presence of SARS-CoV-2 in patients with severe or unusual cases.\n",
       "\n",
       "**5. Other tests:**\n",
       "Other tests, such as loop-mediated isothermal amplification (LAMP) and transcription-mediated amplification (TMA), are also being used in some settings. These tests are similar to RT-PCR but have different mechanisms of detection.\n",
       "\n",
       "**When to test:**\n",
       "Testing for SARS-CoV-2 is recommended in the</td>\n",
       "        <td>Testing for SARS-CoV-2, the virus that causes COVID-19, is a crucial step in diagnosing and managing the infection. Here are the common methods used to test for SARS-CoV-2:\n",
       "\n",
       "1. **RT-PCR (Reverse Transcription Polymerase Chain Reaction)**: This is the most widely used method for detecting SARS-CoV-2. A sample of respiratory secretions (e.g., nasal swab, throat swab, or sputum) is collected and sent to a laboratory for testing. The sample is then mixed with primers and probes that target specific regions of the SARS-CoV-2 genome. If the virus is present, the primers and probes bind to the viral RNA, and the PCR process amplifies the signal, allowing for detection.\n",
       "2. **Antigen tests**: These tests detect the presence of SARS-CoV-2 antigens (proteins) in a sample of respiratory secretions. Antigen tests are often rapid and can provide results in 15-30 minutes. They are less sensitive than PCR tests but can still detect the virus in the early stages of infection.\n",
       "3. **Serology tests**: These tests detect the presence of antibodies against SARS-CoV-2 in a patient's blood. Antibodies are produced by the immune system in response to infection. Serology tests can help diagnose past infections, but they may not detect active infection.\n",
       "4. **Nucleic acid sequencing**: This method involves sequencing the genetic material of the virus to identify specific mutations or variations. It is often used to track the spread of the virus and monitor the emergence of new variants.\n",
       "\n",
       "The choice of testing method depends on the patient's symptoms, medical history, and the stage of the infection. In general, PCR tests are used for:\n",
       "\n",
       "* Suspected cases of COVID-19 with symptoms\n",
       "* Patients with severe symptoms or those who are hospitalized\n",
       "* Healthcare workers or individuals in close contact with confirmed cases\n",
       "\n",
       "Antigen tests are often used for:\n",
       "\n",
       "* Rapid screening in high-risk settings, such as healthcare facilities or long-term care facilities\n",
       "* Symptomatic patients who are not severely ill\n",
       "\n",
       "Serology tests are used for:\n",
       "\n",
       "* Monitoring the spread of the virus in a population\n",
       "* Diagnosing past infections\n",
       "* Research purposes\n",
       "\n",
       "It's essential to note that testing for SARS-CoV-2 is not a single test, but rather a combination of tests that may be used in different situations. The specific testing strategy may vary depending on the country</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>Should facilities that perform elective medical pre-procedural testing of asymptomatic patients modify their testing based on a patient's report of a positive viral test for SARS-CoV-2 in the prior 90 days?</td>\n",
       "        <td>In general, viral testing is not necessary for asymptomatic patients who have recovered from SARS-CoV-2 infection in the prior 90 days.  However, if a facility is performing medical pre-procedural viral testing and the patient cannot provide documentation of their prior positive viral test, the safest approach is for the facility to retest using their current protocol.  In this situation, antigen testing is preferred. As viral tests for SARS-CoV-2 can remain positive for extended periods, even among patients who are no longer infectious, this approach could result in unnecessary isolation of the patient or delay of an elective medical procedure. Facilities may choose to accept self-test results to determine when to proceed with elective medical procedures. Such an approach assumes that the patient self-administered the viral test properly and interpreted the viral test results correctly.  If there is uncertainty about the validity of reported self-test results, facilities might attempt to acquire documentation of the test results or elect to proceed with their own pre-procedure viral testing.</td>\n",
       "        <td>According to the Centers for Disease Control and Prevention (CDC) and the American College of Cardiology (ACC), facilities that perform elective medical procedures should modify their testing protocols for asymptomatic patients who report a positive viral test for SARS-CoV-2 within the prior 90 days.\n",
       "\n",
       "**CDC Guidelines:**\n",
       "\n",
       "The CDC recommends that healthcare facilities consider the following when deciding whether to modify testing for asymptomatic patients who report a positive viral test for SARS-CoV-2 within the prior 90 days:\n",
       "\n",
       "1. **Duration of illness**: If the patient has been asymptomatic for at least 10 days after the date of their positive test, they are likely no longer infectious and can proceed with their elective procedure without additional testing.\n",
       "2. **Timing of testing**: If the patient's positive test was within the prior 90 days, but they have not been asymptomatic for at least 10 days, they should be tested again for SARS-CoV-2 before proceeding with their elective procedure.\n",
       "3. **Clinical judgment**: Healthcare providers should use their clinical judgment to determine whether additional testing is necessary based on the patient's individual circumstances, including the type of procedure, the patient's underlying health status, and the potential risks and benefits of proceeding with the procedure.\n",
       "\n",
       "**ACC Guidelines:**\n",
       "\n",
       "The ACC recommends that healthcare providers consider the following when deciding whether to modify testing for asymptomatic patients who report a positive viral test for SARS-CoV-2 within the prior 90 days:\n",
       "\n",
       "1. **Timing of testing**: If the patient's positive test was within the prior 90 days, they should be tested again for SARS-CoV-2 before proceeding with their elective procedure.\n",
       "2. **Clinical judgment**: Healthcare providers should use their clinical judgment to determine whether additional testing is necessary based on the patient's individual circumstances, including the type of procedure, the patient's underlying health status, and the potential risks and benefits of proceeding with the procedure.\n",
       "\n",
       "**Key Takeaways:**\n",
       "\n",
       "* If a patient reports a positive viral test for SARS-CoV-2 within the prior 90 days, facilities should consider the duration of illness, timing of testing, and clinical judgment when deciding whether to modify testing for asymptomatic patients.\n",
       "* If the patient has been asymptomatic for at least 10 days after the date of their positive test, they are likely no longer infectious and can proceed with their elective procedure without additional testing.\n",
       "* If the patient has not been asymptomatic for at least 10 days, they should be tested again for SARS-CoV-2</td>\n",
       "        <td>The Centers for Disease Control and Prevention (CDC) and the American College of Cardiology (ACC) have provided guidance on this topic. Here's a summary:\n",
       "\n",
       "**CDC Guidance:**\n",
       "\n",
       "The CDC recommends that facilities performing elective medical procedures consider the following:\n",
       "\n",
       "1. **No testing is needed**: If the patient reports a positive SARS-CoV-2 test result within the past 90 days, and they have completed their isolation period (typically 10 days after symptom onset or 20 days after exposure), they do not need to be re-tested before elective procedures.\n",
       "2. **Testing may be considered**: If the patient reports a positive SARS-CoV-2 test result within the past 90 days, but has not completed their isolation period, they may need to be re-tested before elective procedures, depending on the specific circumstances and local public health guidance.\n",
       "\n",
       "**ACC Guidance:**\n",
       "\n",
       "The ACC recommends that facilities performing elective cardiovascular procedures consider the following:\n",
       "\n",
       "1. **No testing is needed**: If the patient reports a positive SARS-CoV-2 test result within the past 90 days, and they have completed their isolation period, they do not need to be re-tested before elective cardiovascular procedures.\n",
       "2. **Testing may be considered**: If the patient reports a positive SARS-CoV-2 test result within the past 90 days, but has not completed their isolation period, they may need to be re-tested before elective cardiovascular procedures, depending on the specific circumstances and local public health guidance.\n",
       "\n",
       "**Key considerations:**\n",
       "\n",
       "1. **Patient symptoms**: If the patient is asymptomatic, they do not need to be re-tested.\n",
       "2. **Isolation period**: If the patient has completed their isolation period, they do not need to be re-tested.\n",
       "3. **Local public health guidance**: Facilities should consult with local public health authorities for specific guidance on testing and isolation requirements.\n",
       "4. **Procedure-specific considerations**: Some procedures, such as those that involve aerosol-generating procedures or close contact with the patient, may require additional precautions or testing.\n",
       "\n",
       "In summary, facilities performing elective medical procedures should consider the patient's report of a positive SARS-CoV-2 test result within the past 90 days and consult with local public health authorities to determine if re-testing is necessary.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>How long does an examination room need to remain vacant after being occupied by a patient with confirmed or suspected COVID-19?</td>\n",
       "        <td>The amount of time that the air inside an examination room remains potentially infectious depends on a number of factors including the size of the room, the number of air changes per hour, how long the patient was in the room, if the patient was coughing or sneezing, and if an aerosol-generating procedure was performed. In general, it is recommended to restrict HCP and patients without PPE from entering the room until sufficient time has elapsed for enough air changes to remove potentially infectious particles. General guidance is available on clearance rates under differing ventilation conditions. In addition to ensuring sufficient time for enough air changes to remove potentially infectious particles, HCP should clean and disinfect environmental surfaces and shared equipment before the room is used for another patient.</td>\n",
       "        <td>According to the Centers for Disease Control and Prevention (CDC) and the World Health Organization (WHO), the recommended time for an examination room to remain vacant after being occupied by a patient with confirmed or suspected COVID-19 varies depending on the specific circumstances. Here are some general guidelines:\n",
       "\n",
       "**Confirmed COVID-19 case:**\n",
       "\n",
       "* The CDC recommends that the examination room be closed for at least 24 hours after the patient has been discharged or transferred to a separate isolation area.\n",
       "* The room should be thoroughly cleaned and disinfected according to the CDC's guidelines for cleaning and disinfection of surfaces.\n",
       "\n",
       "**Suspected COVID-19 case:**\n",
       "\n",
       "* If a patient is suspected of having COVID-19 but has not been confirmed, the room should be closed for at least 24 hours after the patient has been discharged or transferred to a separate isolation area.\n",
       "* If the patient is confirmed to not have COVID-19, the room can be reopened after a thorough cleaning and disinfection.\n",
       "\n",
       "**Additional considerations:**\n",
       "\n",
       "* If the patient was in the room for a prolonged period (e.g., overnight), the room may need to be closed for a longer period, such as 48 hours, to ensure adequate time for any potential viral particles to dissipate.\n",
       "* If the patient was in the room for a short period (e.g., a brief examination), the room may be able to be reopened sooner, such as after a 12-hour period, depending on the specific circumstances.\n",
       "* The decision to reopen the room should be made in consultation with local health authorities and based on the specific situation.\n",
       "\n",
       "It's essential to note that these guidelines are subject to change as new information becomes available, and healthcare facilities should follow the most up-to-date guidance from local and national health authorities. Additionally, proper cleaning and disinfection protocols should always be followed to minimize the risk of transmission.</td>\n",
       "        <td>According to the Centers for Disease Control and Prevention (CDC) and the World Health Organization (WHO), an examination room should remain vacant for a certain period after being occupied by a patient with confirmed or suspected COVID-19 to minimize the risk of transmission to others. Here are the guidelines:\n",
       "\n",
       "**CDC Guidelines:**\n",
       "\n",
       "* If a patient with suspected or confirmed COVID-19 has been in the examination room, the room should be closed for at least 24 hours before being cleaned and disinfected.\n",
       "* If the patient was in the room for less than 24 hours, the room can be cleaned and disinfected immediately after the patient leaves, but it should still be closed for at least 1 hour before being occupied again.\n",
       "\n",
       "**WHO Guidelines:**\n",
       "\n",
       "* If a patient with suspected or confirmed COVID-19 has been in the examination room, the room should be closed for at least 1 hour before being cleaned and disinfected.\n",
       "* If the patient was in the room for less than 1 hour, the room can be cleaned and disinfected immediately after the patient leaves, but it should still be closed for at least 30 minutes before being occupied again.\n",
       "\n",
       "**Additional Considerations:**\n",
       "\n",
       "* The room should be thoroughly cleaned and disinfected according to the manufacturer's instructions for the disinfectant used.\n",
       "* All surfaces, including high-touch areas such as doorknobs, light switches, and countertops, should be disinfected.\n",
       "* The room should be ventilated to remove any airborne pathogens.\n",
       "* Healthcare personnel should wear personal protective equipment (PPE) when entering the room, including gloves, gown, and mask.\n",
       "\n",
       "It's essential to note that these guidelines may vary depending on the specific situation and local health authorities' recommendations. Healthcare facilities should consult with local health authorities and follow their guidance on infection control measures.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>If a patient tests positive for another respiratory virus, should that exclude SARS-CoV-2 as a cause of illness?</td>\n",
       "        <td>Patients can be infected with more than one virus at the same time. Coinfections with other respiratory viruses in people with COVID-19 have been reported. Therefore, identifying infection with one respiratory virus does not exclude SARS-CoV-2 virus infection.</td>\n",
       "        <td>No, testing positive for another respiratory virus does not necessarily exclude SARS-CoV-2 as a cause of illness. In fact, it's common for people to be infected with multiple respiratory viruses at the same time, a condition known as coinfection or co-infection.\n",
       "\n",
       "SARS-CoV-2 is a highly contagious virus that can cause COVID-19, and it's not uncommon for people to be infected with other respiratory viruses, such as:\n",
       "\n",
       "1. Influenza (flu)\n",
       "2. Respiratory syncytial virus (RSV)\n",
       "3. Human metapneumovirus (HMPV)\n",
       "4. Adenovirus\n",
       "5. Rhinovirus (common cold)\n",
       "\n",
       "The presence of another respiratory virus does not necessarily rule out SARS-CoV-2 infection, as both viruses can cause similar symptoms, such as:\n",
       "\n",
       "* Cough\n",
       "* Fever\n",
       "* Shortness of breath\n",
       "* Fatigue\n",
       "* Headache\n",
       "* Sore throat\n",
       "\n",
       "In fact, some studies have shown that up to 20% of people with COVID-19 may also have another respiratory virus present.\n",
       "\n",
       "To confirm the diagnosis of SARS-CoV-2 infection, healthcare providers typically use a combination of:\n",
       "\n",
       "1. Clinical evaluation: Assessing symptoms, medical history, and physical examination findings.\n",
       "2. Laboratory testing: Performing a polymerase chain reaction (PCR) or antigen test to detect SARS-CoV-2 RNA or antigens in respiratory samples, such as nasopharyngeal swabs or sputum.\n",
       "3. Serology: Testing for SARS-CoV-2 antibodies in blood or serum to detect past or current infection.\n",
       "\n",
       "If a patient tests positive for another respiratory virus, it's essential to consider the following:\n",
       "\n",
       "* The patient may still have SARS-CoV-2 infection, even if another virus is present.\n",
       "* The presence of another virus may affect the severity or course of SARS-CoV-2 infection.\n",
       "* The patient may require additional testing or treatment for the co-infecting virus.\n",
       "\n",
       "In summary, a positive test result for another respiratory virus does not necessarily exclude SARS-CoV-2 as a cause of illness. A comprehensive diagnostic approach, including clinical evaluation, laboratory testing, and serology, is necessary to confirm the diagnosis of SARS-CoV-2 infection.</td>\n",
       "        <td>No, testing positive for another respiratory virus does not necessarily exclude SARS-CoV-2 as a cause of illness. In fact, it's common for people to be infected with multiple respiratory viruses at the same time, a condition known as coinfection.\n",
       "\n",
       "SARS-CoV-2 is a highly contagious virus that can cause a range of respiratory illnesses, from mild to severe. It's possible for someone to test positive for SARS-CoV-2 and also test positive for another respiratory virus, such as:\n",
       "\n",
       "1. Influenza (flu)\n",
       "2. Respiratory syncytial virus (RSV)\n",
       "3. Human metapneumovirus (HMPV)\n",
       "4. Adenovirus\n",
       "5. Rhinovirus (common cold)\n",
       "\n",
       "Coinfection with SARS-CoV-2 and another respiratory virus can occur for several reasons:\n",
       "\n",
       "1. Overlapping transmission routes: Both viruses can be spread through respiratory droplets, contact with contaminated surfaces, and close contact with an infected person.\n",
       "2. Similar symptoms: The symptoms of SARS-CoV-2 and other respiratory viruses can be similar, making it challenging to diagnose without testing.\n",
       "3. Coincidental infection: It's possible for someone to be exposed to both viruses at the same time, increasing the likelihood of coinfection.\n",
       "\n",
       "In cases where a patient tests positive for another respiratory virus, healthcare providers may still consider SARS-CoV-2 as a possible cause of illness, especially if the patient presents with symptoms consistent with COVID-19, such as fever, cough, and shortness of breath.\n",
       "\n",
       "To determine the cause of illness, healthcare providers may:\n",
       "\n",
       "1. Perform additional testing: Such as PCR (polymerase chain reaction) or antigen testing to confirm the presence of SARS-CoV-2.\n",
       "2. Review medical history: To identify any underlying conditions or risk factors that may increase the likelihood of coinfection.\n",
       "3. Monitor symptoms: Closely monitor the patient's symptoms and response to treatment to determine the most likely cause of illness.\n",
       "\n",
       "In summary, testing positive for another respiratory virus does not necessarily exclude SARS-CoV-2 as a cause of illness. Healthcare providers should consider the possibility of coinfection and perform additional testing and evaluation to determine the most likely cause of illness.</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>What do we know about detection of SARS-CoV-2 RNA after clinical recovery from COVID-19?</td>\n",
       "        <td>Many recovered persons do not have detectable SARS-CoV-2 RNA in upper respiratory tract specimens. However, viral RNA can be persistently detected by reverse transcription polymerase chain reaction (RT-PCR) in respiratory tract samples in some persons after apparent clinical recovery.  In some persons, after testing negative by RT-PCR in two consecutive samples, later samples can test positive again. These persistent detections of viral RNA usually are associated with higher cycle threshold (Ct) values (i.e., fewer RNA copies) than Ct values found in RT-PCR results from samples collected shortly before or during clinical illness. Studies that have examined how long SARS-CoV-2 RNA can be detected in adults have demonstrated that, in some persons, it can be detected for weeks.</td>\n",
       "        <td>Detection of SARS-CoV-2 RNA after clinical recovery from COVID-19 is an important aspect of understanding the persistence of the virus in the body and the potential for transmission. Here's what we know:\n",
       "\n",
       "**Duration of detectable SARS-CoV-2 RNA:**\n",
       "\n",
       "* Studies have shown that SARS-CoV-2 RNA can be detected in the upper respiratory tract, such as the nose, throat, and lungs, for several weeks after clinical recovery from COVID-19.\n",
       "* The duration of detectable RNA varies widely, but most studies suggest that:\n",
       "\t+ In mild cases, SARS-CoV-2 RNA can be detected for 2-4 weeks after symptom onset.\n",
       "\t+ In severe cases, SARS-CoV-2 RNA can be detected for 4-6 weeks or even longer after symptom onset.\n",
       "* A study published in The New England Journal of Medicine found that SARS-CoV-2 RNA was detectable in the upper respiratory tract for up to 12 weeks after symptom onset in some patients.\n",
       "\n",
       "**Factors influencing detectability:**\n",
       "\n",
       "* The duration of detectable SARS-CoV-2 RNA is influenced by several factors, including:\n",
       "\t+ Severity of illness: Patients with severe disease tend to have longer periods of detectable RNA.\n",
       "\t+ Age: Older adults may have longer periods of detectable RNA.\n",
       "\t+ Underlying health conditions: Patients with underlying health conditions, such as diabetes or cardiovascular disease, may have longer periods of detectable RNA.\n",
       "\t+ Treatment: Antiviral therapy, such as remdesivir, may shorten the duration of detectable RNA.\n",
       "\n",
       "**Importance of detection:**\n",
       "\n",
       "* Detection of SARS-CoV-2 RNA after clinical recovery is important for several reasons:\n",
       "\t+ It can inform decisions about when patients are no longer infectious and can safely return to work or resume normal activities.\n",
       "\t+ It can help guide public health strategies, such as contact tracing and quarantine measures.\n",
       "\t+ It can inform the development of diagnostic tests and vaccines.\n",
       "\n",
       "**Limitations and future research:**\n",
       "\n",
       "* The current understanding of SARS-CoV-2 RNA detection after clinical recovery is based on a limited number of studies, and more research is needed to fully understand the dynamics of viral persistence and transmission.\n",
       "* Future studies should investigate the relationship between detectable RNA and infectiousness, as well as the impact of various factors on the duration of detectable RNA.\n",
       "\n",
       "In summary, while we have some understanding of the duration of detectable SARS-CoV-2 RNA after clinical recovery</td>\n",
       "        <td>Detection of SARS-CoV-2 RNA after clinical recovery from COVID-19 is an important topic of ongoing research. Here's a summary of what we currently know:\n",
       "\n",
       "**Duration of viral shedding:**\n",
       "\n",
       "* Studies have shown that SARS-CoV-2 RNA can be detected in the upper respiratory tract, such as the nose, throat, and saliva, for several weeks after clinical recovery from COVID-19.\n",
       "* The duration of viral shedding varies widely, but most studies suggest that it can last for:\n",
       "\t+ 2-3 weeks in the upper respiratory tract (URT) (e.g., nose, throat, saliva)\n",
       "\t+ 4-6 weeks in the lower respiratory tract (LRT) (e.g., lungs, bronchi)\n",
       "\t+ Up to 12 weeks or more in some cases, particularly in individuals with severe disease or those who are immunocompromised\n",
       "\n",
       "**Factors influencing viral shedding:**\n",
       "\n",
       "* Age: Older adults may shed virus for longer periods than younger individuals.\n",
       "* Immune status: Individuals with compromised immune systems, such as those with HIV/AIDS or undergoing immunosuppressive therapy, may shed virus for longer periods.\n",
       "* Disease severity: Patients with severe COVID-19 may shed virus for longer periods than those with mild disease.\n",
       "* Treatment: Antiviral therapy, such as remdesivir, may reduce viral shedding.\n",
       "\n",
       "**Detection methods:**\n",
       "\n",
       "* Polymerase chain reaction (PCR) is the most commonly used method for detecting SARS-CoV-2 RNA in respiratory samples.\n",
       "* Other methods, such as loop-mediated isothermal amplification (LAMP) and reverse transcription-PCR (RT-PCR), have also been used.\n",
       "* The sensitivity and specificity of these methods can vary depending on the type of sample, the quality of the sample, and the laboratory performing the test.\n",
       "\n",
       "**Clinical implications:**\n",
       "\n",
       "* Detection of SARS-CoV-2 RNA after clinical recovery does not necessarily mean that the individual is infectious or contagious.\n",
       "* However, it is still important to take precautions to prevent transmission, such as wearing personal protective equipment (PPE) and maintaining social distancing.\n",
       "* The detection of SARS-CoV-2 RNA may be useful for monitoring the effectiveness of treatment and for identifying individuals who may require further isolation or quarantine.\n",
       "\n",
       "**Ongoing research:**\n",
       "\n",
       "* Studies are ongoing to better understand the duration of viral shedding and the factors that influence it.\n",
       "* Researchers are also exploring the use of SARS-CoV-2 RNA detection as a tool for monitoring the effectiveness of public health interventions</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <td>Questions addressing the proper handling of healthcare personnel (HCP) who have recovered from SARS-CoV-2 infection, but are still within 3 months of onset of their prior infection.</td>\n",
       "        <td>If HCP within 3 months of their initial infection develop symptoms consistent with COVID-19, should they be excluded from work or retested? If HCP develop symptoms consistent with COVID-19 within 3 months of a confirmed SARS-CoV-2 infection they should be evaluated to identify potential etiologies for their symptoms. If an etiology for the symptoms other than SARS-CoV-2 cannot be identified, they may need to be retested for SARS-CoV-2 infection. If testing is performed, an antigen test instead of a nucleic acid amplification test (NAAT) is recommended.  Decisions about the need for and duration of work restriction should be based upon their suspected diagnosis (e.g., influenza, SARS-CoV-2 infection). Do HCP within 3 months of their initial infection need to wear all recommended personal protective equipment (PPE) when caring for patients with suspected or confirmed SARS-CoV-2 infection? For example, if there are limited respirators, should respirators be prioritized for HCP who have not been previously infected? Regardless of suspected or confirmed immunity, HCP should always wear all recommended PPE when caring for patients. In situations of PPE shortages, facilities should refer to CDC strategies for optimizing PPE supply. However, as with other infectious diseases (e.g., measles), allocation of available PPE should not be based on whether HCP have been previously infected or have evidence of immunity.</td>\n",
       "        <td>Here are some questions addressing the proper handling of healthcare personnel (HCP) who have recovered from SARS-CoV-2 infection, but are still within 3 months of onset of their prior infection:\n",
       "\n",
       "**Q1:** Can HCP who have recovered from SARS-CoV-2 infection return to work immediately after their isolation period is completed?\n",
       "\n",
       "**A1:** According to the Centers for Disease Control and Prevention (CDC) and the World Health Organization (WHO), HCP who have recovered from SARS-CoV-2 infection can return to work after their isolation period is completed, which is typically 10 days after symptom onset or 24 hours after fever resolution, whichever is longer. However, they should be cleared by their healthcare provider before returning to work.\n",
       "\n",
       "**Q2:** Are there any specific precautions that HCP who have recovered from SARS-CoV-2 infection should take when returning to work?\n",
       "\n",
       "**A2:** Yes, HCP who have recovered from SARS-CoV-2 infection should take the following precautions when returning to work:\n",
       "\n",
       "* Continue to follow standard infection control practices, including proper hand hygiene, use of personal protective equipment (PPE), and respiratory hygiene.\n",
       "* Avoid close contact with patients who are suspected or confirmed to have SARS-CoV-2 infection.\n",
       "* Avoid working in areas where patients with SARS-CoV-2 infection are being treated or cared for.\n",
       "* Consider wearing a mask in the workplace, especially in areas where patients with SARS-CoV-2 infection are being treated or cared for.\n",
       "\n",
       "**Q3:** Can HCP who have recovered from SARS-CoV-2 infection participate in aerosol-generating procedures (AGPs) or high-risk procedures?\n",
       "\n",
       "**A3:** According to the CDC, HCP who have recovered from SARS-CoV-2 infection can participate in AGPs or high-risk procedures if they are asymptomatic and have been cleared by their healthcare provider. However, they should follow standard precautions, including wearing PPE, and take additional precautions to minimize the risk of transmission, such as using a mask and eye protection.\n",
       "\n",
       "**Q4:** Are there any specific considerations for HCP who have recovered from SARS-CoV-2 infection and are working in areas with high transmission rates or in areas with limited resources?\n",
       "\n",
       "**A4:** Yes, HCP who have recovered from SARS-CoV-2 infection and are working in areas with high transmission rates or limited resources should take additional precautions, such as:\n",
       "\n",
       "* Wearing a mask at all times while in</td>\n",
       "        <td>Here are some questions addressing the proper handling of healthcare personnel (HCP) who have recovered from SARS-CoV-2 infection, but are still within 3 months of onset of their prior infection:\n",
       "\n",
       "**General Questions**\n",
       "\n",
       "1. Can HCP who have recovered from SARS-CoV-2 infection return to work immediately after their symptoms have resolved?\n",
       "\t* Answer: No, HCP who have recovered from SARS-CoV-2 infection should wait at least 3 months after the onset of their symptoms before returning to work.\n",
       "2. What is the purpose of the 3-month waiting period?\n",
       "\t* Answer: The 3-month waiting period is to ensure that the HCP has fully recovered from the infection and is no longer shedding the virus, reducing the risk of transmission to patients.\n",
       "\n",
       "**Infection Control Measures**\n",
       "\n",
       "3. Should HCP who have recovered from SARS-CoV-2 infection wear personal protective equipment (PPE) when caring for patients?\n",
       "\t* Answer: Yes, HCP who have recovered from SARS-CoV-2 infection should continue to wear PPE when caring for patients, including gloves, gown, and eye protection, to prevent transmission of the virus.\n",
       "4. Are there any specific infection control measures that should be taken when caring for patients with SARS-CoV-2 infection?\n",
       "\t* Answer: Yes, HCP should follow standard infection control practices, including proper hand hygiene, use of PPE, and proper disposal of waste and linen.\n",
       "\n",
       "**Return-to-Work Considerations**\n",
       "\n",
       "5. What factors should be considered when determining whether an HCP who has recovered from SARS-CoV-2 infection can return to work?\n",
       "\t* Answer: Factors to consider include the HCP's symptoms, laboratory test results, and the duration since onset of symptoms.\n",
       "6. Can HCP who have recovered from SARS-CoV-2 infection return to work if they have a mild or asymptomatic infection?\n",
       "\t* Answer: No, HCP who have a mild or asymptomatic infection should wait at least 3 months after the onset of their symptoms before returning to work.\n",
       "\n",
       "**Laboratory Testing**\n",
       "\n",
       "7. Is laboratory testing necessary for HCP who have recovered from SARS-CoV-2 infection?\n",
       "\t* Answer: No, laboratory testing is not necessary for HCP who have recovered from SARS-CoV-2 infection, as the risk of transmission is low.\n",
       "8. Can HCP who have recovered from SARS-CoV-2 infection be cleared for return to work based on a</td>\n",
       "    </tr>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract 8 questions, options, and their correct answers from the dataset\n",
    "# Extract questions, options, and correct answers from the dataset\n",
    "# Extract 8 questions and their correct answers from the test split\n",
    "num_questions = 8\n",
    "questions = dataset['test'].select(range(num_questions))['question']\n",
    "answers = dataset['test'].select(range(num_questions))['answer']\n",
    "\n",
    "# Define the inference parameters\n",
    "params = {\n",
    "    \"max_new_tokens\": 512,  # Increase this value to allow longer responses\n",
    "    \"top_p\": 0.9,  # Adjust to introduce variability\n",
    "    \"temperature\": 0.0,  # Adjust to introduce variability\n",
    "    \"details\": True,\n",
    "    \"stop\": \"<|eot_id|>\"\n",
    "}\n",
    "\n",
    "# Define the example payloads list\n",
    "example_payloads = [\n",
    "    {\n",
    "        \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    for question in questions\n",
    "]\n",
    "\n",
    "# Function to run inference and collect results\n",
    "def run_inference(predictor, example_payloads):\n",
    "    results = []\n",
    "    for payload in example_payloads:\n",
    "        response = predictor.predict(payload)\n",
    "        response = response[0] if isinstance(response, list) else response\n",
    "        generated_text = response[\"generated_text\"].strip()\n",
    "        \n",
    "        # Check if the response is truncated\n",
    "        if generated_text.endswith(\"...\"):\n",
    "            generated_text += \" [TRUNCATED]\"\n",
    "        \n",
    "        results.append(generated_text)\n",
    "    return results\n",
    "\n",
    "# Run inference with both models\n",
    "print(\"Running inference with fine-tuned LLama 3 8B model...\\n\")\n",
    "results_fine_tuned_8b = run_inference(finetuned_predictor, example_payloads)\n",
    "\n",
    "print(\"Running inference with LLama 3 8B model...\\n\")\n",
    "results_8b = run_inference(llama_3_8b_predictor, example_payloads)\n",
    "\n",
    "\n",
    "# Create a table of the outputs using HTML\n",
    "table_html = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Question</th>\n",
    "        <th>Dataset Answer</th>\n",
    "        <th>Fine-tuned LLama 3 8B Output</th>\n",
    "        <th>LLama 3 8B Output</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i in range(8):\n",
    "    table_html += f\"\"\"\n",
    "    <tr>\n",
    "        <td>{questions[i]}</td>\n",
    "        <td>{answers[i]}</td>\n",
    "        <td>{results_fine_tuned_8b[i]}</td>\n",
    "        <td>{results_8b[i]}</td>\n",
    "    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "# Display the table using HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully demonstrated the process of distillation by fine-tuning and evaluating the LLama 3 8B model using Amazon SageMaker JumpStart. By leveraging the advanced capabilities of the LLama 3.1 405B model, we generated high-quality synthetic data that served as a foundation for fine-tuning the smaller 8B model. This approach allowed us to enhance the performance of the Llama 3 8B model, tailoring it to specific domain tasks and improving its accuracy and effectiveness.\n",
    "\n",
    "### Key Steps Accomplished:\n",
    "1. **Dataset Exploration**: We explored a sample dataset to understand its structure and contents, preparing it for use in model training and evaluation.\n",
    "2. **Data Generation with LLama 3.1 405B**: Utilizing the LLama 3.1 405B model, we generated synthetic data that provided high-quality responses to domain-specific prompts.\n",
    "3. **Distillation by Fine-Tuning LLama 3 8B**: We fine-tuned the LLaMA 3 8B model using the synthetic data, adapting it to better handle specific tasks and improving its overall performance.\n",
    "4. **Model Testing**: We tested the fine-tuned model against a set of evaluation questions, comparing its responses to those of the pre-trained model and assessing the improvements achieved through distillation by fine-tuning.\n",
    "\n",
    "### Results and Insights:\n",
    "- **Enhanced Performance**: The fine-tuned LLama 3 8B model demonstrated significant improvements in generating accurate and contextually relevant responses, showcasing the effectiveness of the fine-tuning process.\n",
    "- **Cost-Effective Adaptation**: By fine-tuning the smaller 8B model with data generated from the larger 405B model, we achieved high performance without the need for extensive computational resources, highlighting a cost-effective approach to model adaptation.\n",
    "- **Scalability and Flexibility**: The workflow outlined in this notebook can be scaled and adapted to various domains and tasks, providing a flexible framework for enhancing the capabilities of language models.\n",
    "\n",
    "### Future Work:\n",
    "- **Further Fine-Tuning**: Additional fine-tuning with more diverse and extensive datasets can further improve the model's performance and adaptability to different domains.\n",
    "- **Real-World Applications**: Deploying the fine-tuned model in real-world applications such as customer support, content generation, and domain-specific research can provide valuable insights and practical benefits.\n",
    "- **Continuous Evaluation**: Ongoing evaluation and monitoring of the model's performance will ensure that it remains effective and relevant as new data and requirements emerge.\n",
    "\n",
    "In conclusion, this notebook has provided a comprehensive guide to generate synthetic data using Llama 3.1 405B and use the generated data for distillation by fine-tuning and evaluating the LLama 3 8B model, demonstrating the potential of using advanced language models to address specific domain needs. By the steps outlined, practitioners can enhance their models' performance, achieve cost-effective adaptations, and unlock new possibilities in natural language processing and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama_3_8b_predictor.delete_predictor()\n",
    "\n",
    "# llama_3_1_405b_predictor.delete_predictor()\n",
    "\n",
    "# finetuned_predictor.delete_predictor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
