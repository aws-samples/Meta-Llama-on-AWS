{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e12e578",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model Distillation Guide using Meta Llama models\n",
    "\n",
    "## Introduction\n",
    "Model distillation is the process of transferring knowledge from a larger more intelligent teacher model to a smaller, faster, and more cost-efficient student model. To use Amazon Bedrock Model Distillation, first select a teacher model whose accuracy you want to achieve. Then, provide use case specific prompts as the task-specific fine-tuning dataset. Note that you can either provide your own prompts in JSON Line (JSONL) format, or you can use Bedrock invocation logs. With invocation logs, Bedrock can either use prompt-response pairs or prompts alone to fine-tune the student model. Amazon Bedrock doesnâ€™t use your data to train any other teacher or student model for public use, and only you can access the final distilled model.\n",
    "\n",
    "Depending on the use case, Amazon Bedrock model distillation may use proprietary data synthesis techniques to generate high-quality responses from the teacher model. Data synthesis techniques might increase the size of the fine-tuning dataset to a maximum of 15K prompt-response pairs. Charges here are billed at the on-demand inference rate of the teacher model. [Click here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-distillation.html) to read more about model distillation from the Amazon Bedrock User Guide. [Click here](https://aws.amazon.com/blogs/machine-learning/a-guide-to-amazon-bedrock-model-distillation-preview/) to read an AWS Blog that that covers the distillation workflow. This notebook goes over Bedrock Model Distillation using **invocation logs**. Llama 3.1 405B is the teacher model and Llama 3.1 8B is the student model.\n",
    "\n",
    "The guide covers essential API operations including:\n",
    "- Creating and configuring distillation jobs\n",
    "- Invoke model to generate invocation logs using ConverseAPI\n",
    "- Working with historical invocation logs in your account to create distillation job\n",
    "- Managing model provisioning and deployment\n",
    "- Running inference with distilled models\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "When using model distillation:\n",
    "1. Ensure your training data is diverse and representative of your use case\n",
    "2. Monitor distillation metrics in the S3 output location\n",
    "3. Evaluate the distilled model's performance against your requirements\n",
    "4. Consider cost-performance tradeoffs when selecting model units for deployment\n",
    "\n",
    "The distilled model should provide faster responses and lower costs while maintaining acceptable performance for your specific use case.\n",
    "\n",
    "## Setup and Prerequisites\n",
    "\n",
    "Before starting with model distillation, ensure you have the following:\n",
    "\n",
    "#### Required AWS Resources:\n",
    "- An AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing invocation logs \n",
    "- An S3 bucket to store output metrics\n",
    "- Sufficient service quota to use Provisioned Throughput in Bedrock\n",
    "- An IAM role with the following permissions:\n",
    "\n",
    "IAM Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET/*\",\n",
    "                \"arn:aws:s3:::YOUR_INVOCATION_LOG_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_INVOCATION_LOG_BUCKET/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:ListModelCustomizationJobs\",\n",
    "                \"bedrock:StopModelCustomizationJob\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Trust Relationship:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"YOUR_ACCOUNT_ID\"\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Dataset:\n",
    "In this notebook we will be using a modified version of the [Paul Graham Essay dataset](https://github.com/run-llama/llama_index/tree/main/llama-datasets/paul_graham_essay) from [LlamaIndex](https://docs.llamaindex.ai/en/latest/). LlamaIndex is a framework for building LLM-powered agents, and Retrieval Augmented Generation (RAG) is a core technique for building data-backed LLM applications. LlamaIndex has public datasets consisting of queries, reference contexts, and responses. Open the file named paul_graham.json to inspect the queries, context, and responses. Open paul_graham_source.txt to get a feel for the information source written about [Paul Graham](https://www.paulgraham.com/articles.html). For your own distillation jobs, you can use datasets that are specific to your use case.\n",
    "\n",
    "We also cover the creation of a RAG dataset using a Llama model. Readers can use this technique to generate their own datasets for distillation using Amazon Bedrock.\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sts_client = session.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8536fd-fe76-47e4-aa5c-9f2b829cc52a",
   "metadata": {},
   "source": [
    "Let's first test the response from the model before it is distilled. We take a question from one of the prompts of the paul_graham.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1722e5d-db2a-4c99-871f-26a631f3b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Specify the model ID as Meta Llama 3.1 8B\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Define the message with the prompt\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": \"In the essay, the author discusses his initial interest in AI and his eventual disillusionment with it. According to the author, what were the two main influences that initially drew him to AI and what realization led him to believe that the approach to AI during his time was a hoax?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=model_id,\n",
    "    messages=[message],  \n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 150,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": .5\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Extract the 'text' content from the response\n",
    "    text_content = response['output']['message']['content'][0]['text']\n",
    "    \n",
    "    # Replace newline escape sequences ('\\\\n') with actual newline characters\n",
    "    formatted_text = text_content.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Print the formatted text\n",
    "    print(formatted_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Unable to invoke '{model_id}'. Reason: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567bcdb-50c7-4287-9c21-f1c767f6d73f",
   "metadata": {},
   "source": [
    "As expected, the response does not reflect the information in the Paul Graham essay. Compare the response above with the response from the paul_graham.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1b977-9578-45da-9627-b86e280f2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paul_graham_rag_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data[\"examples\"][1][\"reference_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf4606",
   "metadata": {},
   "source": [
    "####  Model selection\n",
    "When selecting models for distillation, consider:\n",
    "1. Performance targets\n",
    "2. Latency requirements\n",
    "3. Total Cost of Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0555707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup teacher and student model pairs\n",
    "teacher_model_id = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "student_model = \"meta.llama3-1-8b-instruct-v1:0:128k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ef8cf",
   "metadata": {},
   "source": [
    "### Step 1. Configure Model Invocation Logging using the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a869312",
   "metadata": {},
   "source": [
    "In this example, we only store loggings to S3 bucket, but you can optionally enable logging in Cloudwatch as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce95174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and prefix to store invocation logs\n",
    "s3_bucket_for_log = \"\"\n",
    "prefix_for_log = \"\" # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_s3_bucket_policy(bucket_name, prefix, account_id, region):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    bucket_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"AmazonBedrockLogsWrite\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": [\n",
    "                    \"s3:PutObject\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                     f\"arn:aws:s3:::{bucket_name}/{prefix}/AWSLogs/{account_id}/BedrockModelInvocationLogs/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnLike\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    bucket_policy_string = json.dumps(bucket_policy)\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.put_bucket_policy(\n",
    "            Bucket=bucket_name,\n",
    "            Policy=bucket_policy_string\n",
    "        )\n",
    "        print(\"Successfully set bucket policy\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting bucket policy: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bucket policy\n",
    "setup_s3_bucket_policy(s3_bucket_for_log, prefix_for_log, account_id, region)\n",
    "\n",
    "# Setup logging configuration\n",
    "bedrock_client.put_model_invocation_logging_configuration(\n",
    "    loggingConfig={\n",
    "        's3Config': {\n",
    "            'bucketName': s3_bucket_for_log,\n",
    "            'keyPrefix': prefix_for_log\n",
    "        },\n",
    "        'textDataDeliveryEnabled': True,\n",
    "        'imageDataDeliveryEnabled': True,\n",
    "        'embeddingDataDeliveryEnabled': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05fb69-8129-4902-8630-b4ad64a4cf20",
   "metadata": {},
   "source": [
    "### Step 2: Prepare input dataset in jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ebef6-bd25-4e87-a06a-c68bd9b384d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that the JSONL fine-tuning dataset will be named paul_graham.jsonl\n",
    "output_jsonl = \"paul_graham.jsonl\"\n",
    "\n",
    "# Load the original JSON file\n",
    "with open('paul_graham_rag_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    data=data['examples']\n",
    "    with open(output_jsonl, mode=\"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "        for row in data:\n",
    "            # Construct the JSON object for each line\n",
    "            json_line = {\n",
    "            \"prompt\": (\n",
    "                \"You are a historian charged with answering questions about Paul Graham.\\n\"\n",
    "                \"Given the context below, answer the following question.\\n\\n\"\n",
    "                f\"<context>\\nDocument 1: {row.get('reference_contexts', '')}\\n</context>\\n\\n\"\n",
    "                f\"<question>{row.get('query', '')}</question>\"\n",
    "            ),\n",
    "            \"completion\": row.get('reference_answer', '')  \n",
    "            }\n",
    "            # Write the JSON object as a line in the JSONL file\n",
    "            jsonl_file.write(json.dumps(json_line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64f36a-69bf-43f3-9bac-0e41b81f9ea5",
   "metadata": {},
   "source": [
    "Note that you need [at least 100 prompt response](https://docs.aws.amazon.com/bedrock/latest/userguide/distillation-data-prep-option-2.html) pairs. paul_graham.jsonl currently only has 44 prompt response pairs. Continue below to generate more synthetic prompt response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f934a3-01d0-4c2b-ae35-614c1a5a5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path, encoding='utf-8'):\n",
    "    \"\"\"Reads a text file and returns its content.\"\"\"\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.read()\n",
    "\n",
    "def split_text(text, chunk_size=1500, chunk_overlap=100, separators=None):\n",
    "    \"\"\"Splits text into chunks with optional overlap and custom separators.\"\"\"\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    # Split text into sentences or paragraphs first\n",
    "    for sep in separators:\n",
    "        if sep in text:\n",
    "            parts = text.split(sep)\n",
    "            for part in parts:\n",
    "                if len(current_chunk) + len(part) <= chunk_size:\n",
    "                    current_chunk += part + sep\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        # Handle overlap\n",
    "                        overlap_start = max(0, len(current_chunk) - chunk_overlap)\n",
    "                        current_chunk = current_chunk[overlap_start:] + part + sep\n",
    "                    else:\n",
    "                        current_chunk = part + sep\n",
    "            break  # Use the first separator that matches\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load the text file\n",
    "text_content = read_text_file(\"paul_graham_source.txt\")\n",
    "\n",
    "# Split the text into chunks\n",
    "essay_chunks = split_text(\n",
    "    text_content,\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Print the first few chunks for verification\n",
    "for i in range(0,3):\n",
    "    print(f\"Chunk number {i+1}:\\n\" + essay_chunks[i] + '\\n*****\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4895a8-d105-4d12-8526-df76d8d5bd63",
   "metadata": {},
   "source": [
    "Next, we need to generate sample prompts and answers. For simplicity, we generate 60 prompt and answer pairs. This process can be refined using the techniques in this [AWS Blog](https://aws.amazon.com/blogs/machine-learning/generate-synthetic-data-for-evaluating-rag-systems-using-amazon-bedrock/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfacab-34b2-4a96-8859-dab6a6bf73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Specify the model ID\n",
    "# model_id = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "# We append the results to our JSONL dataset\n",
    "for i in range(0, 67):\n",
    "    # Modify the sleep time to avoid throttling\n",
    "    time.sleep(15)\n",
    "    output_jsonl = \"paul_graham.jsonl\"\n",
    "    context = essay_chunks[i]\n",
    "    prompt_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \n",
    "            f\"\"\"\n",
    "            <Instructions>\n",
    "            Here is some context:\n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "        \n",
    "            Your task is to generate 1 question that can be answered using the provided context, following these rules:\n",
    "        \n",
    "            <rules>\n",
    "            1. The question should make sense to humans even when read without the given context.\n",
    "            2. The question should be fully answered from the given context.\n",
    "            3. The question should be framed from a part of context that contains important information. It can also be from tables, code, etc.\n",
    "            4. The answer to the question should not contain any links.\n",
    "            5. The question should be of moderate difficulty.\n",
    "            6. The question must be reasonable and must be understood and responded by humans.\n",
    "            7. Do not use phrases like 'provided context', etc. in the question.\n",
    "            8. Avoid framing questions using the word \"and\" that can be decomposed into more than one question.\n",
    "            9. The question should not contain more than 10 words, make use of abbreviations wherever possible.\n",
    "            </rules>\n",
    "        \n",
    "            To generate the question, first identify the most important or relevant part of the context. Then frame a question around that part that satisfies all the rules above.\n",
    "        \n",
    "            Output only the generated question with a \"?\" at the end, no other text or characters.\n",
    "            </Instructions>\n",
    "            \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[prompt_message],  \n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 1000,\n",
    "            \"temperature\": 0.2,\n",
    "            \"topP\": .5\n",
    "        }\n",
    "    )\n",
    "    generated_prompt = response['output']['message']['content'][0]['text']\n",
    "    # Replace newline escape sequences ('\\\\n') with actual newline characters\n",
    "    question = generated_prompt.replace('\\\\n', '\\n')\n",
    "    # Print the formatted text\n",
    "    print(question)\n",
    "\n",
    "    # Generate answer\n",
    "    answer_prompt_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \n",
    "            f\"\"\"\n",
    "            <Instructions>\n",
    "            <Task>\n",
    "            <role>You are an experienced QA Engineer for building large language model applications.</role>\n",
    "            <task>It is your task to generate an answer to the following question <question>{question}</question> only based on the <context>{context}</context></task>\n",
    "            The output should be only the answer generated from the context.\n",
    "        \n",
    "            <rules>\n",
    "            1. Only use the given context as a source for generating the answer.\n",
    "            2. Be as precise as possible with answering the question.\n",
    "            3. Be concise in answering the question and only answer the question at hand rather than adding extra information.\n",
    "            </rules>\n",
    "        \n",
    "            Only output the generated answer as a sentence. No extra characters.\n",
    "            </Task>\n",
    "            </Instructions>\n",
    "            \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[answer_prompt_message],  \n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 1000,\n",
    "            \"temperature\": 0.2,\n",
    "            \"topP\": .5\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    generated_answer = response['output']['message']['content'][0]['text']\n",
    "        \n",
    "    # Replace newline escape sequences ('\\\\n') with actual newline characters\n",
    "    answer = generated_answer.replace('\\\\n', '\\n')\n",
    "        \n",
    "    # Print the formatted text\n",
    "    print(answer)\n",
    "\n",
    "    with open(output_jsonl, mode=\"a\", encoding=\"utf-8\") as jsonl_file:\n",
    "        # Construct the JSON object for each line\n",
    "        json_line = {\n",
    "        \"prompt\": (\n",
    "            \"You are a historian charged with answering questions about Paul Graham.\\n\"\n",
    "            \"Given the context below, answer the following question.\\n\\n\"\n",
    "            f\"<context>\\nDocument 1: {context}\\n</context>\\n\\n\"\n",
    "            f\"<question>{question}</question>\"\n",
    "        ),\n",
    "        \"completion\": answer\n",
    "        }\n",
    "        # Write the JSON object as a line in the JSONL file\n",
    "        jsonl_file.write(json.dumps(json_line) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861563a",
   "metadata": {},
   "source": [
    "### Step 4. Invoke teacher model to generate logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9a866",
   "metadata": {},
   "source": [
    "We're using [ConverseAPI](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) in this example, but you can also use [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) in Bedrock.\n",
    "\n",
    "We will invoke `Llama3.1 405b` to generate response on the `Paul Graham Essay` dataset for each input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inference params\n",
    "inference_config = {\"maxTokens\": 2048, \"temperature\": 0.1, \"topP\": 0.9}\n",
    "request_metadata = {\"job_type\": \"paulgraham\",\n",
    "                    \"use_case\": \"RAG\",\n",
    "                    # Options to avoid throttling\n",
    "                    \"invoke_model\": \"llama31-405b\"}\n",
    "                    # \"invoke_model\": \"llama31-70b\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58144f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The following code sample takes about 30mins to complete, which invokes teacher model to generate invocation logs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paul_graham.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        prompt = data['prompt']\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=teacher_model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig=inference_config,\n",
    "            requestMetadata=request_metadata\n",
    "        )\n",
    "\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd720171",
   "metadata": {},
   "source": [
    "### Step 3. Configure and submit distillation job using historical invocation logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b1c7e",
   "metadata": {},
   "source": [
    "Now we have enough logs in our S3 bucket, let's configure and submit our distillation job using historical invocation logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe5a41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Please make sure to update <b>role_arn</b> and <b>output_path</b> in the following code sample\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3bbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique names for the job and model\n",
    "job_name = f\"distillation-job-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Set maximum response length\n",
    "max_response_length = 1000\n",
    "\n",
    "# Setup IAM role\n",
    "role_arn = \"\" # Replace by your IAM role configured for distillation job (Update everything starting with < and ending with >)\n",
    "\n",
    "# Invocation_logs_data\n",
    "invocation_logs_data = f\"s3://{s3_bucket_for_log}/{prefix_for_log}/AWSLogs\"\n",
    "output_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training data using invocation logs\n",
    "training_data_config = {\n",
    "    'invocationLogsConfig': {\n",
    "        'usePromptResponse': True, # By default it is set as \"False\"\n",
    "        'invocationLogSource': {\n",
    "            's3Uri': invocation_logs_data\n",
    "        },\n",
    "        'requestMetadataFilters': { # Replace by our filter\n",
    "            'equals': {\"job_type\": \"Example\"},\n",
    "            'equals': {\"use_case\": \"RAG\"},\n",
    "            'equals': {\"invoke_model\": \"llama31-405b\"},\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distillation job with invocation logs\n",
    "response = bedrock_client.create_model_customization_job(\n",
    "    # Addition\n",
    "    jobName=job_name+'0',\n",
    "    customModelName=model_name,\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=student_model,\n",
    "    customizationType=\"DISTILLATION\",\n",
    "    trainingDataConfig=training_data_config,\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    customizationConfig={\n",
    "        \"distillationConfig\": {\n",
    "            \"teacherModelConfig\": {\n",
    "                \"teacherModelIdentifier\": teacher_model_id,\n",
    "                \"maxResponseLengthForInference\": max_response_length\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe5a47",
   "metadata": {},
   "source": [
    "### Step 4. Monitoring distillation job status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45ac29",
   "metadata": {},
   "source": [
    "After submitted your distillation job, you can run the following code to monitor the job status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59b688",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Please be aware that distillation job could run for up to 7 days\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78caec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the distillation job arn\n",
    "job_arn = response['jobArn']\n",
    "\n",
    "# print job status\n",
    "job_status = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)[\"status\"]\n",
    "print(job_status)\n",
    "\n",
    "# Hugo's addition:\n",
    "print(job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82104f86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Proceed to following sections only when the status shows <b>Complete</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc9af3",
   "metadata": {},
   "source": [
    "### Step 5. Deploying the Distilled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f326a",
   "metadata": {},
   "source": [
    "After distillation is complete, you'll need to set up Provisioned Throughput to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the distilled model\n",
    "custom_model_id = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)['outputModelArn']\n",
    "distilled_model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "provisioned_model_id = bedrock_client.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=distilled_model_name,\n",
    "    modelId=custom_model_id \n",
    ")['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276e891",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Check the provisioned throughput status, proceed until it shows **InService**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd85ba1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print pt status\n",
    "pt_status = bedrock_client.get_provisioned_model_throughput(provisionedModelId=provisioned_model_id)['status']\n",
    "print(pt_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67503550",
   "metadata": {},
   "source": [
    "### Step 6. Run inference with provisioned throughput units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30ef9a",
   "metadata": {},
   "source": [
    "In this example, we use ConverseAPI to invoke the distilled model, you can use both InvokeModel or ConverseAPI to generate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95150c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference with the distilled model\n",
    "input_prompt = \"<Your input prompt here>\"  # Replace by your input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a28e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [ \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [{\"text\": input_prompt}], \n",
    "    } \n",
    "]\n",
    "inferenceConfig = {\n",
    "    \"maxTokens\": 2048, \n",
    "    \"temperature\": 0.1, \n",
    "    \"topP\": 0.9\n",
    "    }\n",
    "\n",
    "# test the deloyed model\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=provisioned_model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig=inferenceConfig,\n",
    ")\n",
    "response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbdbb1",
   "metadata": {},
   "source": [
    "### (Optional) Model Copy and Share\n",
    "\n",
    "If you want to deploy the model to a `different AWS Region` or a `different AWS account`, you can use `Model Share` and `Model Copy` feature of Amazon Bedrock. Please check the following notebook for more information.\n",
    "\n",
    "[Sample notebook](https://github.com/aws-samples/amazon-bedrock-samples/blob/main_archieve_10_06_2024/custom_models/model_copy/cross-region-copy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4db642",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 7. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0836",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "After you're done with the experiment, please ensure to **delete** the provisioned throughput model unit to avoid unnecessary cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f33ac0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response = bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b09f00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, we've walked through the entire process of model distillation using Amazon Bedrock with historical model invocation logs. We covered:\n",
    "\n",
    "1. Setting up the environment and configuring necessary AWS resources\n",
    "2. Configuring model invocation logging using the API\n",
    "3. Invoking the teacher model to generate logs\n",
    "4. Configuring and submitting a distillation job using historical invocation logs\n",
    "5. Monitoring the distillation job's progress\n",
    "6. Deploying the distilled model using Provisioned Throughput\n",
    "7. Running inference with the distilled model\n",
    "8. Optional model copy and share procedures\n",
    "9. Cleaning up resources\n",
    "\n",
    "Remember to always consider your specific use case requirements when selecting models, configuring the distillation process, and filtering invocation logs. The ability to use actual production data from your model invocations can lead to distilled models that are highly optimized for your particular applications.\n",
    "\n",
    "With these tools and techniques at your disposal, you're well-equipped to leverage the power of model distillation to optimize your AI/ML workflows in Amazon Bedrock.\n",
    "\n",
    "**Happy distilling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
