{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4011e2bc-41f6-446c-b83f-d01f671c6b5d",
   "metadata": {},
   "source": [
    "# Llama 4 on Bedrock: Long context window use cases\n",
    "\n",
    "This notebook demonstrates how to use Llama 4 on AWS by deploying on SageMaker JumpStart. We will cover the deployment process, basic invocations to run inference, and important use cases that can be used with Llama 4 Scout's 10 million token window.\n",
    "\n",
    "## Llama 4\n",
    "\n",
    "Llama 4 represents Meta’s most advanced multimodal models to date, featuring a mixture of experts (MoE) architecture and context window support up to 10 million tokens. With native multimodality and early fusion technology, Meta states that these new models demonstrate unprecedented performance across text and vision tasks while maintaining efficient compute requirements. With a dramatic increase on supported context length from 128K in Llama 3, Llama 4 is now suitable for multi-document summarization, parsing extensive user activity for personalized tasks, and reasoning over extensive codebases. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to Llama 4 on Bedrock enabled via AWS console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3d9f36-82c4-4dd3-992a-52c7fdd4fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f838c1-b174-4b56-a494-a603701eda69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4a8858-6294-4669-ae57-c2b442f2cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_llama_4_maverick = 'us.meta.llama4-maverick-17b-instruct-v1:0'\n",
    "meta_llama_4_scout = 'us.meta.llama4-scout-17b-instruct-v1:0'\n",
    "\n",
    "model_id = meta_llama_4_scout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05f0547-c27c-4a89-b6e1-0947e62cbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your payload with the messages format\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"You are a helpful AI assistant.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"What are three key benefits of large language models for businesses?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baa642d-bac0-4116-8a56-a4f2921a9bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) offer several benefits to businesses, and here are three key advantages:\n",
      "\n",
      "1. **Improved Customer Service and Support**: LLMs can be integrated with chatbots and virtual assistants to provide 24/7 customer support. They can understand and respond to customer inquiries, resolve issues, and even route complex problems to human customer support agents. This can lead to increased customer satisfaction, reduced support costs, and improved overall customer experience.\n",
      "\n",
      "2. **Enhanced Content Generation and Automation**: LLMs can generate high-quality content, such as blog posts, social media posts, product descriptions, and email newsletters, at a much faster pace than humans. This can help businesses save time and resources, while also increasing their content output and consistency. Additionally, LLMs can automate tasks like data analysis, reporting, and summarization, freeing up human resources for more strategic and creative work.\n",
      "\n",
      "3. **Data Analysis and Insights**: LLMs can analyze vast amounts of data, including text, sentiment, and trends, to provide businesses with valuable insights and recommendations. They can help identify patterns, predict customer behavior, and optimize business processes. This can enable businesses to make data-driven decisions, improve their operations, and stay competitive in their respective markets.\n",
      "\n",
      "These benefits can have a significant impact on a business's efficiency, customer engagement, and bottom line, making LLMs a valuable tool for companies looking to stay ahead in today's fast-paced digital landscape.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_client.converse(\n",
    "    modelId=model_id,\n",
    "    messages=payload[\"messages\"],  # Wrap the message in a list\n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 2000,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": .5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4bce8-d78b-4959-b290-086aa8201a60",
   "metadata": {},
   "source": [
    "# Codebase Analysis \n",
    "\n",
    "Using Llama 4 Scout’s industry-leading context window, this section showcases its ability to deeply analyze expansive codebases. The example extracts and contextualizes the buildspec-1-10-2.yml file from the AWS Deep Learning Containers GitHub repository, illustrating how the model synthesizes information across an entire repository. We used a tool to ingest the whole repository into plaintext that we provided to the model as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a67009ef-27af-4049-ad76-328cfb634afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "def read_markdown_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of a markdown file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the markdown file to be read\n",
    "        \n",
    "    Returns:\n",
    "        str: The content of the file as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def summarize_text_bedrock(text, model_id, max_length=10000):\n",
    "    \"\"\"\n",
    "    Summarize the provided text using Amazon Bedrock's converse API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to summarize\n",
    "        model_id (str): Bedrock model ID to use\n",
    "        max_length (int): Maximum length of text to send (to avoid token limits)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Summarized text, Latency metrics)\n",
    "    \"\"\"\n",
    "    # Configure the Bedrock client with extended timeout\n",
    "    config = Config(read_timeout=300, connect_timeout=30)\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\", config=config)\n",
    "    \n",
    "    # Truncate text if necessary to avoid exceeding token limits\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length] + \"... [Content truncated due to length]\"\n",
    "    \n",
    "    # Start measuring preprocessing time\n",
    "    preprocess_start = time.time()\n",
    "    \n",
    "    # Format the messages correctly for Bedrock's converse API\n",
    "    # First, create two separate messages\n",
    "    system_instruction = \"You are a helpful AI assistant that summarizes codebases to me to help me understand how to analyze code by synthesizing through the entire codebase before responding. Be thorough in your search as the file may be nested within a markdown code block or within a directory listing.\"\n",
    "    \n",
    "    user_question = f\"Can you explain to me the buildspec-1-10-2.yml file and how it relates to the rest of the huggingface directory? Use this information:\\n\\n{text} as reference.\"\n",
    "    \n",
    "    # Format messages as per Llama 4 Bedrock requirements\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": system_instruction  # Include system instruction as first user message\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Measure payload size\n",
    "    payload_size = len(json.dumps({\"messages\": messages}).encode('utf-8'))\n",
    "    \n",
    "    preprocess_end = time.time()\n",
    "    preprocess_time = preprocess_end - preprocess_start\n",
    "    \n",
    "    # Invoke the Bedrock model and measure latency\n",
    "    api_start = time.time()\n",
    "    try:\n",
    "        # Use invoke_model first to test if converse is the issue\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 4000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Bedrock converse: {e}\")\n",
    "        # Try using invoke_model instead as a fallback\n",
    "        try:\n",
    "            # Format for invoke_model is different\n",
    "            payload = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": user_question}\n",
    "                ],\n",
    "                \"max_tokens\": 2000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            body = json.dumps(payload)\n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=body\n",
    "            )\n",
    "            \n",
    "            # Parse the response from invoke_model\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            # Convert to a format similar to converse for consistent handling\n",
    "            response = {\n",
    "                \"output\": {\n",
    "                    \"content\": [\n",
    "                        {\"text\": response_body.get('content', '') or response_body.get('generation', '')}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback also failed: {e2}\")\n",
    "            raise\n",
    "    \n",
    "    api_end = time.time()\n",
    "    api_latency = api_end - api_start\n",
    "    \n",
    "    # Start measuring postprocessing time\n",
    "    postprocess_start = time.time()\n",
    "    \n",
    "    # Extract the summary from the response\n",
    "    try:\n",
    "        summary = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        summary = str(response)\n",
    "    \n",
    "    # Calculate output size\n",
    "    output_size = len(summary.encode('utf-8'))\n",
    "    \n",
    "    postprocess_end = time.time()\n",
    "    postprocess_time = postprocess_end - postprocess_start\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        'api_latency': api_latency,\n",
    "        'preprocess_time': preprocess_time,\n",
    "        'postprocess_time': postprocess_time,\n",
    "        'total_latency': preprocess_time + api_latency + postprocess_time,\n",
    "        'payload_size_bytes': payload_size,\n",
    "        'output_size_bytes': output_size,\n",
    "        'payload_tokens': len(text) / 4,  # rough approximation of tokens\n",
    "    }\n",
    "    \n",
    "    return summary, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da9950b1-41a9-4a17-874e-26f8c6e1a695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading markdown file from /home/ec2-user/SageMaker/repomix-output.md...\n",
      "File read time: 0.1781 seconds\n",
      "Generating summary using Bedrock...\n",
      "\n",
      "--- LATENCY METRICS ---\n",
      "API call latency: 6.0696 seconds\n",
      "Preprocessing time: 0.0001 seconds\n",
      "Postprocessing time: 0.0000 seconds\n",
      "Total latency: 6.0696 seconds\n",
      "Payload size: 10.69 KB\n",
      "Output size: 3.44 KB\n",
      "Approximate input tokens: 2509\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "To understand the `buildspec-1-10-2.yml` file and its relation to the rest of the `huggingface` directory, we first need to locate the file within the provided directory structure.\n",
      "\n",
      "Upon examining the directory structure, we find that the `huggingface` directory contains several subdirectories, including `pytorch`, which further contains `inference` and `training` directories. However, the `buildspec-1-10-2.yml` file is not directly listed under the `huggingface` directory or its immediate subdirectories.\n",
      "\n",
      "To proceed, let's search for the file within the provided code repository representation.\n",
      "\n",
      "## File: huggingface/build_artifacts/training/buildspec-1-10-2.yml\n",
      "\n",
      "```yml\n",
      "# Contents of buildspec-1-10-2.yml\n",
      "```\n",
      "\n",
      "Since the actual contents of `buildspec-1-10-2.yml` are not provided in the given text, let's infer its purpose based on similar files and the context.\n",
      "\n",
      "`buildspec` files are typically used in AWS CodeBuild to define the build process. They contain a series of commands and settings that CodeBuild uses to build, test, and deploy software.\n",
      "\n",
      "Given that `buildspec-1-10-2.yml` is located under `huggingface/build_artifacts/training/`, it's likely related to building or training models using PyTorch or other frameworks within the Hugging Face ecosystem.\n",
      "\n",
      "### Relation to the Rest of the `huggingface` Directory\n",
      "\n",
      "The `huggingface` directory appears to be organized around different aspects of the Hugging Face framework, including PyTorch and inference/training Docker containers.\n",
      "\n",
      "1. **PyTorch and Docker Containers**: The presence of `pytorch/inference/docker/` and various versioned subdirectories (e.g., `1.10/`, `1.13/`) indicates that the repository is managing Docker containers for different PyTorch versions and configurations (e.g., CPU, GPU, Neuron).\n",
      "\n",
      "2. **Build Artifacts**: The `build_artifacts` directory contains files related to the build process, including `buildspec` files for different versions and configurations.\n",
      "\n",
      "3. **Inference and Training**: The distinction between `inference` and `training` suggests that the repository is handling both the deployment (inference) and the training of models.\n",
      "\n",
      "The `buildspec-1-10-2.yml` file likely plays a role in the build process for a specific version or configuration of the Hugging Face PyTorch training or inference setup. Without the exact contents, we can infer that it contains instructions for AWS CodeBuild to build, test, or deploy a particular aspect of the Hugging Face framework, possibly related to PyTorch version 1.10.\n",
      "\n",
      "To fully understand its relation and purpose, examining the contents of `buildspec-1-10-2.yml` and comparing it with other `buildspec` files in the repository (e.g., `buildspec.yml`, other versioned `buildspec` files) would be necessary.\n",
      "\n",
      "### Steps for Further Analysis\n",
      "\n",
      "1. **Locate and Examine `buildspec-1-10-2.yml`**: Review the file's contents to understand the specific build commands and configurations it defines.\n",
      "\n",
      "2. **Compare with Other `buildspec` Files**: Analyze differences and similarities between `buildspec-1-10-2.yml` and other `buildspec` files to understand version-specific or configuration-specific changes.\n",
      "\n",
      "3. **Understand the Build Process**: Study how AWS CodeBuild uses `buildspec` files and how the defined commands and settings apply to the Hugging Face framework's build, test, and deployment processes.\n",
      "\n",
      "By following these steps, you can gain a deeper understanding of how `buildspec-1-10-2.yml` fits into the overall structure and build process of the `huggingface` directory.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "file_path = '/home/ec2-user/SageMaker/repomix-output.md'  # Update with actual markdown file\n",
    "model_id = 'us.meta.llama4-maverick-17b-instruct-v1:0'  # Use your desired Bedrock model ID\n",
    "\n",
    "# Read the markdown file\n",
    "print(f\"Reading markdown file from {file_path}...\")\n",
    "read_start = time.time()\n",
    "markdown_content = read_markdown_file(file_path)\n",
    "read_end = time.time()\n",
    "read_time = read_end - read_start\n",
    "print(f\"File read time: {read_time:.4f} seconds\")\n",
    "\n",
    "# Summarize the content with Bedrock\n",
    "print(\"Generating summary using Bedrock...\")\n",
    "summary, latency_metrics = summarize_text_bedrock(markdown_content, model_id)\n",
    "\n",
    "# Print the latency metrics\n",
    "print(\"\\n--- LATENCY METRICS ---\")\n",
    "print(f\"API call latency: {latency_metrics['api_latency']:.4f} seconds\")\n",
    "print(f\"Preprocessing time: {latency_metrics['preprocess_time']:.4f} seconds\")\n",
    "print(f\"Postprocessing time: {latency_metrics['postprocess_time']:.4f} seconds\")\n",
    "print(f\"Total latency: {latency_metrics['total_latency']:.4f} seconds\")\n",
    "print(f\"Payload size: {latency_metrics['payload_size_bytes'] / 1024:.2f} KB\")\n",
    "print(f\"Output size: {latency_metrics['output_size_bytes'] / 1024:.2f} KB\")\n",
    "print(f\"Approximate input tokens: {int(latency_metrics['payload_tokens'])}\")\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\n--- SUMMARY ---\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a599f-2b97-44d0-86a9-60af764d0c52",
   "metadata": {},
   "source": [
    "# Multi-Doc Processing\n",
    "\n",
    "With Llama 4 Scout's 3.5million token context window on Bedrock - Llama 4 Scout excels in multi-document processing. In this example, the model extracts key financial metrics from Amazon 10-K reports (2017-2024), demonstrating its capability to integrate and analyze data spanning multiple years—all without the need for additional processing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8d3731-2a15-455b-84de-30af3bfe6d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01db56e5-1994-4c06-91d8-7860c3519e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import PyPDF2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from botocore.config import Config\n",
    "\n",
    "# Define helper functions\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"PDF has {len(pdf_reader.pages)} pages\")\n",
    "            # Extract text from first 100 pages (adjust as needed)\n",
    "            for page_num in range(min(100, len(pdf_reader.pages))):\n",
    "                print(f\"Processing page {page_num+1}/{len(pdf_reader.pages)}...\")\n",
    "                text += pdf_reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def retry_with_exponential_backoff(func, max_retries=5, initial_backoff=5):\n",
    "    \"\"\"Execute a function with exponential backoff retry.\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'response') and 'Error' in e.response and e.response['Error'].get('Code') == 'ThrottlingException':\n",
    "                wait_time = initial_backoff * (2 ** retries) + random.uniform(0, 1)\n",
    "                print(f\"Throttling detected. Retrying after {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "                if retries > max_retries:\n",
    "                    print(f\"Maximum retries ({max_retries}) exceeded.\")\n",
    "                    raise\n",
    "            else:\n",
    "                print(f\"Non-throttling error: {e}\")\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "def extract_key_data_from_pdfs(pdf_files, bedrock_client, model_id):\n",
    "    \"\"\"Extract and summarize key data from each PDF file using Amazon Bedrock.\"\"\"\n",
    "    pdf_summaries = {}\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = os.path.basename(pdf_file)\n",
    "        print(f\"\\n--- Extracting key data from {file_name} ---\")\n",
    "        \n",
    "        pdf_text = extract_text_from_pdf(pdf_file)\n",
    "        if not pdf_text:\n",
    "            print(f\"Failed to extract text from {file_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Use a smaller chunk size to avoid token limits\n",
    "        max_text_length = 10000  # Reduced from 25000 to avoid throttling\n",
    "        truncated_text = pdf_text[:max_text_length]\n",
    "        \n",
    "        # Create system instruction and user question\n",
    "        system_instruction = \"You are a financial analyst. Extract key financial data from this 10-K report.\"\n",
    "        \n",
    "        user_question = f\"\"\"Extract the following information from this Amazon 10-K report:\n",
    "1. What year is this report for?\n",
    "2. Total revenue for the year\n",
    "3. Net income\n",
    "4. Key business segments and their performance\n",
    "5. Important trends mentioned\n",
    "\n",
    "Here's the 10-K text (truncated):\n",
    "{truncated_text}\n",
    "\n",
    "Format your response as a JSON object with these fields: year, revenue, net_income, segments, trends.\n",
    "\"\"\"\n",
    "        \n",
    "        # Format messages according to the example\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": system_instruction\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": user_question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            print(f\"Extracting key data from {file_name}...\")\n",
    "            \n",
    "            # Call Bedrock's Converse API with retry logic\n",
    "            def make_api_call():\n",
    "                return bedrock_client.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=messages,\n",
    "                    inferenceConfig={\n",
    "                        \"maxTokens\": 1500,  # Reduced from 2048\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"topP\": 0.9\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            # Add a delay before making the API call to avoid throttling\n",
    "            time.sleep(2)\n",
    "            response = retry_with_exponential_backoff(make_api_call)\n",
    "            \n",
    "            # Extract content from response based on the format\n",
    "            try:\n",
    "                summary = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "                pdf_summaries[file_name] = summary\n",
    "                print(f\"Successfully extracted key data from {file_name}\")\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "                print(f\"Response structure: {json.dumps(response, default=str)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return pdf_summaries\n",
    "\n",
    "def find_pdf_files(directory):\n",
    "    \"\"\"Find all PDF files in the specified directory.\"\"\"\n",
    "    pdf_files = []\n",
    "    try:\n",
    "        # Look for all PDF files in the directory\n",
    "        pdf_pattern = os.path.join(directory, \"*.pdf\")\n",
    "        pdf_files = glob.glob(pdf_pattern)\n",
    "        \n",
    "        if pdf_files:\n",
    "            print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "            for pdf in pdf_files:\n",
    "                print(f\"  - {os.path.basename(pdf)}\")\n",
    "        else:\n",
    "            print(f\"No PDF files found in {directory}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding PDF files: {e}\")\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "def synthesize_and_answer_question(pdf_summaries, question, bedrock_client, model_id):\n",
    "    \"\"\"Synthesize information from all PDFs and answer a specific question.\"\"\"\n",
    "    # Combine all summaries - limiting context size to avoid throttling\n",
    "    combined_context = \"\"\n",
    "    for pdf_name, summary in pdf_summaries.items():\n",
    "        combined_context += f\"--- Data from {pdf_name} ---\\n{summary[:2000]}\\n\\n\"  # Limit each summary\n",
    "    \n",
    "    # Create system instruction and user question\n",
    "    system_instruction = \"\"\"You are an expert financial analyst comparing Amazon's 10-K reports across multiple years.\n",
    "Your task is to synthesize information from these reports and answer questions accurately.\n",
    "Make sure to compare data across years when relevant and highlight significant changes or trends.\"\"\"\n",
    "    \n",
    "    user_question = f\"\"\"I've analyzed multiple Amazon 10-K reports and extracted the key information from each:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Based on this information from all the reports, please answer the following question:\n",
    "{question}\n",
    "\n",
    "Provide a comprehensive answer that compares information across all available years.\"\"\"\n",
    "\n",
    "    # Format messages according to your example\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": system_instruction\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nSynthesizing information and answering question...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call Bedrock's Converse API with retry logic\n",
    "        def make_api_call():\n",
    "            return bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 2000,  # Reduced from 4096\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Add a delay before making the API call to avoid throttling\n",
    "        time.sleep(2)    \n",
    "        response = retry_with_exponential_backoff(make_api_call)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Extract the response content\n",
    "        try:\n",
    "            answer = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            return answer\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            try:\n",
    "                print(f\"Response structure: {json.dumps(response, default=str)}\")\n",
    "            except:\n",
    "                print(f\"Cannot display response structure\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error synthesizing information: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d628c8-1241-4738-bf0b-2dfbe50189b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Amazon Bedrock client...\n",
      "Using model: us.meta.llama4-scout-17b-instruct-v1:0\n",
      "Found 10 PDF files:\n",
      "  - amazon10k_2022.pdf\n",
      "  - amazon_10k_2020.pdf\n",
      "  - amazon_10k_2015.pdf\n",
      "  - amazon_10k_2016.pdf\n",
      "  - amazon_10k_2018.pdf\n",
      "  - amazon_10k_2021.pdf\n",
      "  - amazon_10k_2024.pdf\n",
      "  - amazon_10k_2019.pdf\n",
      "  - amazon_10k_2017.pdf\n",
      "  - amazon_10k_2023.pdf\n",
      "\n",
      "Extracting key data from all PDFs...\n",
      "\n",
      "--- Extracting key data from amazon10k_2022.pdf ---\n",
      "PDF has 194 pages\n",
      "Processing page 1/194...\n",
      "Processing page 2/194...\n",
      "Processing page 3/194...\n",
      "Processing page 4/194...\n",
      "Processing page 5/194...\n",
      "Processing page 6/194...\n",
      "Processing page 7/194...\n",
      "Processing page 8/194...\n",
      "Processing page 9/194...\n",
      "Processing page 10/194...\n",
      "Processing page 11/194...\n",
      "Processing page 12/194...\n",
      "Processing page 13/194...\n",
      "Processing page 14/194...\n",
      "Processing page 15/194...\n",
      "Processing page 16/194...\n",
      "Processing page 17/194...\n",
      "Processing page 18/194...\n",
      "Processing page 19/194...\n",
      "Processing page 20/194...\n",
      "Processing page 21/194...\n",
      "Processing page 22/194...\n",
      "Processing page 23/194...\n",
      "Processing page 24/194...\n",
      "Processing page 25/194...\n",
      "Processing page 26/194...\n",
      "Processing page 27/194...\n",
      "Processing page 28/194...\n",
      "Processing page 29/194...\n",
      "Processing page 30/194...\n",
      "Processing page 31/194...\n",
      "Processing page 32/194...\n",
      "Processing page 33/194...\n",
      "Processing page 34/194...\n",
      "Processing page 35/194...\n",
      "Processing page 36/194...\n",
      "Processing page 37/194...\n",
      "Processing page 38/194...\n",
      "Processing page 39/194...\n",
      "Processing page 40/194...\n",
      "Processing page 41/194...\n",
      "Processing page 42/194...\n",
      "Processing page 43/194...\n",
      "Processing page 44/194...\n",
      "Processing page 45/194...\n",
      "Processing page 46/194...\n",
      "Processing page 47/194...\n",
      "Processing page 48/194...\n",
      "Processing page 49/194...\n",
      "Processing page 50/194...\n",
      "Processing page 51/194...\n",
      "Processing page 52/194...\n",
      "Processing page 53/194...\n",
      "Processing page 54/194...\n",
      "Processing page 55/194...\n",
      "Processing page 56/194...\n",
      "Processing page 57/194...\n",
      "Processing page 58/194...\n",
      "Processing page 59/194...\n",
      "Processing page 60/194...\n",
      "Processing page 61/194...\n",
      "Processing page 62/194...\n",
      "Processing page 63/194...\n",
      "Processing page 64/194...\n",
      "Processing page 65/194...\n",
      "Processing page 66/194...\n",
      "Processing page 67/194...\n",
      "Processing page 68/194...\n",
      "Processing page 69/194...\n",
      "Processing page 70/194...\n",
      "Processing page 71/194...\n",
      "Processing page 72/194...\n",
      "Processing page 73/194...\n",
      "Processing page 74/194...\n",
      "Processing page 75/194...\n",
      "Processing page 76/194...\n",
      "Processing page 77/194...\n",
      "Processing page 78/194...\n",
      "Processing page 79/194...\n",
      "Processing page 80/194...\n",
      "Processing page 81/194...\n",
      "Processing page 82/194...\n",
      "Processing page 83/194...\n",
      "Processing page 84/194...\n",
      "Processing page 85/194...\n",
      "Processing page 86/194...\n",
      "Processing page 87/194...\n",
      "Processing page 88/194...\n",
      "Processing page 89/194...\n",
      "Processing page 90/194...\n",
      "Processing page 91/194...\n",
      "Processing page 92/194...\n",
      "Processing page 93/194...\n",
      "Processing page 94/194...\n",
      "Processing page 95/194...\n",
      "Processing page 96/194...\n",
      "Processing page 97/194...\n",
      "Processing page 98/194...\n",
      "Processing page 99/194...\n",
      "Processing page 100/194...\n",
      "Extracting key data from amazon10k_2022.pdf...\n",
      "Successfully extracted key data from amazon10k_2022.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2020.pdf ---\n",
      "PDF has 83 pages\n",
      "Processing page 1/83...\n",
      "Processing page 2/83...\n",
      "Processing page 3/83...\n",
      "Processing page 4/83...\n",
      "Processing page 5/83...\n",
      "Processing page 6/83...\n",
      "Processing page 7/83...\n",
      "Processing page 8/83...\n",
      "Processing page 9/83...\n",
      "Processing page 10/83...\n",
      "Processing page 11/83...\n",
      "Processing page 12/83...\n",
      "Processing page 13/83...\n",
      "Processing page 14/83...\n",
      "Processing page 15/83...\n",
      "Processing page 16/83...\n",
      "Processing page 17/83...\n",
      "Processing page 18/83...\n",
      "Processing page 19/83...\n",
      "Processing page 20/83...\n",
      "Processing page 21/83...\n",
      "Processing page 22/83...\n",
      "Processing page 23/83...\n",
      "Processing page 24/83...\n",
      "Processing page 25/83...\n",
      "Processing page 26/83...\n",
      "Processing page 27/83...\n",
      "Processing page 28/83...\n",
      "Processing page 29/83...\n",
      "Processing page 30/83...\n",
      "Processing page 31/83...\n",
      "Processing page 32/83...\n",
      "Processing page 33/83...\n",
      "Processing page 34/83...\n",
      "Processing page 35/83...\n",
      "Processing page 36/83...\n",
      "Processing page 37/83...\n",
      "Processing page 38/83...\n",
      "Processing page 39/83...\n",
      "Processing page 40/83...\n",
      "Processing page 41/83...\n",
      "Processing page 42/83...\n",
      "Processing page 43/83...\n",
      "Processing page 44/83...\n",
      "Processing page 45/83...\n",
      "Processing page 46/83...\n",
      "Processing page 47/83...\n",
      "Processing page 48/83...\n",
      "Processing page 49/83...\n",
      "Processing page 50/83...\n",
      "Processing page 51/83...\n",
      "Processing page 52/83...\n",
      "Processing page 53/83...\n",
      "Processing page 54/83...\n",
      "Processing page 55/83...\n",
      "Processing page 56/83...\n",
      "Processing page 57/83...\n",
      "Processing page 58/83...\n",
      "Processing page 59/83...\n",
      "Processing page 60/83...\n",
      "Processing page 61/83...\n",
      "Processing page 62/83...\n",
      "Processing page 63/83...\n",
      "Processing page 64/83...\n",
      "Processing page 65/83...\n",
      "Processing page 66/83...\n",
      "Processing page 67/83...\n",
      "Processing page 68/83...\n",
      "Processing page 69/83...\n",
      "Processing page 70/83...\n",
      "Processing page 71/83...\n",
      "Processing page 72/83...\n",
      "Processing page 73/83...\n",
      "Processing page 74/83...\n",
      "Processing page 75/83...\n",
      "Processing page 76/83...\n",
      "Processing page 77/83...\n",
      "Processing page 78/83...\n",
      "Processing page 79/83...\n",
      "Processing page 80/83...\n",
      "Processing page 81/83...\n",
      "Processing page 82/83...\n",
      "Processing page 83/83...\n",
      "Extracting key data from amazon_10k_2020.pdf...\n",
      "Successfully extracted key data from amazon_10k_2020.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2015.pdf ---\n",
      "PDF has 90 pages\n",
      "Processing page 1/90...\n",
      "Processing page 2/90...\n",
      "Processing page 3/90...\n",
      "Processing page 4/90...\n",
      "Processing page 5/90...\n",
      "Processing page 6/90...\n",
      "Processing page 7/90...\n",
      "Processing page 8/90...\n",
      "Processing page 9/90...\n",
      "Processing page 10/90...\n",
      "Processing page 11/90...\n",
      "Processing page 12/90...\n",
      "Processing page 13/90...\n",
      "Processing page 14/90...\n",
      "Processing page 15/90...\n",
      "Processing page 16/90...\n",
      "Processing page 17/90...\n",
      "Processing page 18/90...\n",
      "Processing page 19/90...\n",
      "Processing page 20/90...\n",
      "Processing page 21/90...\n",
      "Processing page 22/90...\n",
      "Processing page 23/90...\n",
      "Processing page 24/90...\n",
      "Processing page 25/90...\n",
      "Processing page 26/90...\n",
      "Processing page 27/90...\n",
      "Processing page 28/90...\n",
      "Processing page 29/90...\n",
      "Processing page 30/90...\n",
      "Processing page 31/90...\n",
      "Processing page 32/90...\n",
      "Processing page 33/90...\n",
      "Processing page 34/90...\n",
      "Processing page 35/90...\n",
      "Processing page 36/90...\n",
      "Processing page 37/90...\n",
      "Processing page 38/90...\n",
      "Processing page 39/90...\n",
      "Processing page 40/90...\n",
      "Processing page 41/90...\n",
      "Processing page 42/90...\n",
      "Processing page 43/90...\n",
      "Processing page 44/90...\n",
      "Processing page 45/90...\n",
      "Processing page 46/90...\n",
      "Processing page 47/90...\n",
      "Processing page 48/90...\n",
      "Processing page 49/90...\n",
      "Processing page 50/90...\n",
      "Processing page 51/90...\n",
      "Processing page 52/90...\n",
      "Processing page 53/90...\n",
      "Processing page 54/90...\n",
      "Processing page 55/90...\n",
      "Processing page 56/90...\n",
      "Processing page 57/90...\n",
      "Processing page 58/90...\n",
      "Processing page 59/90...\n",
      "Processing page 60/90...\n",
      "Processing page 61/90...\n",
      "Processing page 62/90...\n",
      "Processing page 63/90...\n",
      "Processing page 64/90...\n",
      "Processing page 65/90...\n",
      "Processing page 66/90...\n",
      "Processing page 67/90...\n",
      "Processing page 68/90...\n",
      "Processing page 69/90...\n",
      "Processing page 70/90...\n",
      "Processing page 71/90...\n",
      "Processing page 72/90...\n",
      "Processing page 73/90...\n",
      "Processing page 74/90...\n",
      "Processing page 75/90...\n",
      "Processing page 76/90...\n",
      "Processing page 77/90...\n",
      "Processing page 78/90...\n",
      "Processing page 79/90...\n",
      "Processing page 80/90...\n",
      "Processing page 81/90...\n",
      "Processing page 82/90...\n",
      "Processing page 83/90...\n",
      "Processing page 84/90...\n",
      "Processing page 85/90...\n",
      "Processing page 86/90...\n",
      "Processing page 87/90...\n",
      "Processing page 88/90...\n",
      "Processing page 89/90...\n",
      "Processing page 90/90...\n",
      "Extracting key data from amazon_10k_2015.pdf...\n",
      "Successfully extracted key data from amazon_10k_2015.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2016.pdf ---\n",
      "PDF has 85 pages\n",
      "Processing page 1/85...\n",
      "Processing page 2/85...\n",
      "Processing page 3/85...\n",
      "Processing page 4/85...\n",
      "Processing page 5/85...\n",
      "Processing page 6/85...\n",
      "Processing page 7/85...\n",
      "Processing page 8/85...\n",
      "Processing page 9/85...\n",
      "Processing page 10/85...\n",
      "Processing page 11/85...\n",
      "Processing page 12/85...\n",
      "Processing page 13/85...\n",
      "Processing page 14/85...\n",
      "Processing page 15/85...\n",
      "Processing page 16/85...\n",
      "Processing page 17/85...\n",
      "Processing page 18/85...\n",
      "Processing page 19/85...\n",
      "Processing page 20/85...\n",
      "Processing page 21/85...\n",
      "Processing page 22/85...\n",
      "Processing page 23/85...\n",
      "Processing page 24/85...\n",
      "Processing page 25/85...\n",
      "Processing page 26/85...\n",
      "Processing page 27/85...\n",
      "Processing page 28/85...\n",
      "Processing page 29/85...\n",
      "Processing page 30/85...\n",
      "Processing page 31/85...\n",
      "Processing page 32/85...\n",
      "Processing page 33/85...\n",
      "Processing page 34/85...\n",
      "Processing page 35/85...\n",
      "Processing page 36/85...\n",
      "Processing page 37/85...\n",
      "Processing page 38/85...\n",
      "Processing page 39/85...\n",
      "Processing page 40/85...\n",
      "Processing page 41/85...\n",
      "Processing page 42/85...\n",
      "Processing page 43/85...\n",
      "Processing page 44/85...\n",
      "Processing page 45/85...\n",
      "Processing page 46/85...\n",
      "Processing page 47/85...\n",
      "Processing page 48/85...\n",
      "Processing page 49/85...\n",
      "Processing page 50/85...\n",
      "Processing page 51/85...\n",
      "Processing page 52/85...\n",
      "Processing page 53/85...\n",
      "Processing page 54/85...\n",
      "Processing page 55/85...\n",
      "Processing page 56/85...\n",
      "Processing page 57/85...\n",
      "Processing page 58/85...\n",
      "Processing page 59/85...\n",
      "Processing page 60/85...\n",
      "Processing page 61/85...\n",
      "Processing page 62/85...\n",
      "Processing page 63/85...\n",
      "Processing page 64/85...\n",
      "Processing page 65/85...\n",
      "Processing page 66/85...\n",
      "Processing page 67/85...\n",
      "Processing page 68/85...\n",
      "Processing page 69/85...\n",
      "Processing page 70/85...\n",
      "Processing page 71/85...\n",
      "Processing page 72/85...\n",
      "Processing page 73/85...\n",
      "Processing page 74/85...\n",
      "Processing page 75/85...\n",
      "Processing page 76/85...\n",
      "Processing page 77/85...\n",
      "Processing page 78/85...\n",
      "Processing page 79/85...\n",
      "Processing page 80/85...\n",
      "Processing page 81/85...\n",
      "Processing page 82/85...\n",
      "Processing page 83/85...\n",
      "Processing page 84/85...\n",
      "Processing page 85/85...\n",
      "Extracting key data from amazon_10k_2016.pdf...\n",
      "Successfully extracted key data from amazon_10k_2016.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2018.pdf ---\n",
      "PDF has 85 pages\n",
      "Processing page 1/85...\n",
      "Processing page 2/85...\n",
      "Processing page 3/85...\n",
      "Processing page 4/85...\n",
      "Processing page 5/85...\n",
      "Processing page 6/85...\n",
      "Processing page 7/85...\n",
      "Processing page 8/85...\n",
      "Processing page 9/85...\n",
      "Processing page 10/85...\n",
      "Processing page 11/85...\n",
      "Processing page 12/85...\n",
      "Processing page 13/85...\n",
      "Processing page 14/85...\n",
      "Processing page 15/85...\n",
      "Processing page 16/85...\n",
      "Processing page 17/85...\n",
      "Processing page 18/85...\n",
      "Processing page 19/85...\n",
      "Processing page 20/85...\n",
      "Processing page 21/85...\n",
      "Processing page 22/85...\n",
      "Processing page 23/85...\n",
      "Processing page 24/85...\n",
      "Processing page 25/85...\n",
      "Processing page 26/85...\n",
      "Processing page 27/85...\n",
      "Processing page 28/85...\n",
      "Processing page 29/85...\n",
      "Processing page 30/85...\n",
      "Processing page 31/85...\n",
      "Processing page 32/85...\n",
      "Processing page 33/85...\n",
      "Processing page 34/85...\n",
      "Processing page 35/85...\n",
      "Processing page 36/85...\n",
      "Processing page 37/85...\n",
      "Processing page 38/85...\n",
      "Processing page 39/85...\n",
      "Processing page 40/85...\n",
      "Processing page 41/85...\n",
      "Processing page 42/85...\n",
      "Processing page 43/85...\n",
      "Processing page 44/85...\n",
      "Processing page 45/85...\n",
      "Processing page 46/85...\n",
      "Processing page 47/85...\n",
      "Processing page 48/85...\n",
      "Processing page 49/85...\n",
      "Processing page 50/85...\n",
      "Processing page 51/85...\n",
      "Processing page 52/85...\n",
      "Processing page 53/85...\n",
      "Processing page 54/85...\n",
      "Processing page 55/85...\n",
      "Processing page 56/85...\n",
      "Processing page 57/85...\n",
      "Processing page 58/85...\n",
      "Processing page 59/85...\n",
      "Processing page 60/85...\n",
      "Processing page 61/85...\n",
      "Processing page 62/85...\n",
      "Processing page 63/85...\n",
      "Processing page 64/85...\n",
      "Processing page 65/85...\n",
      "Processing page 66/85...\n",
      "Processing page 67/85...\n",
      "Processing page 68/85...\n",
      "Processing page 69/85...\n",
      "Processing page 70/85...\n",
      "Processing page 71/85...\n",
      "Processing page 72/85...\n",
      "Processing page 73/85...\n",
      "Processing page 74/85...\n",
      "Processing page 75/85...\n",
      "Processing page 76/85...\n",
      "Processing page 77/85...\n",
      "Processing page 78/85...\n",
      "Processing page 79/85...\n",
      "Processing page 80/85...\n",
      "Processing page 81/85...\n",
      "Processing page 82/85...\n",
      "Processing page 83/85...\n",
      "Processing page 84/85...\n",
      "Processing page 85/85...\n",
      "Extracting key data from amazon_10k_2018.pdf...\n",
      "Successfully extracted key data from amazon_10k_2018.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2021.pdf ---\n",
      "PDF has 80 pages\n",
      "Processing page 1/80...\n",
      "Processing page 2/80...\n",
      "Processing page 3/80...\n",
      "Processing page 4/80...\n",
      "Processing page 5/80...\n",
      "Processing page 6/80...\n",
      "Processing page 7/80...\n",
      "Processing page 8/80...\n",
      "Processing page 9/80...\n",
      "Processing page 10/80...\n",
      "Processing page 11/80...\n",
      "Processing page 12/80...\n",
      "Processing page 13/80...\n",
      "Processing page 14/80...\n",
      "Processing page 15/80...\n",
      "Processing page 16/80...\n",
      "Processing page 17/80...\n",
      "Processing page 18/80...\n",
      "Processing page 19/80...\n",
      "Processing page 20/80...\n",
      "Processing page 21/80...\n",
      "Processing page 22/80...\n",
      "Processing page 23/80...\n",
      "Processing page 24/80...\n",
      "Processing page 25/80...\n",
      "Processing page 26/80...\n",
      "Processing page 27/80...\n",
      "Processing page 28/80...\n",
      "Processing page 29/80...\n",
      "Processing page 30/80...\n",
      "Processing page 31/80...\n",
      "Processing page 32/80...\n",
      "Processing page 33/80...\n",
      "Processing page 34/80...\n",
      "Processing page 35/80...\n",
      "Processing page 36/80...\n",
      "Processing page 37/80...\n",
      "Processing page 38/80...\n",
      "Processing page 39/80...\n",
      "Processing page 40/80...\n",
      "Processing page 41/80...\n",
      "Processing page 42/80...\n",
      "Processing page 43/80...\n",
      "Processing page 44/80...\n",
      "Processing page 45/80...\n",
      "Processing page 46/80...\n",
      "Processing page 47/80...\n",
      "Processing page 48/80...\n",
      "Processing page 49/80...\n",
      "Processing page 50/80...\n",
      "Processing page 51/80...\n",
      "Processing page 52/80...\n",
      "Processing page 53/80...\n",
      "Processing page 54/80...\n",
      "Processing page 55/80...\n",
      "Processing page 56/80...\n",
      "Processing page 57/80...\n",
      "Processing page 58/80...\n",
      "Processing page 59/80...\n",
      "Processing page 60/80...\n",
      "Processing page 61/80...\n",
      "Processing page 62/80...\n",
      "Processing page 63/80...\n",
      "Processing page 64/80...\n",
      "Processing page 65/80...\n",
      "Processing page 66/80...\n",
      "Processing page 67/80...\n",
      "Processing page 68/80...\n",
      "Processing page 69/80...\n",
      "Processing page 70/80...\n",
      "Processing page 71/80...\n",
      "Processing page 72/80...\n",
      "Processing page 73/80...\n",
      "Processing page 74/80...\n",
      "Processing page 75/80...\n",
      "Processing page 76/80...\n",
      "Processing page 77/80...\n",
      "Processing page 78/80...\n",
      "Processing page 79/80...\n",
      "Processing page 80/80...\n",
      "Extracting key data from amazon_10k_2021.pdf...\n",
      "Successfully extracted key data from amazon_10k_2021.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2024.pdf ---\n",
      "PDF has 94 pages\n",
      "Processing page 1/94...\n",
      "Processing page 2/94...\n",
      "Processing page 3/94...\n",
      "Processing page 4/94...\n",
      "Processing page 5/94...\n",
      "Processing page 6/94...\n",
      "Processing page 7/94...\n",
      "Processing page 8/94...\n",
      "Processing page 9/94...\n",
      "Processing page 10/94...\n",
      "Processing page 11/94...\n",
      "Processing page 12/94...\n",
      "Processing page 13/94...\n",
      "Processing page 14/94...\n",
      "Processing page 15/94...\n",
      "Processing page 16/94...\n",
      "Processing page 17/94...\n",
      "Processing page 18/94...\n",
      "Processing page 19/94...\n",
      "Processing page 20/94...\n",
      "Processing page 21/94...\n",
      "Processing page 22/94...\n",
      "Processing page 23/94...\n",
      "Processing page 24/94...\n",
      "Processing page 25/94...\n",
      "Processing page 26/94...\n",
      "Processing page 27/94...\n",
      "Processing page 28/94...\n",
      "Processing page 29/94...\n",
      "Processing page 30/94...\n",
      "Processing page 31/94...\n",
      "Processing page 32/94...\n",
      "Processing page 33/94...\n",
      "Processing page 34/94...\n",
      "Processing page 35/94...\n",
      "Processing page 36/94...\n",
      "Processing page 37/94...\n",
      "Processing page 38/94...\n",
      "Processing page 39/94...\n",
      "Processing page 40/94...\n",
      "Processing page 41/94...\n",
      "Processing page 42/94...\n",
      "Processing page 43/94...\n",
      "Processing page 44/94...\n",
      "Processing page 45/94...\n",
      "Processing page 46/94...\n",
      "Processing page 47/94...\n",
      "Processing page 48/94...\n",
      "Processing page 49/94...\n",
      "Processing page 50/94...\n",
      "Processing page 51/94...\n",
      "Processing page 52/94...\n",
      "Processing page 53/94...\n",
      "Processing page 54/94...\n",
      "Processing page 55/94...\n",
      "Processing page 56/94...\n",
      "Processing page 57/94...\n",
      "Processing page 58/94...\n",
      "Processing page 59/94...\n",
      "Processing page 60/94...\n",
      "Processing page 61/94...\n",
      "Processing page 62/94...\n",
      "Processing page 63/94...\n",
      "Processing page 64/94...\n",
      "Processing page 65/94...\n",
      "Processing page 66/94...\n",
      "Processing page 67/94...\n",
      "Processing page 68/94...\n",
      "Processing page 69/94...\n",
      "Processing page 70/94...\n",
      "Processing page 71/94...\n",
      "Processing page 72/94...\n",
      "Processing page 73/94...\n",
      "Processing page 74/94...\n",
      "Processing page 75/94...\n",
      "Processing page 76/94...\n",
      "Processing page 77/94...\n",
      "Processing page 78/94...\n",
      "Processing page 79/94...\n",
      "Processing page 80/94...\n",
      "Processing page 81/94...\n",
      "Processing page 82/94...\n",
      "Processing page 83/94...\n",
      "Processing page 84/94...\n",
      "Processing page 85/94...\n",
      "Processing page 86/94...\n",
      "Processing page 87/94...\n",
      "Processing page 88/94...\n",
      "Processing page 89/94...\n",
      "Processing page 90/94...\n",
      "Processing page 91/94...\n",
      "Processing page 92/94...\n",
      "Processing page 93/94...\n",
      "Processing page 94/94...\n",
      "Extracting key data from amazon_10k_2024.pdf...\n",
      "Successfully extracted key data from amazon_10k_2024.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2019.pdf ---\n",
      "PDF has 89 pages\n",
      "Processing page 1/89...\n",
      "Processing page 2/89...\n",
      "Processing page 3/89...\n",
      "Processing page 4/89...\n",
      "Processing page 5/89...\n",
      "Processing page 6/89...\n",
      "Processing page 7/89...\n",
      "Processing page 8/89...\n",
      "Processing page 9/89...\n",
      "Processing page 10/89...\n",
      "Processing page 11/89...\n",
      "Processing page 12/89...\n",
      "Processing page 13/89...\n",
      "Processing page 14/89...\n",
      "Processing page 15/89...\n",
      "Processing page 16/89...\n",
      "Processing page 17/89...\n",
      "Processing page 18/89...\n",
      "Processing page 19/89...\n",
      "Processing page 20/89...\n",
      "Processing page 21/89...\n",
      "Processing page 22/89...\n",
      "Processing page 23/89...\n",
      "Processing page 24/89...\n",
      "Processing page 25/89...\n",
      "Processing page 26/89...\n",
      "Processing page 27/89...\n",
      "Processing page 28/89...\n",
      "Processing page 29/89...\n",
      "Processing page 30/89...\n",
      "Processing page 31/89...\n",
      "Processing page 32/89...\n",
      "Processing page 33/89...\n",
      "Processing page 34/89...\n",
      "Processing page 35/89...\n",
      "Processing page 36/89...\n",
      "Processing page 37/89...\n",
      "Processing page 38/89...\n",
      "Processing page 39/89...\n",
      "Processing page 40/89...\n",
      "Processing page 41/89...\n",
      "Processing page 42/89...\n",
      "Processing page 43/89...\n",
      "Processing page 44/89...\n",
      "Processing page 45/89...\n",
      "Processing page 46/89...\n",
      "Processing page 47/89...\n",
      "Processing page 48/89...\n",
      "Processing page 49/89...\n",
      "Processing page 50/89...\n",
      "Processing page 51/89...\n",
      "Processing page 52/89...\n",
      "Processing page 53/89...\n",
      "Processing page 54/89...\n",
      "Processing page 55/89...\n",
      "Processing page 56/89...\n",
      "Processing page 57/89...\n",
      "Processing page 58/89...\n",
      "Processing page 59/89...\n",
      "Processing page 60/89...\n",
      "Processing page 61/89...\n",
      "Processing page 62/89...\n",
      "Processing page 63/89...\n",
      "Processing page 64/89...\n",
      "Processing page 65/89...\n",
      "Processing page 66/89...\n",
      "Processing page 67/89...\n",
      "Processing page 68/89...\n",
      "Processing page 69/89...\n",
      "Processing page 70/89...\n",
      "Processing page 71/89...\n",
      "Processing page 72/89...\n",
      "Processing page 73/89...\n",
      "Processing page 74/89...\n",
      "Processing page 75/89...\n",
      "Processing page 76/89...\n",
      "Processing page 77/89...\n",
      "Processing page 78/89...\n",
      "Processing page 79/89...\n",
      "Processing page 80/89...\n",
      "Processing page 81/89...\n",
      "Processing page 82/89...\n",
      "Processing page 83/89...\n",
      "Processing page 84/89...\n",
      "Processing page 85/89...\n",
      "Processing page 86/89...\n",
      "Processing page 87/89...\n",
      "Processing page 88/89...\n",
      "Processing page 89/89...\n",
      "Extracting key data from amazon_10k_2019.pdf...\n",
      "Successfully extracted key data from amazon_10k_2019.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2017.pdf ---\n",
      "PDF has 85 pages\n",
      "Processing page 1/85...\n",
      "Processing page 2/85...\n",
      "Processing page 3/85...\n",
      "Processing page 4/85...\n",
      "Processing page 5/85...\n",
      "Processing page 6/85...\n",
      "Processing page 7/85...\n",
      "Processing page 8/85...\n",
      "Processing page 9/85...\n",
      "Processing page 10/85...\n",
      "Processing page 11/85...\n",
      "Processing page 12/85...\n",
      "Processing page 13/85...\n",
      "Processing page 14/85...\n",
      "Processing page 15/85...\n",
      "Processing page 16/85...\n",
      "Processing page 17/85...\n",
      "Processing page 18/85...\n",
      "Processing page 19/85...\n",
      "Processing page 20/85...\n",
      "Processing page 21/85...\n",
      "Processing page 22/85...\n",
      "Processing page 23/85...\n",
      "Processing page 24/85...\n",
      "Processing page 25/85...\n",
      "Processing page 26/85...\n",
      "Processing page 27/85...\n",
      "Processing page 28/85...\n",
      "Processing page 29/85...\n",
      "Processing page 30/85...\n",
      "Processing page 31/85...\n",
      "Processing page 32/85...\n",
      "Processing page 33/85...\n",
      "Processing page 34/85...\n",
      "Processing page 35/85...\n",
      "Processing page 36/85...\n",
      "Processing page 37/85...\n",
      "Processing page 38/85...\n",
      "Processing page 39/85...\n",
      "Processing page 40/85...\n",
      "Processing page 41/85...\n",
      "Processing page 42/85...\n",
      "Processing page 43/85...\n",
      "Processing page 44/85...\n",
      "Processing page 45/85...\n",
      "Processing page 46/85...\n",
      "Processing page 47/85...\n",
      "Processing page 48/85...\n",
      "Processing page 49/85...\n",
      "Processing page 50/85...\n",
      "Processing page 51/85...\n",
      "Processing page 52/85...\n",
      "Processing page 53/85...\n",
      "Processing page 54/85...\n",
      "Processing page 55/85...\n",
      "Processing page 56/85...\n",
      "Processing page 57/85...\n",
      "Processing page 58/85...\n",
      "Processing page 59/85...\n",
      "Processing page 60/85...\n",
      "Processing page 61/85...\n",
      "Processing page 62/85...\n",
      "Processing page 63/85...\n",
      "Processing page 64/85...\n",
      "Processing page 65/85...\n",
      "Processing page 66/85...\n",
      "Processing page 67/85...\n",
      "Processing page 68/85...\n",
      "Processing page 69/85...\n",
      "Processing page 70/85...\n",
      "Processing page 71/85...\n",
      "Processing page 72/85...\n",
      "Processing page 73/85...\n",
      "Processing page 74/85...\n",
      "Processing page 75/85...\n",
      "Processing page 76/85...\n",
      "Processing page 77/85...\n",
      "Processing page 78/85...\n",
      "Processing page 79/85...\n",
      "Processing page 80/85...\n",
      "Processing page 81/85...\n",
      "Processing page 82/85...\n",
      "Processing page 83/85...\n",
      "Processing page 84/85...\n",
      "Processing page 85/85...\n",
      "Extracting key data from amazon_10k_2017.pdf...\n",
      "Successfully extracted key data from amazon_10k_2017.pdf\n",
      "\n",
      "--- Extracting key data from amazon_10k_2023.pdf ---\n",
      "PDF has 81 pages\n",
      "Processing page 1/81...\n",
      "Processing page 2/81...\n",
      "Processing page 3/81...\n",
      "Processing page 4/81...\n",
      "Processing page 5/81...\n",
      "Processing page 6/81...\n",
      "Processing page 7/81...\n",
      "Processing page 8/81...\n",
      "Processing page 9/81...\n",
      "Processing page 10/81...\n",
      "Processing page 11/81...\n",
      "Processing page 12/81...\n",
      "Processing page 13/81...\n",
      "Processing page 14/81...\n",
      "Processing page 15/81...\n",
      "Processing page 16/81...\n",
      "Processing page 17/81...\n",
      "Processing page 18/81...\n",
      "Processing page 19/81...\n",
      "Processing page 20/81...\n",
      "Processing page 21/81...\n",
      "Processing page 22/81...\n",
      "Processing page 23/81...\n",
      "Processing page 24/81...\n",
      "Processing page 25/81...\n",
      "Processing page 26/81...\n",
      "Processing page 27/81...\n",
      "Processing page 28/81...\n",
      "Processing page 29/81...\n",
      "Processing page 30/81...\n",
      "Processing page 31/81...\n",
      "Processing page 32/81...\n",
      "Processing page 33/81...\n",
      "Processing page 34/81...\n",
      "Processing page 35/81...\n",
      "Processing page 36/81...\n",
      "Processing page 37/81...\n",
      "Processing page 38/81...\n",
      "Processing page 39/81...\n",
      "Processing page 40/81...\n",
      "Processing page 41/81...\n",
      "Processing page 42/81...\n",
      "Processing page 43/81...\n",
      "Processing page 44/81...\n",
      "Processing page 45/81...\n",
      "Processing page 46/81...\n",
      "Processing page 47/81...\n",
      "Processing page 48/81...\n",
      "Processing page 49/81...\n",
      "Processing page 50/81...\n",
      "Processing page 51/81...\n",
      "Processing page 52/81...\n",
      "Processing page 53/81...\n",
      "Processing page 54/81...\n",
      "Processing page 55/81...\n",
      "Processing page 56/81...\n",
      "Processing page 57/81...\n",
      "Processing page 58/81...\n",
      "Processing page 59/81...\n",
      "Processing page 60/81...\n",
      "Processing page 61/81...\n",
      "Processing page 62/81...\n",
      "Processing page 63/81...\n",
      "Processing page 64/81...\n",
      "Processing page 65/81...\n",
      "Processing page 66/81...\n",
      "Processing page 67/81...\n",
      "Processing page 68/81...\n",
      "Processing page 69/81...\n",
      "Processing page 70/81...\n",
      "Processing page 71/81...\n",
      "Processing page 72/81...\n",
      "Processing page 73/81...\n",
      "Processing page 74/81...\n",
      "Processing page 75/81...\n",
      "Processing page 76/81...\n",
      "Processing page 77/81...\n",
      "Processing page 78/81...\n",
      "Processing page 79/81...\n",
      "Processing page 80/81...\n",
      "Processing page 81/81...\n",
      "Extracting key data from amazon_10k_2023.pdf...\n",
      "Successfully extracted key data from amazon_10k_2023.pdf\n",
      "\n",
      "Successfully extracted data from the following PDFs:\n",
      "  - amazon10k_2022.pdf\n",
      "  - amazon_10k_2020.pdf\n",
      "  - amazon_10k_2015.pdf\n",
      "  - amazon_10k_2016.pdf\n",
      "  - amazon_10k_2018.pdf\n",
      "  - amazon_10k_2021.pdf\n",
      "  - amazon_10k_2024.pdf\n",
      "  - amazon_10k_2019.pdf\n",
      "  - amazon_10k_2017.pdf\n",
      "  - amazon_10k_2023.pdf\n",
      "\n",
      "Answering question: How did Amazon's revenue and net income change over time?\n",
      "\n",
      "Synthesizing information and answering question...\n",
      "Time taken: 6.91 seconds\n",
      "\n",
      "==================================================\n",
      "ANSWER to: How did Amazon's revenue and net income change over time?\n",
      "==================================================\n",
      "## Amazon's Revenue and Net Income Change Over Time\n",
      "\n",
      "To analyze how Amazon's revenue and net income changed over time, we will compare the data from the available years: 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, and 2023.\n",
      "\n",
      "### Revenue Comparison\n",
      "\n",
      "| Year | Revenue          |\n",
      "|-------|------------------|\n",
      "| 2014  | $89 billion      |\n",
      "| 2015  | $107.01 billion  |\n",
      "| 2016  | Not directly provided, but approximately $135.99 billion |\n",
      "| 2017  | $177.87 billion  |\n",
      "| 2018  | Not directly provided, but approximately $232.88 billion |\n",
      "| 2019  | $280.52 billion  |\n",
      "| 2020  | $386.06 billion  |\n",
      "| 2021  | $478 billion     |\n",
      "| 2022  | $513.98 billion  |\n",
      "| 2023  | Not directly provided, but approximately $574.75 billion |\n",
      "\n",
      "### Net Income Comparison\n",
      "\n",
      "| Year | Net Income       |\n",
      "|-------|------------------|\n",
      "| 2014  | $236 million     |\n",
      "| 2015  | $596 million     |\n",
      "| 2016  | Not directly provided, but approximately $2.37 billion |\n",
      "| 2017  | $3.03 billion    |\n",
      "| 2018  | Not directly provided, but approximately $10.07 billion |\n",
      "| 2019  | $18.75 billion   |\n",
      "| 2020  | $18.39 billion   |\n",
      "| 2021  | $33.4 billion    |\n",
      "| 2022  | $18.39 billion   |\n",
      "| 2023  | Not directly provided |\n",
      "\n",
      "## Analysis\n",
      "\n",
      "1. **Revenue Growth**: Amazon's revenue has consistently increased over the years, showing a significant growth trend. From $89 billion in 2014 to $513.98 billion in 2022, there's a substantial increase. The revenue growth rate has been impressive, with a compound annual growth rate (CAGR) that reflects Amazon's expanding business operations and market presence.\n",
      "\n",
      "2. **Net Income Fluctuations**: Net income has also shown an overall increase but with more fluctuations. Starting from $236 million in 2014, it reached $33.4 billion in 2021. However, there was a notable decrease in 2022 to $18.39 billion, which could be attributed to various factors such as increased costs, market conditions, or strategic investments.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Amazon's revenue and net income have generally increased over the years, reflecting the company's growth and expansion into new markets and services. However, the net income has experienced fluctuations, indicating the challenges and investments Amazon faces in its diverse business operations.\n",
      "\n",
      "Answering question: What are the main business segments driving growth at Amazon over the years?\n",
      "\n",
      "Synthesizing information and answering question...\n",
      "Time taken: 9.81 seconds\n",
      "\n",
      "==================================================\n",
      "ANSWER to: What are the main business segments driving growth at Amazon over the years?\n",
      "==================================================\n",
      "## Main Business Segments Driving Growth at Amazon\n",
      "\n",
      "Amazon's business has evolved significantly over the years, with a focus on expanding its e-commerce platform, cloud computing services, and other digital offerings. Based on the analysis of multiple 10-K reports from Amazon, the main business segments driving growth at Amazon can be identified as follows:\n",
      "\n",
      "### 1. **North America Segment**\n",
      "\n",
      "*   The North America segment has consistently been a significant contributor to Amazon's revenue and growth.\n",
      "*   This segment focuses on consumer sales through online and physical stores, with an emphasis on selection, price, and convenience.\n",
      "*   The segment has seen steady growth over the years, driven by increasing demand for online shopping and digital services.\n",
      "\n",
      "### 2. **International Segment**\n",
      "\n",
      "*   The International segment has also played a crucial role in Amazon's growth, with a focus on consumer sales through online and physical stores outside of North America.\n",
      "*   This segment has faced intense competition and rapidly changing consumer behavior, but has still managed to contribute significantly to Amazon's revenue.\n",
      "\n",
      "### 3. **Amazon Web Services (AWS) Segment**\n",
      "\n",
      "*   The AWS segment has emerged as a major growth driver for Amazon, offering on-demand technology services for developers and enterprises of all sizes.\n",
      "*   AWS provides a broad set of global compute, storage, database, and other service offerings, and has become increasingly important to Amazon's overall performance.\n",
      "*   The growth of AWS has been driven by the increasing demand for cloud computing, artificial intelligence, and machine learning services.\n",
      "\n",
      "### Comparison Across Years\n",
      "\n",
      "A comparison of Amazon's business segments across multiple years reveals some key trends:\n",
      "\n",
      "*   **Early Years (2014-2016):** Amazon's business was primarily driven by its North America and International segments, with a focus on e-commerce and online shopping.\n",
      "*   **Mid-Years (2017-2020):** The AWS segment started to gain prominence, with a focus on cloud computing and digital services. The North America and International segments continued to grow, but at a slower pace.\n",
      "*   **Recent Years (2021-2023):** The AWS segment has become a significant contributor to Amazon's revenue and growth, with a focus on cloud computing, artificial intelligence, and machine learning services. The North America and International segments continue to grow, but with an increasing emphasis on online and physical stores, as well as digital services.\n",
      "\n",
      "### Significant Changes and Trends\n",
      "\n",
      "Some significant changes and trends in Amazon's business segments over the years include:\n",
      "\n",
      "*   **Increased Focus on AWS:** Amazon has increasingly focused on growing its AWS segment, which has become a major driver of revenue and growth.\n",
      "*   **Expansion of E-commerce Platform:** Amazon has continued to expand its e-commerce platform, both in terms of product offerings and geographic reach.\n",
      "*   **Growing Importance of Digital Services:** Digital services, including advertising, have become increasingly important to Amazon's business model.\n",
      "*   **Changing Consumer Behavior:** Amazon has had to adapt to changing consumer behavior, including the shift towards online shopping and digital services.\n",
      "\n",
      "Overall, Amazon's business segments have evolved significantly over the years, with a growing emphasis on AWS, e-commerce, and digital services. The company's ability to adapt to changing consumer behavior and technological advancements has been a key driver of its growth and success. \n",
      "\n",
      "### Revenue and Net Income Growth\n",
      "\n",
      "| Year  | Revenue (in billion USD) | Net Income (in billion USD) |\n",
      "| :----: | :-----------------------: | :-------------------------: |\n",
      "|  2014  |            89            |             0.236           |\n",
      "|  2015  |           107.006        |             0.596           |\n",
      "|  2016  |           135.987        |             2.371           |\n",
      "|  2017  |           177.866        |             3.033           |\n",
      "|  2018  |           232.884        |             10.07          |\n",
      "|  2019  |           280.522        |             18.75          |\n",
      "|  2020  |           386.064        |             18.386          |\n",
      "|  2021  |           478            |             33.4           |\n",
      "|  2022  |           513.983        |             18.339          |\n",
      "|  2023  |           554.028        |             20.058          |\n",
      "\n",
      "The revenue and net income of Amazon have shown a steady increase over the years, with some fluctuations. The growth in revenue and net income can be attributed to the company's expanding business segments, particularly AWS, and its ability to adapt to changing consumer behavior and technological advancements.\n",
      "\n",
      "Answering question: How has AWS evolved as a segment and what is its contribution to Amazon's overall business?\n",
      "\n",
      "Synthesizing information and answering question...\n",
      "Throttling detected. Retrying after 5.28 seconds...\n",
      "Time taken: 20.10 seconds\n",
      "\n",
      "==================================================\n",
      "ANSWER to: How has AWS evolved as a segment and what is its contribution to Amazon's overall business?\n",
      "==================================================\n",
      "## Evolution of AWS as a Segment and Its Contribution to Amazon's Overall Business\n",
      "\n",
      "Amazon Web Services (AWS) has emerged as a pivotal segment in Amazon's business, showcasing significant growth and contribution over the years. Here's a comprehensive analysis of AWS's evolution and its impact on Amazon's overall business, based on the provided 10-K reports across multiple years.\n",
      "\n",
      "### Early Years (2014-2015)\n",
      "\n",
      "- **2014**: AWS was not explicitly mentioned as a separate segment in the provided data. However, its importance was noted.\n",
      "- **2015**: AWS started gaining recognition as a crucial segment, offering cloud computing services. The revenue and net income figures for AWS were not separately provided in the given data.\n",
      "\n",
      "### Growth Phase (2016-2017)\n",
      "\n",
      "- **2016**: AWS became one of the three main segments, alongside North America and International. It offered on-demand technology services for developers and enterprises.\n",
      "- **2017**: AWS continued to grow, offering a broad set of global compute, storage, database, and other service offerings to developers and enterprises of all sizes. The revenue for 2017 was $177,866 million, with a net income of $3,033 million for the company as a whole.\n",
      "\n",
      "### Expansion and Maturity (2018-2020)\n",
      "\n",
      "- **2018**: AWS was a key contributor, with a focus on on-demand technology services for developers and enterprises. The exact financials for AWS weren't provided, but its importance in the company's portfolio was highlighted.\n",
      "- **2020**: AWS played a significant role, with a description that included on-demand technology services for developers and enterprises of all sizes, including compute, storage, database, analytics, and machine learning. The company's total revenue was $386,064,000,000, with a net income of $18,386,000,000.\n",
      "\n",
      "### Current Status (2021-2023)\n",
      "\n",
      "- **2021**: AWS was described as offering a broad set of on-demand technology services, including compute, storage, database, analytics, machine learning, and more. The company's total revenue was $478 billion, with a net income of $33.4 billion.\n",
      "- **2022**: AWS continued to be a vital segment, with a focus on cloud computing services. The total revenue for Amazon was $513,983,000,000 (example figure), and the net income was $18,339,000,000 (example figure).\n",
      "- **2023**: The report highlighted AWS's ongoing importance, though specific financials were not provided in the text.\n",
      "\n",
      "### Contribution to Amazon's Overall Business\n",
      "\n",
      "AWS has evolved from being a nascent cloud computing service to a major contributor to Amazon's revenue and profitability. While specific financial contributions of AWS were not detailed in the provided text for each year, its growth and significance are evident:\n",
      "\n",
      "- **Revenue Growth**: AWS has been instrumental in Amazon's top-line growth, contributing significantly to the company's revenue.\n",
      "- **Profitability**: AWS has been a key driver of Amazon's profitability, given the high margins of cloud computing services compared to e-commerce.\n",
      "- **Strategic Importance**: AWS has become a strategic asset for Amazon, enabling the company to diversify its business beyond e-commerce and establish a strong presence in the cloud computing market.\n",
      "\n",
      "In conclusion, AWS has transformed from a relatively minor segment to a crucial part of Amazon's business, driving growth, profitability, and diversification. Its evolution reflects Amazon's successful strategy of investing in cloud computing and technology services, positioning the company for long-term success in a rapidly changing business landscape.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing PDF files\n",
    "pdf_directory = \"/home/ec2-user/SageMaker/Llama4_Assets/\"  # Replace with your directory path\n",
    "\n",
    "# Initialize the Amazon Bedrock client\n",
    "print(\"Initializing Amazon Bedrock client...\")\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "# You can choose from available models in your Bedrock setup, like:\n",
    "# model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_id = \"us.meta.llama4-scout-17b-instruct-v1:0\"  # Replace with your preferred Bedrock model\n",
    "print(f\"Using model: {model_id}\")\n",
    "\n",
    "try:\n",
    "    # Find PDF files\n",
    "    pdf_files = find_pdf_files(pdf_directory)\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found. Exiting.\")\n",
    "        raise SystemExit(0)\n",
    "    \n",
    "    # Extract summaries from PDFs\n",
    "    print(\"\\nExtracting key data from all PDFs...\")\n",
    "    pdf_summaries = extract_key_data_from_pdfs(pdf_files, bedrock_client, model_id)\n",
    "    \n",
    "    if not pdf_summaries:\n",
    "        print(\"Failed to extract data from any PDFs. Exiting.\")\n",
    "        raise SystemExit(0)\n",
    "    \n",
    "    print(\"\\nSuccessfully extracted data from the following PDFs:\")\n",
    "    for pdf_name in pdf_summaries.keys():\n",
    "        print(f\"  - {pdf_name}\")\n",
    "    \n",
    "    # Define your questions\n",
    "    questions = [\n",
    "        \"How did Amazon's revenue and net income change over time?\",\n",
    "        \"What are the main business segments driving growth at Amazon over the years?\",\n",
    "        \"How has AWS evolved as a segment and what is its contribution to Amazon's overall business?\"\n",
    "    ]\n",
    "    \n",
    "    # Answer each question\n",
    "    for question in questions:\n",
    "        print(f\"\\nAnswering question: {question}\")\n",
    "        answer = synthesize_and_answer_question(pdf_summaries, question, bedrock_client, model_id)\n",
    "        \n",
    "        if answer:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"ANSWER to: {question}\")\n",
    "            print(\"=\"*50)\n",
    "            print(answer)\n",
    "        else:\n",
    "            print(f\"Sorry, I couldn't generate an answer for: {question}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ffaa1-10c6-40a7-b8d4-f330f1ffe61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
