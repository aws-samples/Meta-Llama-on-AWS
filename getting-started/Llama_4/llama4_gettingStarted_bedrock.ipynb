{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4011e2bc-41f6-446c-b83f-d01f671c6b5d",
   "metadata": {},
   "source": [
    "# Llama 4 on Bedrock: Long context window use cases\n",
    "\n",
    "This notebook demonstrates how to use Llama 4 on AWS by deploying on SageMaker JumpStart. We will cover the deployment process, basic invocations to run inference, and important use cases that can be used with Llama 4 Scout's 10 million token window.\n",
    "\n",
    "## Llama 4\n",
    "\n",
    "Llama 4 represents Meta’s most advanced multimodal models to date, featuring a mixture of experts (MoE) architecture and context window support up to 10 million tokens. With native multimodality and early fusion technology, Meta states that these new models demonstrate unprecedented performance across text and vision tasks while maintaining efficient compute requirements. With a dramatic increase on supported context length from 128K in Llama 3, Llama 4 is now suitable for multi-document summarization, parsing extensive user activity for personalized tasks, and reasoning over extensive codebases. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to Llama 4 on Bedrock enabled via AWS console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d9f36-82c4-4dd3-992a-52c7fdd4fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f838c1-b174-4b56-a494-a603701eda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a8858-6294-4669-ae57-c2b442f2cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_llama_4_scout = 'us.meta.llama4-scout-17b-instruct-v1:0'\n",
    "\n",
    "model_id = meta_llama_4_scout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f0547-c27c-4a89-b6e1-0947e62cbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your payload with the messages format\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"You are a helpful AI assistant.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"What are three key benefits of large language models for businesses?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa642d-bac0-4116-8a56-a4f2921a9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.converse(\n",
    "    modelId=model_id,\n",
    "    messages=payload[\"messages\"],  # Wrap the message in a list\n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 2000,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": .5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4bce8-d78b-4959-b290-086aa8201a60",
   "metadata": {},
   "source": [
    "# Codebase Analysis \n",
    "\n",
    "Using Llama 4 Scout’s industry-leading context window, this section showcases its ability to deeply analyze expansive codebases. The example extracts and contextualizes the buildspec-1-10-2.yml file from the AWS Deep Learning Containers GitHub repository, illustrating how the model synthesizes information across an entire repository. We used a tool to ingest the whole repository into plaintext that we provided to the model as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67009ef-27af-4049-ad76-328cfb634afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "def read_markdown_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of a markdown file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the markdown file to be read\n",
    "        \n",
    "    Returns:\n",
    "        str: The content of the file as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def summarize_text_bedrock(text, model_id, max_length=10000):\n",
    "    \"\"\"\n",
    "    Summarize the provided text using Amazon Bedrock's converse API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to summarize\n",
    "        model_id (str): Bedrock model ID to use\n",
    "        max_length (int): Maximum length of text to send (to avoid token limits)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Summarized text, Latency metrics)\n",
    "    \"\"\"\n",
    "    # Configure the Bedrock client with extended timeout\n",
    "    config = Config(read_timeout=300, connect_timeout=30)\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\", config=config)\n",
    "    \n",
    "    # Truncate text if necessary to avoid exceeding token limits\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length] + \"... [Content truncated due to length]\"\n",
    "    \n",
    "    # Start measuring preprocessing time\n",
    "    preprocess_start = time.time()\n",
    "    \n",
    "    # Format the messages correctly for Bedrock's converse API\n",
    "    # First, create two separate messages\n",
    "    system_instruction = \"You are a helpful AI assistant that summarizes codebases to me to help me understand how to analyze code by synthesizing through the entire codebase before responding. Be thorough in your search as the file may be nested within a markdown code block or within a directory listing.\"\n",
    "    \n",
    "    user_question = f\"Can you explain to me the buildspec-1-10-2.yml file and how it relates to the rest of the huggingface directory? Use this information:\\n\\n{text} as reference.\"\n",
    "    \n",
    "    # Format messages as per Llama 4 Bedrock requirements\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": system_instruction  # Include system instruction as first user message\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Measure payload size\n",
    "    payload_size = len(json.dumps({\"messages\": messages}).encode('utf-8'))\n",
    "    \n",
    "    preprocess_end = time.time()\n",
    "    preprocess_time = preprocess_end - preprocess_start\n",
    "    \n",
    "    # Invoke the Bedrock model and measure latency\n",
    "    api_start = time.time()\n",
    "    try:\n",
    "        # Use invoke_model first to test if converse is the issue\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 4000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Bedrock converse: {e}\")\n",
    "        # Try using invoke_model instead as a fallback\n",
    "        try:\n",
    "            # Format for invoke_model is different\n",
    "            payload = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": user_question}\n",
    "                ],\n",
    "                \"max_tokens\": 2000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            body = json.dumps(payload)\n",
    "            response = bedrock_client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=body\n",
    "            )\n",
    "            \n",
    "            # Parse the response from invoke_model\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            # Convert to a format similar to converse for consistent handling\n",
    "            response = {\n",
    "                \"output\": {\n",
    "                    \"content\": [\n",
    "                        {\"text\": response_body.get('content', '') or response_body.get('generation', '')}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback also failed: {e2}\")\n",
    "            raise\n",
    "    \n",
    "    api_end = time.time()\n",
    "    api_latency = api_end - api_start\n",
    "    \n",
    "    # Start measuring postprocessing time\n",
    "    postprocess_start = time.time()\n",
    "    \n",
    "    # Extract the summary from the response\n",
    "    try:\n",
    "        summary = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        summary = str(response)\n",
    "    \n",
    "    # Calculate output size\n",
    "    output_size = len(summary.encode('utf-8'))\n",
    "    \n",
    "    postprocess_end = time.time()\n",
    "    postprocess_time = postprocess_end - postprocess_start\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        'api_latency': api_latency,\n",
    "        'preprocess_time': preprocess_time,\n",
    "        'postprocess_time': postprocess_time,\n",
    "        'total_latency': preprocess_time + api_latency + postprocess_time,\n",
    "        'payload_size_bytes': payload_size,\n",
    "        'output_size_bytes': output_size,\n",
    "        'payload_tokens': len(text) / 4,  # rough approximation of tokens\n",
    "    }\n",
    "    \n",
    "    return summary, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9950b1-41a9-4a17-874e-26f8c6e1a695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "file_path = '/home/ec2-user/SageMaker/repomix-output.md'  # Update with actual file location\n",
    "model_id = 'us.meta.llama4-maverick-17b-instruct-v1:0'  # Use your desired Bedrock model ID\n",
    "\n",
    "# Read the markdown file\n",
    "print(f\"Reading markdown file from {file_path}...\")\n",
    "read_start = time.time()\n",
    "markdown_content = read_markdown_file(file_path)\n",
    "read_end = time.time()\n",
    "read_time = read_end - read_start\n",
    "print(f\"File read time: {read_time:.4f} seconds\")\n",
    "\n",
    "# Summarize the content with Bedrock\n",
    "print(\"Generating summary using Bedrock...\")\n",
    "summary, latency_metrics = summarize_text_bedrock(markdown_content, model_id)\n",
    "\n",
    "# Print the latency metrics\n",
    "print(\"\\n--- LATENCY METRICS ---\")\n",
    "print(f\"API call latency: {latency_metrics['api_latency']:.4f} seconds\")\n",
    "print(f\"Preprocessing time: {latency_metrics['preprocess_time']:.4f} seconds\")\n",
    "print(f\"Postprocessing time: {latency_metrics['postprocess_time']:.4f} seconds\")\n",
    "print(f\"Total latency: {latency_metrics['total_latency']:.4f} seconds\")\n",
    "print(f\"Payload size: {latency_metrics['payload_size_bytes'] / 1024:.2f} KB\")\n",
    "print(f\"Output size: {latency_metrics['output_size_bytes'] / 1024:.2f} KB\")\n",
    "print(f\"Approximate input tokens: {int(latency_metrics['payload_tokens'])}\")\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\n--- SUMMARY ---\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a599f-2b97-44d0-86a9-60af764d0c52",
   "metadata": {},
   "source": [
    "# Multi-Doc Processing\n",
    "\n",
    "With Llama 4 Scout's 3.5million token context window on Bedrock - Llama 4 Scout excels in multi-document processing. In this example, the model extracts key financial metrics from Amazon 10-K reports (2017-2024), demonstrating its capability to integrate and analyze data spanning multiple years—all without the need for additional processing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d3731-2a15-455b-84de-30af3bfe6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db56e5-1994-4c06-91d8-7860c3519e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import PyPDF2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from botocore.config import Config\n",
    "\n",
    "# Define helper functions\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"PDF has {len(pdf_reader.pages)} pages\")\n",
    "            # Extract text from first 100 pages (adjust as needed)\n",
    "            for page_num in range(min(100, len(pdf_reader.pages))):\n",
    "                print(f\"Processing page {page_num+1}/{len(pdf_reader.pages)}...\")\n",
    "                text += pdf_reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def retry_with_exponential_backoff(func, max_retries=5, initial_backoff=5):\n",
    "    \"\"\"Execute a function with exponential backoff retry.\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'response') and 'Error' in e.response and e.response['Error'].get('Code') == 'ThrottlingException':\n",
    "                wait_time = initial_backoff * (2 ** retries) + random.uniform(0, 1)\n",
    "                print(f\"Throttling detected. Retrying after {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "                if retries > max_retries:\n",
    "                    print(f\"Maximum retries ({max_retries}) exceeded.\")\n",
    "                    raise\n",
    "            else:\n",
    "                print(f\"Non-throttling error: {e}\")\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "def extract_key_data_from_pdfs(pdf_files, bedrock_client, model_id):\n",
    "    \"\"\"Extract and summarize key data from each PDF file using Amazon Bedrock.\"\"\"\n",
    "    pdf_summaries = {}\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = os.path.basename(pdf_file)\n",
    "        print(f\"\\n--- Extracting key data from {file_name} ---\")\n",
    "        \n",
    "        pdf_text = extract_text_from_pdf(pdf_file)\n",
    "        if not pdf_text:\n",
    "            print(f\"Failed to extract text from {file_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Use a smaller chunk size to avoid token limits\n",
    "        max_text_length = 10000  # Reduced from 25000 to avoid throttling\n",
    "        truncated_text = pdf_text[:max_text_length]\n",
    "        \n",
    "        # Create system instruction and user question\n",
    "        system_instruction = \"You are a financial analyst. Extract key financial data from this 10-K report.\"\n",
    "        \n",
    "        user_question = f\"\"\"Extract the following information from this Amazon 10-K report:\n",
    "1. What year is this report for?\n",
    "2. Total revenue for the year\n",
    "3. Net income\n",
    "4. Key business segments and their performance\n",
    "5. Important trends mentioned\n",
    "\n",
    "Here's the 10-K text (truncated):\n",
    "{truncated_text}\n",
    "\n",
    "Format your response as a JSON object with these fields: year, revenue, net_income, segments, trends.\n",
    "\"\"\"\n",
    "        \n",
    "        # Format messages according to the example\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": system_instruction\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": user_question\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            print(f\"Extracting key data from {file_name}...\")\n",
    "            \n",
    "            # Call Bedrock's Converse API with retry logic\n",
    "            def make_api_call():\n",
    "                return bedrock_client.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=messages,\n",
    "                    inferenceConfig={\n",
    "                        \"maxTokens\": 1500,  # Reduced from 2048\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"topP\": 0.9\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "            # Add a delay before making the API call to avoid throttling\n",
    "            time.sleep(2)\n",
    "            response = retry_with_exponential_backoff(make_api_call)\n",
    "            \n",
    "            # Extract content from response based on the format\n",
    "            try:\n",
    "                summary = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "                pdf_summaries[file_name] = summary\n",
    "                print(f\"Successfully extracted key data from {file_name}\")\n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "                print(f\"Response structure: {json.dumps(response, default=str)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return pdf_summaries\n",
    "\n",
    "def find_pdf_files(directory):\n",
    "    \"\"\"Find all PDF files in the specified directory.\"\"\"\n",
    "    pdf_files = []\n",
    "    try:\n",
    "        # Look for all PDF files in the directory\n",
    "        pdf_pattern = os.path.join(directory, \"*.pdf\")\n",
    "        pdf_files = glob.glob(pdf_pattern)\n",
    "        \n",
    "        if pdf_files:\n",
    "            print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "            for pdf in pdf_files:\n",
    "                print(f\"  - {os.path.basename(pdf)}\")\n",
    "        else:\n",
    "            print(f\"No PDF files found in {directory}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding PDF files: {e}\")\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "def synthesize_and_answer_question(pdf_summaries, question, bedrock_client, model_id):\n",
    "    \"\"\"Synthesize information from all PDFs and answer a specific question.\"\"\"\n",
    "    # Combine all summaries - limiting context size to avoid throttling\n",
    "    combined_context = \"\"\n",
    "    for pdf_name, summary in pdf_summaries.items():\n",
    "        combined_context += f\"--- Data from {pdf_name} ---\\n{summary[:2000]}\\n\\n\"  # Limit each summary\n",
    "    \n",
    "    # Create system instruction and user question\n",
    "    system_instruction = \"\"\"You are an expert financial analyst comparing Amazon's 10-K reports across multiple years.\n",
    "Your task is to synthesize information from these reports and answer questions accurately.\n",
    "Make sure to compare data across years when relevant and highlight significant changes or trends.\"\"\"\n",
    "    \n",
    "    user_question = f\"\"\"I've analyzed multiple Amazon 10-K reports and extracted the key information from each:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Based on this information from all the reports, please answer the following question:\n",
    "{question}\n",
    "\n",
    "Provide a comprehensive answer that compares information across all available years.\"\"\"\n",
    "\n",
    "    # Format messages according to your example\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": system_instruction\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nSynthesizing information and answering question...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call Bedrock's Converse API with retry logic\n",
    "        def make_api_call():\n",
    "            return bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": 2000,  # Reduced from 4096\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Add a delay before making the API call to avoid throttling\n",
    "        time.sleep(2)    \n",
    "        response = retry_with_exponential_backoff(make_api_call)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Extract the response content\n",
    "        try:\n",
    "            answer = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            return answer\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            try:\n",
    "                print(f\"Response structure: {json.dumps(response, default=str)}\")\n",
    "            except:\n",
    "                print(f\"Cannot display response structure\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error synthesizing information: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d628c8-1241-4738-bf0b-2dfbe50189b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the directory containing PDF files\n",
    "pdf_directory = \"/home/ec2-user/SageMaker/Llama4_Assets/\"  # Replace with your directory path\n",
    "\n",
    "# Initialize the Amazon Bedrock client\n",
    "print(\"Initializing Amazon Bedrock client...\")\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "# You can choose from available models in your Bedrock setup, like:\n",
    "# model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_id = \"us.meta.llama4-scout-17b-instruct-v1:0\"  # Replace with your preferred Bedrock model\n",
    "print(f\"Using model: {model_id}\")\n",
    "\n",
    "try:\n",
    "    # Find PDF files\n",
    "    pdf_files = find_pdf_files(pdf_directory)\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found. Exiting.\")\n",
    "        raise SystemExit(0)\n",
    "    \n",
    "    # Extract summaries from PDFs\n",
    "    print(\"\\nExtracting key data from all PDFs...\")\n",
    "    pdf_summaries = extract_key_data_from_pdfs(pdf_files, bedrock_client, model_id)\n",
    "    \n",
    "    if not pdf_summaries:\n",
    "        print(\"Failed to extract data from any PDFs. Exiting.\")\n",
    "        raise SystemExit(0)\n",
    "    \n",
    "    print(\"\\nSuccessfully extracted data from the following PDFs:\")\n",
    "    for pdf_name in pdf_summaries.keys():\n",
    "        print(f\"  - {pdf_name}\")\n",
    "    \n",
    "    # Define your questions\n",
    "    questions = [\n",
    "        \"How did Amazon's revenue and net income change over time?\",\n",
    "        \"What are the main business segments driving growth at Amazon over the years?\",\n",
    "        \"How has AWS evolved as a segment and what is its contribution to Amazon's overall business?\"\n",
    "    ]\n",
    "    \n",
    "    # Answer each question\n",
    "    for question in questions:\n",
    "        print(f\"\\nAnswering question: {question}\")\n",
    "        answer = synthesize_and_answer_question(pdf_summaries, question, bedrock_client, model_id)\n",
    "        \n",
    "        if answer:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"ANSWER to: {question}\")\n",
    "            print(\"=\"*50)\n",
    "            print(answer)\n",
    "        else:\n",
    "            print(f\"Sorry, I couldn't generate an answer for: {question}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
