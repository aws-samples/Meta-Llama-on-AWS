# script parameters
# Hugging Face model id
model_id: "meta-llama/Llama-3.3-70B-Instruct" 
# path to dataset
dataset_path: "." 
#max sequence length for model and packing of the dataset
max_seq_length: 2048 

## training parameters
# Temporary output directory for model checkpoints
output_dir: "./llama-3-70b-dolly" 
# report metrics to tensorboard
report_to: "tensorboard" 
# learning rate 2e-4
learning_rate: 0.0002 
# learning rate scheduler
lr_scheduler_type: "constant" 
# number of training epochs
num_train_epochs: 3 
# batch size per device during training
per_device_train_batch_size: 4 #We used 4 and 8, Please refer to the below table.
# batch size for evaluation
per_device_eval_batch_size: 4 #We used 4 and 8, Please refer to the below table.
# number of steps before performing a backward/update pass
gradient_accumulation_steps: 2 
# use torch adamw optimizer
optim: adamw_torch 
# log every 10 steps
logging_steps: 10 
# save checkpoint every epoch
save_strategy: epoch 
# evaluate every epoch
evaluation_strategy: epoch 
# max gradient norm
max_grad_norm: 0.3 
# warmup ratio
warmup_ratio: 0.03 
#use bfloat16 precision
bf16: true 
# use tf32 precision
tf32: true 
# use gradient checkpointing to save memory
gradient_checkpointing: true 

# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard auto_wrap offload" 
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"
  cpu_ram_efficient_loading: "true"
