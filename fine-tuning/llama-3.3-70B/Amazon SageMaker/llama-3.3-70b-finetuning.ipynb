{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb67fda8-2a04-4632-83a2-c60f62390888",
   "metadata": {},
   "source": [
    "# Fine Tuning Llama-3.3 70b model with HuggingFace Estimator on ml.g6e.48xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981d0f9-4db1-4ca9-9a2b-4b134fd80892",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc294ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[48 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Looking in indexes: https://pypi.org/simple, https://plugin.us-east-1.prod.workshops.aws, https://plugin.us-east-1.prod.workshops.aws\n",
      "  \u001b[31m   \u001b[0m Ignoring oldest-supported-numpy: markers 'python_version < \"3.9\"' don't match your environment\n",
      "  \u001b[31m   \u001b[0m Collecting cython>=3\n",
      "  \u001b[31m   \u001b[0m   Using cached cython-3.1.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting numpy>=1.25\n",
      "  \u001b[31m   \u001b[0m   Using cached numpy-2.3.2.tar.gz (20.5 MB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Installing backend dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing backend dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m╰─>\u001b[0m \u001b[31m[19 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ /local/home/rivasge/projects/meta_champions_main_work_dir/Meta-Llama-on-AWS/.conda/bin/python3.11 /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4/vendored-meson/meson/meson.py setup /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4 /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4/.mesonpy-mxottgcr -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4/.mesonpy-mxottgcr/meson-python-native-file.ini\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m The Meson build system\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Version: 1.6.1\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Source dir: /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Build dir: /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4/.mesonpy-mxottgcr\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Build type: native build\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Project name: NumPy\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Project version: 2.3.2\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C compiler for the host machine: cc (gcc 7.3.1 \"cc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\")\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C linker for the host machine: cc ld.bfd 2.29.1-31\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C++ compiler for the host machine: c++ (gcc 7.3.1 \"c++ (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\")\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C++ linker for the host machine: c++ ld.bfd 2.29.1-31\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 3.1.3)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Host machine cpu family: x86_64\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Host machine cpu: x86_64\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m ../meson.build:28:4: ERROR: Problem encountered: NumPy requires GCC >= 9.3\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m A full log can be found at /tmp/pip-install-6fw4n0ih/numpy_e0e4a959ff6b4c57aee9de590c10edd4/.mesonpy-mxottgcr/meson-logs/meson-log.txt\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c804b-4284-4a34-8d76-b72857a72e8b",
   "metadata": {},
   "source": [
    "### Login to huggingface using your tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc88b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `sagemaker_test` has been saved to /home/rivasge/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/rivasge/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `sagemaker_test`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"<YOUR_HF_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524a2f0b-4905-4287-9259-8d581239db30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/rivasge/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name dev_desktop to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::786045444066:role/service-role/AmazonSageMaker-ExecutionRole-20220929T161862\n",
      "sagemaker bucket: sagemaker-us-west-2-786045444066\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "source_dir = \"./fsdp_v2\"\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20220929T161862')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be136d0-5fec-405c-b0be-ac151d0c52c2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f5e2b-b84b-4c50-9c86-414290f98e68",
   "metadata": {},
   "source": [
    "### Download databricks-dolly-15k dataset from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5489157e-9821-4aee-b2aa-dae5924ccced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-03 18:54:13--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 3.175.34.8, 3.175.34.46, 3.175.34.95, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.175.34.8|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/64358e2179c45fcf1ada09f4/63c4dabe683d7254493568d2d3995c0e51abc8528ef3b4936497c538cb501e93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250903%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250903T185413Z&X-Amz-Expires=3600&X-Amz-Signature=13e64c5cec3bb22760dc0a043d614ccc85bb659990a1c963661e9ced7f243630&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&x-id=GetObject&Expires=1756929253&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NjkyOTI1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDM1OGUyMTc5YzQ1ZmNmMWFkYTA5ZjQvNjNjNGRhYmU2ODNkNzI1NDQ5MzU2OGQyZDM5OTVjMGU1MWFiYzg1MjhlZjNiNDkzNjQ5N2M1MzhjYjUwMWU5MyoifV19&Signature=DciE61t-ioQINETA-iDOqBNeYiFU7ySwX9GmUPLl%7E1ZoAfDPFC1tY2k%7EoVjqnHHkNbVJlddGxGbG%7EqRnEub7V6QyFOi4nHjR-gkuNMkby9nJtZuT4qtDBd1fCLiGi%7Eh2M1sRKN2eeWnn0m5F6HrQBHspCj7Cc3ySgVVKUlWzAr%7El71R0yBwP2QemBHbb12SeEj2H9oaZVK1bCCniB7c-MIGkOwNHpPqO9zhbW5H7QWCUmIEtkwmwC6Vye3bd-Nv0KZlaJifSfgxQYLC00RWepvLcQVVZkzJPmk-K9%7E-egLU5Gt1JqGHH5WAMZe1unqo5e5vpuDQ-AtXgCBTvGUtq2Q__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-09-03 18:54:13--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64358e2179c45fcf1ada09f4/63c4dabe683d7254493568d2d3995c0e51abc8528ef3b4936497c538cb501e93?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250903%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250903T185413Z&X-Amz-Expires=3600&X-Amz-Signature=13e64c5cec3bb22760dc0a043d614ccc85bb659990a1c963661e9ced7f243630&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&x-id=GetObject&Expires=1756929253&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NjkyOTI1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDM1OGUyMTc5YzQ1ZmNmMWFkYTA5ZjQvNjNjNGRhYmU2ODNkNzI1NDQ5MzU2OGQyZDM5OTVjMGU1MWFiYzg1MjhlZjNiNDkzNjQ5N2M1MzhjYjUwMWU5MyoifV19&Signature=DciE61t-ioQINETA-iDOqBNeYiFU7ySwX9GmUPLl%7E1ZoAfDPFC1tY2k%7EoVjqnHHkNbVJlddGxGbG%7EqRnEub7V6QyFOi4nHjR-gkuNMkby9nJtZuT4qtDBd1fCLiGi%7Eh2M1sRKN2eeWnn0m5F6HrQBHspCj7Cc3ySgVVKUlWzAr%7El71R0yBwP2QemBHbb12SeEj2H9oaZVK1bCCniB7c-MIGkOwNHpPqO9zhbW5H7QWCUmIEtkwmwC6Vye3bd-Nv0KZlaJifSfgxQYLC00RWepvLcQVVZkzJPmk-K9%7E-egLU5Gt1JqGHH5WAMZe1unqo5e5vpuDQ-AtXgCBTvGUtq2Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.163.24.58, 3.163.24.122, 3.163.24.64, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.163.24.58|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13085339 (12M)\n",
      "Saving to: ‘databricks-dolly-15k.jsonl.20’\n",
      "\n",
      "100%[======================================>] 13,085,339  --.-K/s   in 0.09s   \n",
      "\n",
      "2025-09-03 18:54:14 (144 MB/s) - ‘databricks-dolly-15k.jsonl.20’ saved [13085339/13085339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a11c0-53e2-4e36-b75d-bc3f19740685",
   "metadata": {},
   "source": [
    "### Format and split train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bfe091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def generate_prompt(row):\n",
    "    prompt = f\"Instruction: {row['instruction']}\\nContext: {row['context']}\\nResponse: {row['response']}\"\n",
    "    return prompt\n",
    "\n",
    "data = []\n",
    "with open('databricks-dolly-15k.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['text'] = df.apply(generate_prompt, axis=1)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2,random_state = 42)\n",
    "\n",
    "\n",
    "train.to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "test.to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5e1ca-4c40-4481-9b2b-8ec5360d65ad",
   "metadata": {},
   "source": [
    "### Upload the train/test dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537471a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-west-2-786045444066/datasets/llama3/train_v3/train_dataset.json\n",
      "s3://sagemaker-us-west-2-786045444066/datasets/llama3/test_v3/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sagemaker_session_bucket}/datasets/llama3'\n",
    " \n",
    "from sagemaker.s3 import S3Uploader\n",
    "train_dataset_s3_path = S3Uploader.upload(local_path=\"./train_dataset.json\", desired_s3_uri=f\"{input_path}/train_v3\")\n",
    "test_dataset_s3_path = S3Uploader.upload(local_path=\"./test_dataset.json\", desired_s3_uri=f\"{input_path}/test_v3\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c394d2-788c-4442-9507-5c308bf4c620",
   "metadata": {},
   "source": [
    "### Upload config.yaml from source_dir to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c6e6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config uploaded to:\n",
      "s3://sagemaker-us-west-2-786045444066/datasets/llama3/config_v2/llama_3_70b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    " \n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"{}/llama_3_70b_fsdp_qlora.yaml\".format(source_dir)\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config_v2\")\n",
    " \n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349af4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/rivasge/projects/meta_champions_main_work_dir/Meta-Llama-on-AWS/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "train_dlc_image = \"763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\".format(sess.boto_region_name)\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-3-70b-exp1'\n",
    " \n",
    "# create the Estimator\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point          = 'run_fsdp_qlora.py',      # train script\n",
    "    source_dir           = source_dir,  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g6e.48xlarge',  # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    py_version           = 'py312',           # the python version used in the training job\n",
    "    image_uri            = train_dlc_image,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    keep_alive_period_in_seconds=1800, #warm pool\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb685b60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: llama3-3-70b-exp1-2025-09-03-18-54-15-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-03 18:54:17 Starting - Starting the training job\n",
      "2025-09-03 18:54:17 Pending - Training job waiting for capacity......\n",
      "2025-09-03 18:55:09 Pending - Preparing the instances for training.........\n",
      "2025-09-03 18:56:44 Downloading - Downloading input data...\n",
      "2025-09-03 18:56:54 Downloading - Downloading the training image.....................\n",
      "2025-09-03 19:00:51 Training - Training image download completed. Training in progress........bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "Skipping CUDA compat setup as package not found\n",
      "2025-09-03 19:01:57,345 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2025-09-03 19:01:57,425 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-03 19:01:57,445 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2025-09-03 19:01:57,447 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2025-09-03 19:01:57,447 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2025-09-03 19:01:58,862 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\n",
      "Collecting transformers>=4.53.0 (from -r requirements.txt (line 1))\n",
      "Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets>=2.18.0 (from -r requirements.txt (line 2))\n",
      "Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.10.1)\n",
      "Collecting evaluate>=0.4.1 (from -r requirements.txt (line 4))\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting bitsandbytes>=0.44.0 (from -r requirements.txt (line 5))\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting trl>=0.11.0 (from -r requirements.txt (line 6))\n",
      "Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft>=0.13.0 (from -r requirements.txt (line 7))\n",
      "Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.34.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.7.1)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2.7.1+cu128)\n",
      "Requirement already satisfied: flash-attn>=2.6.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (2.7.4.post1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.53.0->-r requirements.txt (line 1))\n",
      "Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.53.0->-r requirements.txt (line 1))\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/site-packages (from transformers>=4.53.0->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->-r requirements.txt (line 8)) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->-r requirements.txt (line 8)) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->-r requirements.txt (line 8)) (1.1.9)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 2)) (21.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets>=2.18.0->-r requirements.txt (line 2)) (2.3.2)\n",
      "Collecting xxhash (from datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.25.0->-r requirements.txt (line 8))\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from accelerate>=0.34.0->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.12/site-packages (from torch>=2.4.0->-r requirements.txt (line 10)) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from flash-attn>=2.6.0->-r requirements.txt (line 11)) (0.8.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2)) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers>=4.53.0->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers>=4.53.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers>=4.53.0->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.4.0->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->-r requirements.txt (line 10)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.18.0->-r requirements.txt (line 2)) (1.17.0)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 139.9 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 136.1 MB/s  0:00:00\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.3/61.3 MB 166.1 MB/s  0:00:00\n",
      "Downloading trl-0.22.2-py3-none-any.whl (544 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 544.8/544.8 kB 23.4 MB/s  0:00:00\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 110.3 MB/s  0:00:00\n",
      "Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 802.0/802.0 kB 76.6 MB/s  0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, regex, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, tokenizers, aiohttp, transformers, bitsandbytes, peft, datasets, trl, evaluate\n",
      "Attempting uninstall: fsspec\n",
      "Found existing installation: fsspec 2025.9.0\n",
      "Uninstalling fsspec-2025.9.0:\n",
      "Successfully uninstalled fsspec-2025.9.0\n",
      "Attempting uninstall: dill\n",
      "Found existing installation: dill 0.4.0\n",
      "Uninstalling dill-0.4.0:\n",
      "Successfully uninstalled dill-0.4.0\n",
      "Attempting uninstall: multiprocess\n",
      "Found existing installation: multiprocess 0.70.18\n",
      "Uninstalling multiprocess-0.70.18:\n",
      "Successfully uninstalled multiprocess-0.70.18\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 bitsandbytes-0.47.0 datasets-4.0.0 dill-0.3.8 evaluate-0.4.5 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 peft-0.17.1 propcache-0.3.2 regex-2025.9.1 tokenizers-0.22.0 transformers-4.56.0 trl-0.22.2 xxhash-3.5.0 yarl-1.20.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2025-09-03 19:02:10,571 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-09-03 19:02:10,571 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-09-03 19:02:10,704 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-03 19:02:10,817 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-03 19:02:10,827 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2025-09-03 19:02:10,930 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-03 19:02:10,939 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g6e.48xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"config\": \"/opt/ml/input/data/config\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g6e.48xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g6e.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"llama3-3-70b-exp1-2025-09-03-18-54-15-900\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-786045444066/llama3-3-70b-exp1-2025-09-03-18-54-15-900/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_fsdp_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g6e.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g6e.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"topology\": null,\n",
      "    \"user_entry_point\": \"run_fsdp_qlora.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"config\":\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"}\n",
      "SM_USER_ENTRY_POINT=run_fsdp_qlora.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g6e.48xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g6e.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g6e.48xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"config\",\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g6e.48xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g6e.48xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_fsdp_qlora\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=192\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-west-2-786045444066/llama3-3-70b-exp1-2025-09-03-18-54-15-900/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g6e.48xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g6e.48xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g6e.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"llama3-3-70b-exp1-2025-09-03-18-54-15-900\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-786045444066/llama3-3-70b-exp1-2025-09-03-18-54-15-900/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g6e.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g6e.48xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"topology\":null,\"user_entry_point\":\"run_fsdp_qlora.py\"}\n",
      "SM_USER_ARGS=[\"--config\",\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_CONFIG=/opt/ml/input/data/config\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_CONFIG=/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python312.zip:/usr/local/lib/python3.12:/usr/local/lib/python3.12/lib-dynload:/usr/local/lib/python3.12/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 8 run_fsdp_qlora.py --config /opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\n",
      "W0903 19:02:12.203000 197 site-packages/torch/distributed/run.py:766] \n",
      "W0903 19:02:12.203000 197 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0903 19:02:12.203000 197 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0903 19:02:12.203000 197 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 12008 examples [00:00, 23133.82 examples/s]\n",
      "Generating train split: 12008 examples [00:00, 23063.89 examples/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 3003 examples [00:00, 35668.05 examples/s]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank4]:[W903 19:02:21.518991789 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[rank5]:[W903 19:02:21.519485968 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank1]:[W903 19:02:21.526755286 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank2]:[W903 19:02:21.527668363 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank6]:[W903 19:02:21.537658243 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank7]:[W903 19:02:21.549247842 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank3]:[W903 19:02:22.566889426 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[rank0]:[W903 19:02:22.665080487 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "NCCL version 2.26.2+cuda12.2\n",
      "Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.\n",
      "Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.\n",
      "Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.\n",
      "Instruction: How would you define electricity based on the following paragraph?\n",
      "Context: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Electricity is related to magnetism, both being part of the phenomenon of electromagnetism, as described by Maxwell's equations. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\n",
      "The presence of either a positive or negative electric charge produces an electric field. The movement of electric charges is an electric current and produces a magnetic field. In most applications, a force acts on a charge with a magnitude given by Coulomb's law. Electric potential is typically measured in volts.\n",
      "Electricity is at the heart of many modern technologies, being used for:\n",
      "Electric power where electric current is used to energise equipment;\n",
      "Electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\n",
      "Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the 17th and 18th centuries. The theory of electromagnetism was developed in the 19th century, and by the end of that century electricity was being put to industrial and residential use by electrical engineers. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\n",
      "Response: Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "Instruction: Tell me which of the following activities are types of workouts: walking, running, weightlifting, HIIT, cardio, yoga, sleeping, sitting at your chair, standing, looking into the distance, watching TV.\n",
      "Context: \n",
      "Response: Working out typically involves movement of the body and muscles. it often involves the stretching or contracting of muscles and can have a change in heart rate. Walking, running, weightlifting, HIIT, and cardio, and yoga are different types of workouts.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:34,  9.46s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:35,  9.51s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:35,  9.49s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:35,  9.50s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:36,  9.52s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:36,  9.52s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:35,  9.51s/it]\n",
      "Fetching 30 files:   3%|▎         | 1/30 [00:09<04:36,  9.52s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:06, 36.67s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:06, 36.67s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:06, 36.68s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:06, 36.68s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:07, 36.69s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:07, 36.70s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:07, 36.69s/it]\n",
      "Fetching 30 files:   7%|▋         | 2/30 [01:05<17:07, 36.69s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files:  93%|█████████▎| 28/30 [01:06<00:03,  1.64s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]#015Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.26s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.27s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.27s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  1.57s/it]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [01:07<00:00,  2.27s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:13,  2.53s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:23,  2.88s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:24,  2.90s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:26,  2.97s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:26,  3.00s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:02<01:26,  2.98s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:29,  3.07s/it]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:04<02:07,  4.41s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:06<01:36,  3.44s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:43,  3.71s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:45,  3.75s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:47,  3.83s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:47,  3.83s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:47,  3.83s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:50,  3.94s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:09<02:08,  4.60s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:11<01:44,  3.88s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:11<01:51,  4.12s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:11<01:52,  4.17s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:55,  4.28s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:55,  4.27s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:56,  4.30s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:12<01:57,  4.37s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:14<02:11,  4.88s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:15<01:46,  4.08s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:51,  4.31s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:52,  4.34s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:55,  4.45s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:55,  4.44s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:17<01:57,  4.50s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:17<01:58,  4.54s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:19<01:41,  4.07s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:19<02:09,  4.97s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:20<01:47,  4.31s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:20<01:49,  4.36s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:21<01:51,  4.45s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:21<01:51,  4.44s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:21<01:52,  4.48s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:21<01:53,  4.53s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:23<01:37,  4.08s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:24<02:02,  4.90s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:25<01:43,  4.32s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:25<01:44,  4.35s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:25<01:45,  4.40s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:25<01:47,  4.47s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:26<01:47,  4.49s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:26<01:48,  4.54s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:27<01:33,  4.06s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:29<01:56,  4.85s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:29<01:38,  4.29s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:29<01:39,  4.34s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:29<01:39,  4.32s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:30<01:43,  4.50s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:30<01:43,  4.48s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:30<01:43,  4.51s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:31<01:31,  4.16s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:33<01:36,  4.38s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:33<01:51,  4.83s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:34<01:37,  4.42s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:34<01:36,  4.39s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:35<01:41,  4.60s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:35<01:40,  4.59s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:35<01:40,  4.58s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:36<01:28,  4.23s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:38<01:33,  4.45s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:38<01:34,  4.48s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:38<01:48,  4.91s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:38<01:33,  4.44s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:40<01:38,  4.67s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:40<01:37,  4.65s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:40<01:23,  4.18s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:40<01:37,  4.65s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:42<01:27,  4.40s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:43<01:28,  4.40s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:43<01:27,  4.39s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:43<01:43,  4.95s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:44<01:18,  4.15s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:44<01:32,  4.60s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:44<01:32,  4.64s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:44<01:31,  4.59s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:46<01:22,  4.34s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:47<01:23,  4.37s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:47<01:22,  4.35s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:48<01:14,  4.12s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:48<01:37,  4.88s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:49<01:26,  4.57s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:49<01:27,  4.63s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:49<01:27,  4.59s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:51<01:17,  4.33s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:51<01:18,  4.35s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:51<01:17,  4.33s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:52<01:11,  4.20s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:53<01:32,  4.85s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:53<01:21,  4.54s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:53<01:23,  4.63s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:53<01:22,  4.57s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:55<01:14,  4.41s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:56<01:15,  4.42s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:56<01:15,  4.44s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [00:57<01:07,  4.25s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:58<01:27,  4.84s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:58<01:18,  4.61s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:58<01:18,  4.62s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:58<01:20,  4.72s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:00<01:11,  4.47s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:00<01:11,  4.46s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:00<01:11,  4.46s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:01<01:03,  4.21s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:03<01:14,  4.68s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [01:03<01:23,  4.93s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:03<01:14,  4.67s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:03<01:16,  4.80s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:04<01:06,  4.41s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:05<01:06,  4.41s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:05<01:06,  4.42s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:05<00:58,  4.18s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:07<01:09,  4.65s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:07<01:09,  4.61s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:08<01:11,  4.74s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [01:08<01:19,  4.98s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:09<01:01,  4.38s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:09<01:01,  4.38s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:09<00:54,  4.16s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:09<01:01,  4.39s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:12<01:04,  4.62s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:12<01:03,  4.56s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:12<01:05,  4.68s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [01:13<01:13,  4.93s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:13<00:56,  4.37s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:13<00:56,  4.34s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:13<00:56,  4.37s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:14<00:50,  4.23s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:16<00:59,  4.58s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:16<00:58,  4.54s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:17<01:00,  4.66s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:17<00:53,  4.44s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:18<01:08,  4.89s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:18<00:52,  4.42s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:18<00:46,  4.26s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:18<00:53,  4.42s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:21<00:55,  4.65s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:21<00:55,  4.61s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:22<00:42,  4.24s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:22<00:57,  4.75s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:22<00:49,  4.50s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:22<00:48,  4.45s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:22<01:03,  4.86s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:23<00:49,  4.47s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:26<00:51,  4.67s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:26<00:51,  4.65s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:26<00:37,  4.21s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:27<00:44,  4.43s/it]#015Loading checkpoint shards:  67%|██████▋   | 20/30 [01:27<00:44,  4.49s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:27<00:52,  4.80s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:27<00:44,  4.43s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:28<00:59,  4.96s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:30<00:33,  4.17s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:30<00:45,  4.60s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:30<00:45,  4.58s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:31<00:39,  4.42s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:31<00:40,  4.47s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:31<00:39,  4.40s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:31<00:47,  4.72s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:33<00:54,  5.00s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:35<00:29,  4.23s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:35<00:41,  4.58s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:35<00:41,  4.56s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:35<00:35,  4.41s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:35<00:35,  4.45s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:36<00:35,  4.38s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:36<00:42,  4.70s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:37<00:49,  4.95s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:39<00:25,  4.28s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:39<00:36,  4.53s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:39<00:36,  4.57s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:40<00:31,  4.50s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:40<00:31,  4.53s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:40<00:31,  4.47s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:41<00:37,  4.67s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:42<00:44,  4.91s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:43<00:21,  4.24s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:44<00:32,  4.60s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:44<00:32,  4.64s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:45<00:27,  4.61s/it]#015Loading checkpoint shards:  80%|████████  | 24/30 [01:45<00:27,  4.60s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:45<00:27,  4.55s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:46<00:33,  4.73s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:47<00:39,  4.90s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:47<00:16,  4.22s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:49<00:27,  4.65s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:49<00:28,  4.70s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:49<00:22,  4.54s/it]#015Loading checkpoint shards:  83%|████████▎ | 25/30 [01:49<00:22,  4.53s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:49<00:22,  4.50s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:51<00:28,  4.80s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:52<00:12,  4.21s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:52<00:34,  4.99s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:53<00:22,  4.59s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:53<00:23,  4.64s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:54<00:17,  4.49s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:54<00:17,  4.50s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:54<00:17,  4.47s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:55<00:23,  4.74s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [01:56<00:08,  4.29s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:58<00:30,  5.04s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:58<00:18,  4.54s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:58<00:18,  4.60s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:58<00:13,  4.48s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:58<00:13,  4.49s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:58<00:13,  4.48s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [02:00<00:18,  4.69s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:00<00:04,  4.32s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:01<00:00,  3.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:01<00:00,  4.04s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [02:02<00:13,  4.51s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [02:02<00:24,  4.97s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [02:03<00:13,  4.58s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:03<00:09,  4.53s/it]#015Loading checkpoint shards:  93%|█████████▎| 28/30 [02:03<00:09,  4.53s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:03<00:09,  4.54s/it]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [02:04<00:13,  4.65s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:07<00:09,  4.58s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [02:07<00:19,  4.87s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:07<00:04,  4.49s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:07<00:04,  4.53s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:07<00:09,  4.65s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:07<00:04,  4.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  3.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  4.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  3.31s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  4.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  3.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:08<00:00,  4.28s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:09<00:09,  4.70s/it]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [02:11<00:13,  4.65s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:11<00:04,  4.58s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:12<00:04,  4.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:12<00:00,  3.34s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:12<00:00,  4.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:12<00:00,  3.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:12<00:00,  4.43s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:14<00:04,  4.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:14<00:00,  3.39s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:14<00:00,  4.48s/it]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [02:15<00:09,  4.54s/it]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [02:19<00:04,  4.39s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  3.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:21<00:00,  4.70s/it]\n",
      "Applying formatting function to train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]\n",
      "Applying formatting function to train dataset:  17%|█▋        | 2000/12008 [00:00<00:01, 5535.23 examples/s]\n",
      "Applying formatting function to train dataset:  34%|███▍      | 4101/12008 [00:00<00:00, 9918.75 examples/s]\n",
      "Applying formatting function to train dataset:  52%|█████▏    | 6262/12008 [00:00<00:00, 13285.42 examples/s]\n",
      "Applying formatting function to train dataset:  70%|███████   | 8448/12008 [00:00<00:00, 15782.32 examples/s]\n",
      "Applying formatting function to train dataset:  88%|████████▊ | 10624/12008 [00:00<00:00, 17537.36 examples/s]\n",
      "Applying formatting function to train dataset: 100%|██████████| 12008/12008 [00:00<00:00, 14409.54 examples/s]\n",
      "Adding EOS to train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]\n",
      "Adding EOS to train dataset:  19%|█▉        | 2331/12008 [00:00<00:00, 23198.35 examples/s]\n",
      "Adding EOS to train dataset:  39%|███▉      | 4698/12008 [00:00<00:00, 23473.09 examples/s]\n",
      "Adding EOS to train dataset:  68%|██████▊   | 8191/12008 [00:00<00:00, 23358.82 examples/s]\n",
      "Adding EOS to train dataset:  88%|████████▊ | 10581/12008 [00:00<00:00, 23543.24 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 12008/12008 [00:00<00:00, 23304.82 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]\n",
      "Tokenizing train dataset:   2%|▏         | 209/12008 [00:00<00:05, 2070.06 examples/s]\n",
      "Tokenizing train dataset:   4%|▍         | 463/12008 [00:00<00:04, 2342.09 examples/s]\n",
      "Tokenizing train dataset:   6%|▌         | 716/12008 [00:00<00:04, 2416.64 examples/s]\n",
      "Tokenizing train dataset:   8%|▊         | 1000/12008 [00:00<00:05, 2079.46 examples/s]\n",
      "Tokenizing train dataset:  10%|█         | 1248/12008 [00:00<00:04, 2202.72 examples/s]\n",
      "Tokenizing train dataset:  13%|█▎        | 1506/12008 [00:00<00:04, 2317.13 examples/s]\n",
      "Tokenizing train dataset:  15%|█▍        | 1786/12008 [00:00<00:04, 2445.71 examples/s]\n",
      "Tokenizing train dataset:  18%|█▊        | 2123/12008 [00:00<00:04, 2285.20 examples/s]\n",
      "Tokenizing train dataset:  20%|█▉        | 2398/12008 [00:01<00:03, 2406.03 examples/s]\n",
      "Tokenizing train dataset:  22%|██▏       | 2648/12008 [00:01<00:03, 2427.07 examples/s]\n",
      "Tokenizing train dataset:  25%|██▍       | 2993/12008 [00:01<00:03, 2370.30 examples/s]\n",
      "Tokenizing train dataset:  27%|██▋       | 3290/12008 [00:01<00:03, 2232.39 examples/s]\n",
      "Tokenizing train dataset:  29%|██▉       | 3535/12008 [00:01<00:03, 2279.13 examples/s]\n",
      "Tokenizing train dataset:  32%|███▏      | 3805/12008 [00:01<00:03, 2387.25 examples/s]\n",
      "Tokenizing train dataset:  34%|███▍      | 4124/12008 [00:01<00:03, 2287.45 examples/s]\n",
      "Tokenizing train dataset:  36%|███▋      | 4381/12008 [00:01<00:03, 2357.94 examples/s]\n",
      "Tokenizing train dataset:  39%|███▉      | 4730/12008 [00:02<00:03, 2340.53 examples/s]\n",
      "Tokenizing train dataset:  42%|████▏     | 5000/12008 [00:02<00:03, 2289.23 examples/s]\n",
      "Tokenizing train dataset:  44%|████▍     | 5261/12008 [00:02<00:02, 2364.07 examples/s]\n",
      "Tokenizing train dataset:  46%|████▌     | 5508/12008 [00:02<00:02, 2390.87 examples/s]\n",
      "Tokenizing train dataset:  48%|████▊     | 5776/12008 [00:02<00:02, 2459.71 examples/s]\n",
      "Tokenizing train dataset:  51%|█████     | 6113/12008 [00:02<00:02, 2305.88 examples/s]\n",
      "Tokenizing train dataset:  54%|█████▍    | 6481/12008 [00:02<00:02, 2351.75 examples/s]\n",
      "Tokenizing train dataset:  56%|█████▋    | 6765/12008 [00:02<00:02, 2467.15 examples/s]\n",
      "Tokenizing train dataset:  59%|█████▉    | 7135/12008 [00:03<00:02, 2352.70 examples/s]\n",
      "Tokenizing train dataset:  61%|██████▏   | 7383/12008 [00:03<00:01, 2376.38 examples/s]\n",
      "Tokenizing train dataset:  64%|██████▎   | 7649/12008 [00:03<00:01, 2443.06 examples/s]\n",
      "Tokenizing train dataset:  66%|██████▌   | 7907/12008 [00:03<00:01, 2477.57 examples/s]\n",
      "Tokenizing train dataset:  69%|██████▉   | 8265/12008 [00:03<00:01, 2394.89 examples/s]\n",
      "Tokenizing train dataset:  71%|███████   | 8524/12008 [00:03<00:01, 2434.39 examples/s]\n",
      "Tokenizing train dataset:  73%|███████▎  | 8777/12008 [00:03<00:01, 2459.20 examples/s]\n",
      "Tokenizing train dataset:  76%|███████▌  | 9124/12008 [00:03<00:01, 2356.61 examples/s]\n",
      "Tokenizing train dataset:  78%|███████▊  | 9395/12008 [00:03<00:01, 2442.58 examples/s]\n",
      "Tokenizing train dataset:  80%|████████  | 9665/12008 [00:04<00:00, 2507.69 examples/s]\n",
      "Tokenizing train dataset:  83%|████████▎ | 9923/12008 [00:04<00:00, 2520.86 examples/s]\n",
      "Tokenizing train dataset:  85%|████████▌ | 10244/12008 [00:04<00:00, 2343.06 examples/s]\n",
      "Tokenizing train dataset:  88%|████████▊ | 10619/12008 [00:04<00:00, 2390.05 examples/s]\n",
      "Tokenizing train dataset:  91%|█████████ | 10899/12008 [00:04<00:00, 2487.60 examples/s]\n",
      "Tokenizing train dataset:  94%|█████████▎| 11239/12008 [00:04<00:00, 2329.22 examples/s]\n",
      "Tokenizing train dataset:  96%|█████████▌| 11525/12008 [00:04<00:00, 2453.79 examples/s]\n",
      "Tokenizing train dataset:  98%|█████████▊| 11793/12008 [00:04<00:00, 2505.97 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 12008/12008 [00:05<00:00, 2369.96 examples/s]\n",
      "Truncating train dataset:   0%|          | 0/12008 [00:00<?, ? examples/s]\n",
      "Truncating train dataset: 100%|██████████| 12008/12008 [00:00<00:00, 622023.00 examples/s]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Applying formatting function to eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]\n",
      "Applying formatting function to eval dataset:  69%|██████▉   | 2082/3003 [00:00<00:00, 20648.50 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 3003/3003 [00:00<00:00, 20563.33 examples/s]\n",
      "Adding EOS to eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]\n",
      "Adding EOS to eval dataset:  80%|███████▉  | 2400/3003 [00:00<00:00, 23890.94 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 3003/3003 [00:00<00:00, 23372.69 examples/s]\n",
      "Tokenizing eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]\n",
      "Tokenizing eval dataset:   8%|▊         | 239/3003 [00:00<00:01, 2364.59 examples/s]\n",
      "Tokenizing eval dataset:  20%|█▉        | 591/3003 [00:00<00:01, 2339.52 examples/s]\n",
      "Tokenizing eval dataset:  28%|██▊       | 851/3003 [00:00<00:00, 2436.73 examples/s]\n",
      "Tokenizing eval dataset:  38%|███▊      | 1145/3003 [00:00<00:00, 2212.26 examples/s]\n",
      "Tokenizing eval dataset:  47%|████▋     | 1403/3003 [00:00<00:00, 2319.19 examples/s]\n",
      "Tokenizing eval dataset:  56%|█████▌    | 1687/3003 [00:00<00:00, 2470.55 examples/s]\n",
      "Tokenizing eval dataset:  65%|██████▍   | 1948/3003 [00:00<00:00, 2510.52 examples/s]\n",
      "Tokenizing eval dataset:  74%|███████▍  | 2233/3003 [00:01<00:00, 1367.46 examples/s]\n",
      "Tokenizing eval dataset:  83%|████████▎ | 2494/3003 [00:01<00:00, 1592.82 examples/s]\n",
      "Tokenizing eval dataset:  92%|█████████▏| 2761/3003 [00:01<00:00, 1815.36 examples/s]\n",
      "Tokenizing eval dataset: 100%|█████████▉| 3000/3003 [00:01<00:00, 1886.74 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 3003/3003 [00:01<00:00, 1965.27 examples/s]\n",
      "Truncating eval dataset:   0%|          | 0/3003 [00:00<?, ? examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 3003/3003 [00:00<00:00, 463171.84 examples/s]\n",
      "trainable params: 207,093,760 || all params: 70,760,800,256 || trainable%: 0.2927\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128004}.\n",
      "/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:1962: UserWarning: Upcasted low precision parameters in Linear because mixed precision turned on in FSDP. Affects: weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:1968: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "0%|          | 0/100 [00:00<?, ?it/s]\n",
      "1%|          | 1/100 [00:58<1:36:09, 58.28s/it]\n",
      "2%|▏         | 2/100 [01:44<1:23:34, 51.17s/it]\n",
      "3%|▎         | 3/100 [02:30<1:19:03, 48.90s/it]\n",
      "4%|▍         | 4/100 [03:09<1:11:55, 44.96s/it]\n",
      "5%|▌         | 5/100 [03:55<1:11:30, 45.17s/it]\n",
      "6%|▌         | 6/100 [04:35<1:08:22, 43.65s/it]\n",
      "7%|▋         | 7/100 [05:22<1:09:16, 44.69s/it]\n",
      "8%|▊         | 8/100 [06:09<1:09:39, 45.43s/it]\n",
      "9%|▉         | 9/100 [06:49<1:06:03, 43.56s/it]\n",
      "10%|█         | 10/100 [07:31<1:04:56, 43.30s/it]\n",
      "{'loss': 1.6299, 'grad_norm': 0.11208777129650116, 'learning_rate': 0.0002, 'entropy': 1.5131276369094848, 'num_tokens': 108145.0, 'mean_token_accuracy': 0.639604139328003, 'epoch': 0.05}\n",
      "10%|█         | 10/100 [07:31<1:04:56, 43.30s/it]\n",
      "11%|█         | 11/100 [08:10<1:02:02, 41.82s/it]\n",
      "12%|█▏        | 12/100 [08:57<1:03:37, 43.38s/it]\n",
      "13%|█▎        | 13/100 [09:34<1:00:01, 41.40s/it]\n",
      "14%|█▍        | 14/100 [10:12<58:14, 40.63s/it]\n",
      "15%|█▌        | 15/100 [11:00<1:00:30, 42.71s/it]\n",
      "16%|█▌        | 16/100 [11:46<1:01:17, 43.78s/it]\n",
      "17%|█▋        | 17/100 [12:33<1:01:53, 44.74s/it]\n",
      "18%|█▊        | 18/100 [13:08<57:09, 41.82s/it]\n",
      "19%|█▉        | 19/100 [13:53<57:46, 42.79s/it]\n",
      "20%|██        | 20/100 [14:38<57:50, 43.38s/it]\n",
      "{'loss': 1.3864, 'grad_norm': 0.11224852502346039, 'learning_rate': 0.0002, 'entropy': 1.3881786465644836, 'num_tokens': 213244.0, 'mean_token_accuracy': 0.6795344114303589, 'epoch': 0.11}\n",
      "20%|██        | 20/100 [14:38<57:50, 43.38s/it]\n",
      "21%|██        | 21/100 [15:23<57:44, 43.85s/it]\n",
      "22%|██▏       | 22/100 [16:09<57:52, 44.52s/it]\n",
      "23%|██▎       | 23/100 [16:55<57:42, 44.96s/it]\n",
      "24%|██▍       | 24/100 [17:32<53:42, 42.40s/it]\n",
      "25%|██▌       | 25/100 [18:13<52:33, 42.05s/it]\n",
      "26%|██▌       | 26/100 [18:53<51:01, 41.38s/it]\n",
      "27%|██▋       | 27/100 [19:31<49:21, 40.56s/it]\n",
      "28%|██▊       | 28/100 [20:12<48:53, 40.74s/it]\n",
      "29%|██▉       | 29/100 [20:59<50:17, 42.50s/it]\n",
      "30%|███       | 30/100 [21:46<51:19, 44.00s/it]\n",
      "{'loss': 1.3981, 'grad_norm': 0.13681794703006744, 'learning_rate': 0.0002, 'entropy': 1.3913979053497314, 'num_tokens': 328952.0, 'mean_token_accuracy': 0.6710592091083527, 'epoch': 0.16}\n",
      "30%|███       | 30/100 [21:46<51:19, 44.00s/it]\n",
      "31%|███       | 31/100 [22:32<51:16, 44.59s/it]\n",
      "32%|███▏      | 32/100 [23:19<51:04, 45.07s/it]\n",
      "33%|███▎      | 33/100 [23:56<47:50, 42.84s/it]\n",
      "34%|███▍      | 34/100 [24:35<45:51, 41.69s/it]\n",
      "35%|███▌      | 35/100 [25:08<42:12, 38.97s/it]\n",
      "36%|███▌      | 36/100 [25:54<43:55, 41.18s/it]\n",
      "37%|███▋      | 37/100 [26:41<44:56, 42.80s/it]\n",
      "38%|███▊      | 38/100 [27:28<45:31, 44.06s/it]\n",
      "39%|███▉      | 39/100 [28:14<45:29, 44.75s/it]\n",
      "40%|████      | 40/100 [29:01<45:22, 45.37s/it]\n",
      "{'loss': 1.3235, 'grad_norm': 0.10152433812618256, 'learning_rate': 0.0002, 'entropy': 1.3645096182823182, 'num_tokens': 444071.0, 'mean_token_accuracy': 0.6788823664188385, 'epoch': 0.21}\n",
      "40%|████      | 40/100 [29:01<45:22, 45.37s/it]\n",
      "41%|████      | 41/100 [29:47<44:55, 45.69s/it]\n",
      "42%|████▏     | 42/100 [30:34<44:17, 45.81s/it]\n",
      "43%|████▎     | 43/100 [31:20<43:38, 45.94s/it]\n",
      "44%|████▍     | 44/100 [32:06<42:56, 46.02s/it]\n",
      "45%|████▌     | 45/100 [32:52<42:10, 46.01s/it]\n",
      "46%|████▌     | 46/100 [33:30<39:19, 43.69s/it]\n",
      "47%|████▋     | 47/100 [34:17<39:24, 44.62s/it]\n",
      "48%|████▊     | 48/100 [34:54<36:41, 42.33s/it]\n",
      "49%|████▉     | 49/100 [35:37<36:06, 42.47s/it]\n",
      "50%|█████     | 50/100 [36:24<36:27, 43.75s/it]\n",
      "{'loss': 1.2603, 'grad_norm': 0.11455947160720825, 'learning_rate': 0.0002, 'entropy': 1.2888413667678833, 'num_tokens': 555990.0, 'mean_token_accuracy': 0.6949312210083007, 'epoch': 0.27}\n",
      "50%|█████     | 50/100 [36:24<36:27, 43.75s/it]\n",
      "51%|█████     | 51/100 [37:01<34:17, 41.99s/it]\n",
      "52%|█████▏    | 52/100 [37:49<34:50, 43.54s/it]\n",
      "53%|█████▎    | 53/100 [38:24<32:14, 41.16s/it]\n",
      "54%|█████▍    | 54/100 [39:11<32:51, 42.86s/it]\n",
      "55%|█████▌    | 55/100 [39:44<29:49, 39.76s/it]\n",
      "56%|█████▌    | 56/100 [40:30<30:36, 41.73s/it]\n",
      "57%|█████▋    | 57/100 [41:11<29:42, 41.44s/it]\n",
      "58%|█████▊    | 58/100 [41:57<30:02, 42.91s/it]\n",
      "59%|█████▉    | 59/100 [42:43<29:54, 43.76s/it]\n",
      "60%|██████    | 60/100 [43:19<27:45, 41.64s/it]\n",
      "{'loss': 1.364, 'grad_norm': 0.08534834533929825, 'learning_rate': 0.0002, 'entropy': 1.3773842692375182, 'num_tokens': 662312.0, 'mean_token_accuracy': 0.6731994271278381, 'epoch': 0.32}\n",
      "60%|██████    | 60/100 [43:19<27:45, 41.64s/it]\n",
      "61%|██████    | 61/100 [44:03<27:26, 42.23s/it]\n",
      "62%|██████▏   | 62/100 [44:50<27:37, 43.62s/it]\n",
      "63%|██████▎   | 63/100 [45:29<26:08, 42.38s/it]\n",
      "64%|██████▍   | 64/100 [46:07<24:32, 40.90s/it]\n",
      "65%|██████▌   | 65/100 [46:54<24:52, 42.64s/it]\n",
      "66%|██████▌   | 66/100 [47:39<24:39, 43.51s/it]\n",
      "67%|██████▋   | 67/100 [48:26<24:25, 44.40s/it]\n",
      "68%|██████▊   | 68/100 [49:12<24:04, 45.15s/it]\n",
      "69%|██████▉   | 69/100 [49:59<23:33, 45.61s/it]\n",
      "70%|███████   | 70/100 [50:45<22:48, 45.61s/it]\n",
      "{'loss': 1.2248, 'grad_norm': 0.10357503592967987, 'learning_rate': 0.0002, 'entropy': 1.2192053556442262, 'num_tokens': 782631.0, 'mean_token_accuracy': 0.7021449446678162, 'epoch': 0.37}\n",
      "70%|███████   | 70/100 [50:45<22:48, 45.61s/it]\n",
      "71%|███████   | 71/100 [51:25<21:19, 44.13s/it]\n",
      "72%|███████▏  | 72/100 [52:12<20:52, 44.73s/it]\n",
      "73%|███████▎  | 73/100 [52:57<20:12, 44.90s/it]\n",
      "74%|███████▍  | 74/100 [53:43<19:37, 45.30s/it]\n",
      "75%|███████▌  | 75/100 [54:29<18:58, 45.53s/it]\n",
      "76%|███████▌  | 76/100 [55:16<18:22, 45.95s/it]\n",
      "77%|███████▋  | 77/100 [55:55<16:45, 43.74s/it]\n",
      "78%|███████▊  | 78/100 [56:41<16:20, 44.56s/it]\n",
      "79%|███████▉  | 79/100 [57:27<15:46, 45.08s/it]\n",
      "80%|████████  | 80/100 [58:13<15:07, 45.36s/it]\n",
      "{'loss': 1.2355, 'grad_norm': 0.10273676365613937, 'learning_rate': 0.0002, 'entropy': 1.272364068031311, 'num_tokens': 889389.0, 'mean_token_accuracy': 0.7015823304653168, 'epoch': 0.43}\n",
      "80%|████████  | 80/100 [58:13<15:07, 45.36s/it]\n",
      "81%|████████  | 81/100 [59:00<14:31, 45.88s/it]\n",
      "82%|████████▏ | 82/100 [59:47<13:49, 46.11s/it]\n",
      "83%|████████▎ | 83/100 [1:00:34<13:08, 46.39s/it]\n",
      "84%|████████▍ | 84/100 [1:01:11<11:36, 43.54s/it]\n",
      "85%|████████▌ | 85/100 [1:01:58<11:10, 44.69s/it]\n",
      "86%|████████▌ | 86/100 [1:02:45<10:32, 45.18s/it]\n",
      "87%|████████▋ | 87/100 [1:03:31<09:52, 45.55s/it]\n",
      "88%|████████▊ | 88/100 [1:04:18<09:09, 45.79s/it]\n",
      "89%|████████▉ | 89/100 [1:05:02<08:17, 45.25s/it]\n",
      "90%|█████████ | 90/100 [1:05:48<07:34, 45.48s/it]\n",
      "{'loss': 1.2264, 'grad_norm': 0.10462836921215057, 'learning_rate': 0.0002, 'entropy': 1.252699077129364, 'num_tokens': 998085.0, 'mean_token_accuracy': 0.7018890619277954, 'epoch': 0.48}\n",
      "90%|█████████ | 90/100 [1:05:48<07:34, 45.48s/it]\n",
      "91%|█████████ | 91/100 [1:06:33<06:49, 45.52s/it]\n",
      "92%|█████████▏| 92/100 [1:07:19<06:06, 45.75s/it]\n",
      "93%|█████████▎| 93/100 [1:07:57<05:02, 43.15s/it]\n",
      "94%|█████████▍| 94/100 [1:08:40<04:20, 43.38s/it]\n",
      "95%|█████████▌| 95/100 [1:09:27<03:41, 44.31s/it]\n",
      "96%|█████████▌| 96/100 [1:10:14<03:00, 45.06s/it]\n",
      "97%|█████████▋| 97/100 [1:10:49<02:06, 42.07s/it]\n",
      "98%|█████████▊| 98/100 [1:11:26<01:21, 40.69s/it]\n",
      "99%|█████████▉| 99/100 [1:12:13<00:42, 42.47s/it]\n",
      "100%|██████████| 100/100 [1:13:00<00:00, 43.96s/it]\n",
      "{'loss': 1.2788, 'grad_norm': 0.09648602455854416, 'learning_rate': 0.0002, 'entropy': 1.2938026785850525, 'num_tokens': 1107114.0, 'mean_token_accuracy': 0.6880885422229767, 'epoch': 0.53}\n",
      "100%|██████████| 100/100 [1:13:00<00:00, 43.96s/it]\n",
      "0%|          | 0/47 [00:00<?, ?it/s]#033[A\n",
      "4%|▍         | 2/47 [00:16<06:19,  8.42s/it]#033[A\n",
      "6%|▋         | 3/47 [00:34<09:02, 12.32s/it]#033[A\n",
      "9%|▊         | 4/47 [00:52<10:18, 14.39s/it]#033[A\n",
      "11%|█         | 5/47 [01:10<10:54, 15.59s/it]#033[A\n",
      "13%|█▎        | 6/47 [01:26<10:42, 15.67s/it]#033[A\n",
      "15%|█▍        | 7/47 [01:44<10:57, 16.43s/it]#033[A\n",
      "17%|█▋        | 8/47 [02:02<11:01, 16.96s/it]#033[A\n",
      "19%|█▉        | 9/47 [02:19<10:48, 17.06s/it]#033[A\n",
      "21%|██▏       | 10/47 [02:37<10:38, 17.26s/it]#033[A\n",
      "23%|██▎       | 11/47 [02:55<10:27, 17.44s/it]#033[A\n",
      "26%|██▌       | 12/47 [03:13<10:16, 17.61s/it]#033[A\n",
      "28%|██▊       | 13/47 [03:30<10:00, 17.66s/it]#033[A\n",
      "30%|██▉       | 14/47 [03:46<09:18, 16.93s/it]#033[A\n",
      "32%|███▏      | 15/47 [04:04<09:10, 17.21s/it]#033[A\n",
      "34%|███▍      | 16/47 [04:21<08:58, 17.37s/it]#033[A\n",
      "36%|███▌      | 17/47 [04:39<08:46, 17.55s/it]#033[A\n",
      "38%|███▊      | 18/47 [04:57<08:29, 17.58s/it]#033[A\n",
      "40%|████      | 19/47 [05:15<08:15, 17.70s/it]#033[A\n",
      "43%|████▎     | 20/47 [05:30<07:34, 16.85s/it]#033[A\n",
      "45%|████▍     | 21/47 [05:46<07:16, 16.77s/it]#033[A\n",
      "47%|████▋     | 22/47 [06:04<07:05, 17.04s/it]#033[A\n",
      "49%|████▉     | 23/47 [06:20<06:39, 16.66s/it]#033[A\n",
      "51%|█████     | 24/47 [06:38<06:32, 17.05s/it]#033[A\n",
      "53%|█████▎    | 25/47 [06:53<06:01, 16.42s/it]#033[A\n",
      "55%|█████▌    | 26/47 [07:11<05:54, 16.87s/it]#033[A\n",
      "57%|█████▋    | 27/47 [07:25<05:23, 16.16s/it]#033[A\n",
      "60%|█████▉    | 28/47 [07:41<05:06, 16.15s/it]#033[A\n",
      "62%|██████▏   | 29/47 [07:59<04:59, 16.65s/it]#033[A\n",
      "64%|██████▍   | 30/47 [08:17<04:48, 16.98s/it]#033[A\n",
      "66%|██████▌   | 31/47 [08:34<04:34, 17.18s/it]#033[A\n",
      "68%|██████▊   | 32/47 [08:52<04:21, 17.40s/it]#033[A\n",
      "70%|███████   | 33/47 [09:10<04:05, 17.56s/it]#033[A\n",
      "72%|███████▏  | 34/47 [09:28<03:48, 17.59s/it]#033[A\n",
      "74%|███████▍  | 35/47 [09:42<03:18, 16.54s/it]#033[A\n",
      "77%|███████▋  | 36/47 [09:58<02:58, 16.27s/it]#033[A\n",
      "79%|███████▊  | 37/47 [10:15<02:46, 16.70s/it]#033[A\n",
      "81%|████████  | 38/47 [10:33<02:32, 16.98s/it]#033[A\n",
      "83%|████████▎ | 39/47 [10:48<02:11, 16.39s/it]#033[A\n",
      "85%|████████▌ | 40/47 [11:02<01:49, 15.68s/it]#033[A\n",
      "87%|████████▋ | 41/47 [11:17<01:33, 15.60s/it]#033[A\n",
      "89%|████████▉ | 42/47 [11:32<01:16, 15.24s/it]#033[A\n",
      "91%|█████████▏| 43/47 [11:50<01:03, 15.98s/it]#033[A\n",
      "94%|█████████▎| 44/47 [12:07<00:49, 16.51s/it]#033[A\n",
      "96%|█████████▌| 45/47 [12:22<00:31, 15.87s/it]#033[A\n",
      "98%|█████████▊| 46/47 [12:39<00:16, 16.41s/it]#033[A\n",
      "100%|██████████| 47/47 [12:56<00:00, 16.49s/it]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 1.282362699508667, 'eval_runtime': 793.6713, 'eval_samples_per_second': 3.784, 'eval_steps_per_second': 0.059, 'eval_entropy': 1.2572821023616385, 'eval_num_tokens': 1107114.0, 'eval_mean_token_accuracy': 0.6994581920035342, 'epoch': 0.53}\n",
      "100%|██████████| 100/100 [1:26:14<00:00, 43.96s/it]\n",
      "100%|██████████| 47/47 [12:56<00:00, 16.49s/it]#033[A\n",
      "#033[A\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:859: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`. \n",
      "  warnings.warn(\n",
      "{'train_runtime': 5281.9928, 'train_samples_per_second': 1.212, 'train_steps_per_second': 0.019, 'train_loss': 1.3327703952789307, 'epoch': 0.53}\n",
      "100%|██████████| 100/100 [1:28:01<00:00, 43.96s/it]\n",
      "100%|██████████| 100/100 [1:28:01<00:00, 52.82s/it]\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "Trying to load a Peft model. It might take a while without feedback\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:53,  3.90s/it]\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:07<01:48,  3.87s/it]\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:11<01:47,  4.00s/it]\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:16<01:45,  4.05s/it]\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:19<01:39,  3.99s/it]\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:23<01:34,  3.95s/it]\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:27<01:30,  3.92s/it]\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:31<01:27,  3.99s/it]\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:35<01:25,  4.05s/it]\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:39<01:20,  4.01s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:43<01:15,  3.97s/it]\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:47<01:10,  3.94s/it]\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:51<01:08,  4.01s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [00:55<01:04,  4.06s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [00:59<01:00,  4.02s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [01:03<00:55,  3.98s/it]\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [01:07<00:51,  3.95s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [01:11<00:48,  4.02s/it]\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [01:16<00:44,  4.06s/it]\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [01:19<00:40,  4.04s/it]\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [01:23<00:35,  3.99s/it]\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [01:27<00:31,  3.97s/it]\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:31<00:28,  4.03s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:36<00:24,  4.07s/it]\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:40<00:20,  4.02s/it]\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:43<00:15,  3.98s/it]\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:47<00:11,  3.96s/it]\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [01:51<00:08,  4.02s/it]\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [01:56<00:04,  4.05s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:57<00:00,  3.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:57<00:00,  3.93s/it]\n",
      "Saving the newly created merged model to /opt/ml/model\n",
      "/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W903 20:48:46.575459716 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "2025-09-03 20:49:02,986 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-09-03 20:49:02,986 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-09-03 20:49:02,986 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2025-09-03 20:49:24 Uploading - Uploading generated training model\n",
      "2025-09-03 20:58:00 Completed - Training job completed\n",
      "Training seconds: 7277\n",
      "Billable seconds: 7277\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4be615-cd81-4fd4-a707-f970bfd53ea7",
   "metadata": {},
   "source": [
    "## Deploy the Fine-tuned model in a Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124c4e20-0f54-412d-8126-df1acd71f6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py311\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.6.0-tgi3.2.3-gpu-py311-cu124-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,)\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973469ea-37e1-4b06-a348-c37ec395c2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g6e.12xlarge\"\n",
    "health_check_timeout = 1200 # 20 minutes\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",       # Path to the model in the container\n",
    "  'SM_NUM_GPUS': \"4\",                   # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"1024\",           # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"2048\",           # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_PREFILL_TOKENS': \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  # path to s3 bucket with model, we are not using a compressed model\n",
    "  # {'S3DataSource':{'S3Uri': \"s3://...\",'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  model_data=pytorch_estimator.model_data,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce70715b-0513-4d56-bb34-6470965b013a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-277\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-860\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 20 minutes to give SageMaker the time to download and merge model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c335fd8-cd0c-4be9-959a-c1f00b5d9e03",
   "metadata": {},
   "source": [
    "#### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edbabcac-8998-49b2-b0b9-57fd42a9b9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"</s>\"],\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "\n",
    "prompt = \"Tell me about AWS SageMaker\"\n",
    "payload = {\n",
    "    \"inputs\":  prompt,\n",
    "    \"parameters\": inference_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f76f8e78-b5f1-4563-a618-00f58f702679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\nContext: \\nResponse: Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models.\\n\\nWith SageMaker, you can collect and prepare your data, choose an algorithm, train a model, tune and optimize it for deployment, make predictions, and take action. You can use SageMaker with your own data sources for text, image, audio, and other types of data, and get complete access to the most in-demand ML capabilities.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda29284-9150-47f2-b84a-7a97c631ea67",
   "metadata": {},
   "source": [
    "#### Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69985f39-3bda-4eef-8412-5a095d9e2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-277\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-860\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2025-09-03-20-58-31-860\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3282f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
