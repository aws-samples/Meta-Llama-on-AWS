{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39252e80-3578-46c0-be46-301c5bc1320c",
   "metadata": {},
   "source": [
    "# GRPO Fine Tuning Llama-3.1 8B model with HuggingFace Estimator on ml.g6e.48xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771f7e1-7a18-4a48-970a-355f4a63ff74",
   "metadata": {},
   "source": [
    "## Source Directory structure\n",
    "\n",
    "```\n",
    "src/config.yaml - Contains all Training Args including Hyperparameters required for training.\n",
    "src/grpo_training.py - Training script (Contains multiple functions including reward and training functions. Check script args within @dataclass if prompt_length to be udpated.)\n",
    "src/requirements.txt - Dependencies to be installed before training.\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c232e1-15c3-41dc-809c-49e44172dc54",
   "metadata": {},
   "source": [
    "### Overview of grpo_training.py script\n",
    "```\n",
    "class ScriptArguments - Handles the script args like train_datset, test_dataset path, model_id, etc.,\n",
    "def merge_and_save_model - Saves the PEFT adapter model with the base model post training.\n",
    "def format_reward - Reward function(1) to generate rewards based on exact(re) match between groudtruth and model generated responses.\n",
    "def accuracy_reward - Reward function(2) to generate rewards based on the semantic similarity between groudtruth and model generated responses using sentence transformers.\n",
    "def training_function - Training function with Lora config, Quant config, GRPO config and GRPO trainer.\n",
    "def main - main function.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f507da9-78bd-4dbf-a747-b254ebdc5ebe",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656dbec-24c9-41f9-97b1-f6cdc48642e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker transformers datasets \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6f12d-982c-4f0e-88a3-318923abc97b",
   "metadata": {},
   "source": [
    "## Login to huggingface using your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ddef5-129d-4990-b7e0-a0ebedf47d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac7de5-2df6-4140-b2bf-0659ac6a3120",
   "metadata": {},
   "source": [
    "## Import Sagemaker and boto3 modules and define S3 bucket for input and output data with role and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd4c30-dfc7-4550-95db-f934ba871d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "source_dir = \"./src\"\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e59eb3-74e1-430d-be51-da8e99b756f6",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef09a6-b932-4c22-8c5f-fdc295474b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_id = \"trl-lib/tldr\"\n",
    "train_dataset, test_dataset = load_dataset(dataset_id, split=[\"train[:5%]\", \"test[:1%]\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2d4fc-fdd0-4ab5-8d3d-59d5ace7e60f",
   "metadata": {},
   "source": [
    "## Transform data in chat template with system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdd328-7f6a-40c8-974f-df45d2620aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    "\"\"\"\n",
    "\n",
    "def make_conversation(data):\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['prompt']}\n",
    "        ],\n",
    "        'completion': [{'role': 'assistant', 'content':x['completion']}]\n",
    "    })\n",
    "    return data\n",
    "\n",
    "\n",
    "train_dataset = make_conversation(train_dataset)\n",
    "test_dataset = make_conversation(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc116a-2136-426f-b115-780d899b1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_json(\"train_dataset.json\")\n",
    "test_dataset.to_json(\"test_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e69d6b-d9e8-421b-9a82-ed1021c586e1",
   "metadata": {},
   "source": [
    "### Upload the train/test dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b5325-0b02-4c6d-b323-ccbc4025a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sagemaker_session_bucket}/datasets/llama3'\n",
    " \n",
    "from sagemaker.s3 import S3Uploader\n",
    "train_dataset_s3_path = S3Uploader.upload(local_path=\"./train_dataset.json\", desired_s3_uri=f\"{input_path}/train_v3\")\n",
    "test_dataset_s3_path = S3Uploader.upload(local_path=\"./test_dataset.json\", desired_s3_uri=f\"{input_path}/test_v3\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472815d-e443-4cf9-8e67-f7b0e9d823ba",
   "metadata": {},
   "source": [
    "### Upload config.yaml from source_dir to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14196e2c-8a52-4f76-a454-d4951caaf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    " \n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"{}/config.yaml\".format(source_dir)\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    " \n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd728514-3820-455f-a9a3-db950e8ab35d",
   "metadata": {},
   "source": [
    "## Training with PyTorch estimator and DLC(Deep Learning Container) Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be1d14-134a-4157-b982-025b42aa7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "train_dlc_image = \"763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\".format(sess.boto_region_name)\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-1-8b-grpo'\n",
    "\n",
    "\n",
    "# create the Estimator \n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point          = 'grpo_train.py',      # train script\n",
    "    source_dir           = source_dir,  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g6e.48xlarge',  # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    py_version           = 'py312',           # the python version used in the training job\n",
    "    image_uri            = train_dlc_image,\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/config.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    #distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    keep_alive_period_in_seconds=1800, #warm pool\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf9d79-d702-4a23-9cab-cd3f148cfb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06978d7b-c4f3-468a-b9fb-dab2942442f2",
   "metadata": {},
   "source": [
    "## Deploy the Fine-tuned model in a Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c1106-15f9-4ce6-ae6c-7b4ef7b69850",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e80c4-018c-4e8d-b291-1c7cc8159d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "hf_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,)\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {hf_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994f83b-042a-4dff-9d1c-afc3475aadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "health_check_timeout = 1200 # 20 minutes\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",       # Path to the model in the container\n",
    "  'SM_NUM_GPUS': \"4\",                   # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"1024\",           # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"2048\",           # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_PREFILL_TOKENS': \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "grpo_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  # path to s3 bucket with model, we are not using a compressed model\n",
    "  # {'S3DataSource':{'S3Uri': \"s3://...\",'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  model_data=pytorch_estimator.model_data,\n",
    "  image_uri=hf_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d1576-5876-4bf9-bfb6-b129ccf8d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "reasoning_model = grpo_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 20 minutes to give SageMaker the time to download and merge model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14383e5d-d856-4804-9b0f-6caa9370440b",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629a83b-d3a9-4212-86b2-daf8204b4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_request(messages):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = reasoning_model.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "      }\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab19ae-9315-4fc9-8084-a4ee84316111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "\n",
    "prompt = \"SUBREDDIT: r\\/tifu\\n\\nTITLE: TIFU bY brushing with Baking Soda without learning how to do it correctly.\\n\\nPOST: Always wanted White Teeth but never visited the dentist since I was 8 due to fear [gotten bad experience as a kid].        \\n\\nSo I heard that baking soda makes your teeth white if you brush your teeth with it.        \\nWhat I didn't get from all the reading, is that though it is supposed to be made into a paste, it shouldn't still be gritty.       \\n\\nI always kept my baking soda paste gritty by putting very little water.        \\n\\nAfter brushing straight with it for three months, my gum was extremely sore, but on the up side is, it is true, it is all true, I am amazed myself ! My teeth is very VERY white now compared to the past and even when taking pictures, the teeth becomes the center of attention simply because of how white it is, even my friends jokingly asked if I have painted it white.       \\nThese are the images after baking soda brushing for months, understand that I have NEVER visited a dentist ever since I was 8:    \\n   \\n\\nAs my ego grew, I forget about the irritation from the gum and keep on using it.      \\nOne fine day, my gum gave up...I was brushing and I saw a nice chunk of my gum get physically brushed OUT of my teeth, I was shocked and at a lost of what I should do...I tried to piece the gum back in hoping that it would stay, suffices to say by the very next day, the gum eventually fall off.       \\n\\nIt is not that visible if I don't smile too big, but let this be a lesson to all of you out there, baking soda paste works, BUT PLEASE, make sure the paste is not gritty, PLEASE...don't experience this ever.\\n\\nTL;DR:\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b6f1-e344-47ed-83d1-b6e211ac3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb0bca-c74d-46a9-b7cf-b4bc27f8ae05",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33dc9af-8403-4cae-9e65-281cc0a0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_model.delete_model()\n",
    "reasoning_model.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
