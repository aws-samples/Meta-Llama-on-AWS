# script parameters
model_id: "meta-llama/Llama-3.1-8B-Instruct" # Hugging Face model id
# sagemaker specific parameters
train_dataset_path: "/opt/ml/input/data/train/" # path to where SageMaker saves train dataset
test_dataset_path: "/opt/ml/input/data/test/"   # path to where SageMaker saves test dataset
output_dir: "/opt/ml/model"            # path to where SageMaker will upload the model 
# training parameters
# report metrics to tensorboard
report_to: "tensorboard" 
# learning rate 2e-4
learning_rate: 5e-6 
# learning rate scheduler
lr_scheduler_type: "cosine" 
# number of training epochs
num_train_epochs: 1 
# batch size per device during training
per_device_train_batch_size: 8 #We used 4 and 8, Please refer to the below table.
# batch size for evaluation
per_device_eval_batch_size: 8 #We used 4 and 8, Please refer to the below table.
# number of steps before performing a backward/update pass
gradient_accumulation_steps: 8 
# use torch adamw optimizer
optim: paged_adamw_8bit
# max steps
max_steps: 10 
# log every 10 steps
logging_steps: 10 
# save checkpoint every epoch
save_strategy: epoch 
# max gradient norm
max_grad_norm: 0.1 
# warmup ratio
warmup_ratio: 0.03 
#use bfloat16 precision
bf16: true 
# use tf32 precision
tf32: true 

# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard auto_wrap offload" 
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"
  cpu_ram_efficient_loading: "true"