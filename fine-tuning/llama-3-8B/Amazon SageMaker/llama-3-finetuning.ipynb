{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Fine-tune LLaMA 3 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679163a-387a-4ba6-8ce0-d5571614c0dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|llama-2-text-completion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251624f9-1eb6-4051-a774-0a4ba83cabf5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 3 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format.\n",
    "\n",
    "Note: This notebook is inspired from the LLaMA2 model fine tuning notebook from here: https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-finetuning.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9b99d-639b-40f3-91e3-1fe00ee032a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. **By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.**\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c4fcd-d6c5-4381-8425-1d224c0ac197",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Set up\n",
    "\n",
    "---\n",
    "We begin by installing and upgrading necessary packages. Restart the kernel after executing the cell below for the first time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.214.3)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.221.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.34.58)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.58 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.112)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Downloading sagemaker-2.221.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: datasets, sagemaker\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.18.0\n",
      "    Uninstalling datasets-2.18.0:\n",
      "      Successfully uninstalled datasets-2.18.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.214.3\n",
      "    Uninstalling sagemaker-2.214.3:\n",
      "      Successfully uninstalled sagemaker-2.214.3\n",
      "Successfully installed datasets-2.19.1 sagemaker-2.221.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {},
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-3 8B Instruct model as a SageMaker endpoint. To train/deploy 70B models, please change model_id to \"meta-textgeneration-llama-3-70b-instruct\" or \"meta-textgeneration-llama-3-70b\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e01401-82db-4d49-9457-f930f4138618",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-8b-instruct\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id,model_version=model_version)\n",
    "accept_eula = False # Change to True to continue to deploy the endpoint\n",
    "pretrained_predictor = pretrained_model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response['generated_text']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to find your purpose and pursue it with passion and dedication. It's to make a positive impact on the world and leave it a better place than when you entered it. It's to learn, grow, and evolve as a person, and to help others do the same. It's to find joy and fulfillment in the\n",
      "\n",
      "==================================\n",
      "\n",
      "{'generated_text': \" to find your purpose and pursue it with passion and dedication. It's to make a positive impact on the world and leave it a better place than when you entered it. It's to learn, grow, and evolve as a person, and to help others do the same. It's to find joy and fulfillment in the\"}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "         \"stop\": [\"<|eot_id|>\"]\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload)\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6773e6-7cf2-4cea-bce6-905d5995d857",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "To learn about additional use cases of the pre-trained model, please checkout the notebooks from other sessions [RAG with LlaMa 3](https://github.com/aws-samples/Meta-Llama-on-AWS/tree/main/RAG-recipes), more general text generation examples [Text completion: Run Llama 2 models in SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad1687923f410c8a8c4bd0d63c863a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718048cc84834904854e0a5c9abd7da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaad61737f494ac59519f80565a5840f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febf4e7411a3422e927e6b437ba05aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df28f47d57d1442da53b76ec66883f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2049530"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What products and services are offered by Falco electronics?',\n",
       " 'context': \"Falco's main business activities are the design and manufacture of power magnetics, semiconductors and circuitboards. In addition the company designs and manufactures common mode chokes, current sensors, gate drives, power inductors, line transformers, THT inductors, watt hour meters, lighting systems, printed computer boards, mechanical assembly systems, and also provides plastic molding, metal stamping and electronic manufacturing, OEM design and testing services. Falco is a major supplier to international OEMs and brand name electronics manufacturers alike. Falco has regionalized branches in Los Angeles and Miami in the United States; Munich, Germany; Milan, Desenzano, and Bologna, Italy; Manila, The Philippines, Bangalore, India; Xiamen, China and Hong Kong. Falco has manufacturing plants in Mexico, China and India.\",\n",
       " 'response': 'Falco designs and manufactures components used by Original Equipment Manufacturers in producing consumer electronics.  Components include semiconductors, circuitboards, sensors, transformers, inductors, and meters as well as material-related components such as plastic molding and metal stamping.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-west-2-975049888767/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA 3 8B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for training job. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for training job. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-22-06-18-864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 22:06:19 Starting - Starting the training job...\n",
      "2024-05-28 22:06:41 Pending - Training job waiting for capacity...\n",
      "2024-05-28 22:07:05 Pending - Preparing the instances for training...\n",
      "2024-05-28 22:07:39 Downloading - Downloading input data...........................\n",
      "2024-05-28 22:12:20 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-28 22:12:22,120 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-28 22:12:22,156 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-28 22:12:22,166 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-28 22:12:22,168 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-28 22:12:31,328 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.20.3-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.38.0-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.3-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.20.3->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.0->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mhuggingface-hub is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=86399b36aefa62505afb88dee82dcfc8bc3d54e06c2dd129d4f675e1ec6a2bec\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.3 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.15.2 torch-2.2.0 transformers-4.38.0 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:31,923 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:31,923 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:31,984 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:32,031 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:32,078 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:32,088 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": true,\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"1\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-8b-instruct-2024-05-28-22-06-18-864\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":true,\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":true,\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"1\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-8b-instruct-2024-05-28-22-06-18-864\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=true\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-05-28 22:13:32,119 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '1', '--micro_batch_size', '1', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-05-28 22:13:37,510] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-05-28 22:13:37,510] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-05-28 22:13:37,510] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-05-28 22:13:37,510] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14413.42it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 900.07it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 57184.36 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 15755.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 3528.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 3385.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2772.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2739.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2553.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2540.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2547.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2534.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1934.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1917.28 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2068.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2135.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2119.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2035.69 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2101.84 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2082.63 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.24s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.25s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.12s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 8030.261248 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 364\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 91\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/91 [00:09<14:56,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/91 [00:10<15:35, 10.39s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.8836528062820435\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/91 [00:09<14:32,  9.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/91 [00:09<14:28,  9.65s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/91 [00:18<13:43,  9.25s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.8301661014556885\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/91 [00:18<13:33,  9.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/91 [00:18<13:30,  9.11s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/91 [00:19<13:58,  9.43s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 2.047760009765625\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/91 [00:27<13:07,  8.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/91 [00:27<13:06,  8.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/91 [00:27<13:21,  9.10s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/91 [00:27<13:12,  9.01s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 4/91 [00:36<12:54,  8.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 4/91 [00:35<12:50,  8.85s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 2.0974996089935303\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 4/91 [00:35<12:50,  8.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 4/91 [00:36<12:59,  8.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 5/91 [00:44<12:39,  8.83s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.601803183555603\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 5/91 [00:44<12:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 5/91 [00:44<12:37,  8.81s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 5/91 [00:45<12:43,  8.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 6/91 [00:54<12:30,  8.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 6/91 [00:53<12:28,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.6661813259124756\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 6/91 [00:53<12:26,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 6/91 [00:53<12:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 7/91 [01:02<12:18,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 2.1860809326171875\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 7/91 [01:02<12:16,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 7/91 [01:02<12:17,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 7/91 [01:02<12:16,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 8/91 [01:11<12:07,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 8/91 [01:11<12:06,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 2.224822998046875\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 8/91 [01:10<12:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 8/91 [01:10<12:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 9/91 [01:19<11:57,  8.74s/it]#015Training Epoch0:  10%|#033[34m▉         #033[0m| 9/91 [01:20<11:57,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 9/91 [01:19<11:56,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.797950029373169\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 9/91 [01:19<11:56,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 10/91 [01:28<11:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 10/91 [01:28<11:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 2.0138425827026367\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 10/91 [01:28<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 10/91 [01:28<11:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 11/91 [01:37<11:40,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 11/91 [01:37<11:40,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 2.066683053970337\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 11/91 [01:37<11:40,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 11/91 [01:36<11:40,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 12/91 [01:46<11:30,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 12/91 [01:46<11:30,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.945367693901062\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 12/91 [01:45<11:30,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 12/91 [01:45<11:30,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 2.009525775909424\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 13/91 [01:54<11:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 13/91 [01:54<11:21,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 13/91 [01:54<11:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 13/91 [01:55<11:21,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 14/91 [02:03<11:12,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.551371455192566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 14/91 [02:03<11:12,  8.73s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 14/91 [02:03<11:12,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 14/91 [02:03<11:12,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 2.0224695205688477\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 15/91 [02:11<11:03,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 15/91 [02:12<11:03,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 15/91 [02:11<11:03,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 15/91 [02:12<11:03,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 16/91 [02:20<10:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 2.0000712871551514\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 16/91 [02:21<10:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 16/91 [02:20<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 16/91 [02:20<10:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4577431678771973\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 17/91 [02:29<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 17/91 [02:29<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 17/91 [02:30<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 17/91 [02:29<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 18/91 [02:38<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 18/91 [02:38<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.6593083143234253\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 18/91 [02:38<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 18/91 [02:38<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 19/91 [02:47<10:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 19/91 [02:47<10:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.8330525159835815\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 19/91 [02:46<10:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 19/91 [02:46<10:29,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 20/91 [02:56<10:20,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 20/91 [02:55<10:20,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.9029804468154907\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 20/91 [02:55<10:20,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 20/91 [02:55<10:20,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 21/91 [03:05<10:15,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 21/91 [03:04<10:14,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.6743067502975464\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 21/91 [03:04<10:15,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 21/91 [03:04<10:15,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 22/91 [03:13<10:05,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 22/91 [03:13<10:05,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 22/91 [03:13<10:05,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.8051636219024658\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 22/91 [03:13<10:05,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 23/91 [03:21<09:55,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.699088215827942\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 23/91 [03:21<09:55,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 23/91 [03:22<09:55,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 23/91 [03:22<09:55,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 24/91 [03:30<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.7930104732513428\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 24/91 [03:30<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 24/91 [03:30<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 24/91 [03:31<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 25/91 [03:40<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.3104661703109741\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 25/91 [03:39<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 25/91 [03:39<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 25/91 [03:39<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 26/91 [03:48<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 26/91 [03:48<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.7430334091186523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 26/91 [03:48<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 26/91 [03:48<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 27/91 [03:56<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.759818434715271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 27/91 [03:56<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 27/91 [03:57<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 27/91 [03:57<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.3517314195632935\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 28/91 [04:05<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 28/91 [04:05<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 28/91 [04:06<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 28/91 [04:05<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 29/91 [04:15<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 29/91 [04:14<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.4530032873153687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 29/91 [04:14<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 29/91 [04:14<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 30/91 [04:23<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 30/91 [04:23<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.8459359407424927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 30/91 [04:23<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 30/91 [04:23<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.9943956732749939\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 31/91 [04:31<08:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 31/91 [04:32<08:44,  8.74s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 31/91 [04:32<08:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 31/91 [04:31<08:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 32/91 [04:41<08:38,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.672804832458496\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 32/91 [04:40<08:38,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 32/91 [04:40<08:37,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 32/91 [04:41<08:38,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 33/91 [04:49<08:28,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 33/91 [04:49<08:28,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.059411644935608\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 33/91 [04:50<08:28,  8.76s/it]#015Training Epoch0:  36%|#033[34m███▋      #033[0m| 33/91 [04:49<08:28,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 34/91 [04:58<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 34/91 [04:58<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.6643942594528198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 34/91 [04:58<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 34/91 [04:58<08:19,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 35/91 [05:07<08:09,  8.75s/it]#015Training Epoch0:  38%|#033[34m███▊      #033[0m| 35/91 [05:07<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.6449187994003296\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 35/91 [05:06<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 35/91 [05:06<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 36/91 [05:16<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 36/91 [05:15<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.2148327827453613\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 36/91 [05:15<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 36/91 [05:15<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 37/91 [05:24<07:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.4449670314788818\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 37/91 [05:24<07:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 37/91 [05:25<07:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 37/91 [05:24<07:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 38/91 [05:33<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 38/91 [05:33<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 38/91 [05:33<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.2337372303009033\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 38/91 [05:33<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 39/91 [05:42<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 39/91 [05:41<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.2632321119308472\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 39/91 [05:42<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 39/91 [05:41<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 40/91 [05:50<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 40/91 [05:51<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 40/91 [05:50<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.3061916828155518\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 40/91 [05:50<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 41/91 [06:00<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 41/91 [05:59<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 41/91 [05:59<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.4832781553268433\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 41/91 [05:59<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 42/91 [06:08<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 42/91 [06:08<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 2.3404736518859863\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 42/91 [06:08<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 42/91 [06:08<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 43/91 [06:17<07:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 43/91 [06:16<07:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 43/91 [06:17<07:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.6131243705749512\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 43/91 [06:16<07:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 44/91 [06:26<06:53,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 44/91 [06:25<06:53,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 44/91 [06:26<06:53,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.6531291007995605\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 44/91 [06:25<06:53,  8.79s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.6448025703430176\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 45/91 [06:34<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 45/91 [06:34<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 45/91 [06:34<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 45/91 [06:35<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 46/91 [06:43<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 46/91 [06:43<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 46/91 [06:43<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.7476415634155273\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 46/91 [06:43<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 47/91 [06:52<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 47/91 [06:52<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.7692837715148926\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 47/91 [06:51<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 47/91 [06:51<06:25,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 48/91 [07:00<06:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 48/91 [07:00<06:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 48/91 [07:01<06:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.8257415294647217\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 48/91 [07:00<06:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 49/91 [07:10<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 49/91 [07:09<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.8583132028579712\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 49/91 [07:09<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 49/91 [07:09<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 50/91 [07:18<05:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.7046222686767578\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 50/91 [07:18<05:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 50/91 [07:18<05:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 50/91 [07:18<05:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 51/91 [07:27<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.5201834440231323\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 51/91 [07:26<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 51/91 [07:27<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 51/91 [07:26<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 52/91 [07:36<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 52/91 [07:35<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 52/91 [07:35<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.4668452739715576\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 52/91 [07:35<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 53/91 [07:44<05:33,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 53/91 [07:45<05:33,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 53/91 [07:44<05:33,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.3640273809432983\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 53/91 [07:44<05:33,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 54/91 [07:53<05:24,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 54/91 [07:53<05:24,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 54/91 [07:53<05:24,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.0801427364349365\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 54/91 [07:53<05:24,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.8955546021461487\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 55/91 [08:02<05:16,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 55/91 [08:02<05:16,  8.79s/it]#015Training Epoch0:  60%|#033[34m██████    #033[0m| 55/91 [08:02<05:16,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 55/91 [08:02<05:16,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 56/91 [08:11<05:07,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 56/91 [08:11<05:07,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 56/91 [08:10<05:07,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.307025671005249\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 56/91 [08:10<05:07,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 57/91 [08:20<04:57,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 57/91 [08:19<04:57,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.2978731393814087\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 57/91 [08:19<04:57,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 57/91 [08:19<04:57,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 58/91 [08:29<04:48,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 58/91 [08:28<04:48,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.712201714515686\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 58/91 [08:28<04:48,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 58/91 [08:28<04:48,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 59/91 [08:37<04:40,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.538711667060852\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 59/91 [08:37<04:40,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 59/91 [08:37<04:40,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 59/91 [08:37<04:40,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 60/91 [08:46<04:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 60/91 [08:46<04:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.7397171258926392\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 60/91 [08:45<04:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 60/91 [08:45<04:31,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 61/91 [08:55<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 61/91 [08:54<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 61/91 [08:54<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.2019367218017578\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 61/91 [08:54<04:22,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 62/91 [09:03<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.8077963590621948\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 62/91 [09:03<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 62/91 [09:03<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 62/91 [09:03<04:13,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.8504213094711304\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 63/91 [09:12<04:05,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 63/91 [09:12<04:05,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 63/91 [09:12<04:05,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 63/91 [09:12<04:05,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.377962589263916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 64/91 [09:21<03:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 64/91 [09:20<03:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 64/91 [09:20<03:56,  8.77s/it]#015Training Epoch0:  70%|#033[34m███████   #033[0m| 64/91 [09:21<03:56,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.2991629838943481\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 65/91 [09:29<03:47,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 65/91 [09:29<03:47,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 65/91 [09:29<03:47,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 65/91 [09:30<03:47,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.8015162944793701\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 66/91 [09:38<03:39,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 66/91 [09:38<03:39,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 66/91 [09:38<03:39,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 66/91 [09:39<03:39,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 67/91 [09:47<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.661514401435852\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 67/91 [09:47<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 67/91 [09:47<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 67/91 [09:47<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 68/91 [09:56<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 68/91 [09:56<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 68/91 [09:55<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.8986228704452515\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 68/91 [09:55<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 69/91 [10:05<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.7101715803146362\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 69/91 [10:04<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 69/91 [10:04<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 69/91 [10:04<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 70/91 [10:14<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 70/91 [10:13<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 70/91 [10:13<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.4728068113327026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 70/91 [10:13<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.3368949890136719\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 71/91 [10:22<02:54,  8.74s/it]#015Training Epoch0:  78%|#033[34m███████▊  #033[0m| 71/91 [10:22<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 71/91 [10:22<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 71/91 [10:22<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.277983546257019\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 72/91 [10:30<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 72/91 [10:30<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 72/91 [10:31<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 72/91 [10:31<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 73/91 [10:39<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.3356362581253052\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 73/91 [10:39<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 73/91 [10:40<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 73/91 [10:39<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 74/91 [10:49<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 74/91 [10:48<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.4159231185913086\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 74/91 [10:48<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 74/91 [10:48<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 2.322697639465332\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 75/91 [10:57<02:19,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 75/91 [10:57<02:19,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 75/91 [10:57<02:19,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 75/91 [10:57<02:19,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 76/91 [11:06<02:11,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 76/91 [11:06<02:11,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 2.4610683917999268\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 76/91 [11:05<02:11,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 76/91 [11:05<02:11,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 77/91 [11:14<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 2.4003148078918457\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 77/91 [11:14<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 77/91 [11:15<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 77/91 [11:15<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 78/91 [11:24<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 78/91 [11:23<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 78/91 [11:23<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 2.1599271297454834\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 78/91 [11:23<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 79/91 [11:32<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 79/91 [11:32<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 79/91 [11:32<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 2.124865770339966\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 79/91 [11:32<01:45,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 80/91 [11:41<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 80/91 [11:40<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 80/91 [11:41<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.6810635328292847\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 80/91 [11:40<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 81/91 [11:50<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.7903952598571777\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 81/91 [11:49<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 81/91 [11:49<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 81/91 [11:49<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 82/91 [11:59<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 2.110565185546875\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 82/91 [11:58<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 82/91 [11:58<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 82/91 [11:58<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 83/91 [12:07<01:09,  8.75s/it]#015Training Epoch0:  91%|#033[34m█████████ #033[0m| 83/91 [12:07<01:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.6881121397018433\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 83/91 [12:07<01:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 83/91 [12:07<01:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 84/91 [12:15<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 84/91 [12:16<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 84/91 [12:16<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 2.148242712020874\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 84/91 [12:16<01:01,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 85/91 [12:24<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 85/91 [12:25<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.6023887395858765\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 85/91 [12:24<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 85/91 [12:25<00:52,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 86/91 [12:34<00:43,  8.75s/it]#015Training Epoch0:  95%|#033[34m█████████▍#033[0m| 86/91 [12:33<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.5312035083770752\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 86/91 [12:33<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 86/91 [12:33<00:43,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.2652735710144043\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 87/91 [12:43<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 87/91 [12:42<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 87/91 [12:42<00:35,  8.79s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 87/91 [12:42<00:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 88/91 [12:51<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.9044569730758667\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 88/91 [12:51<00:26,  8.78s/it]#015Training Epoch0:  97%|#033[34m█████████▋#033[0m| 88/91 [12:51<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 88/91 [12:51<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.6152955293655396\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 89/91 [12:59<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 89/91 [13:00<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 89/91 [12:59<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 89/91 [13:00<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 2.2096927165985107\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 90/91 [13:08<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 90/91 [13:09<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 90/91 [13:08<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 90/91 [13:08<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.5622479915618896\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:18<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:18<00:00,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [13:17<00:00,  8.76s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 5 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.21s/it]#015evaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.21s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.75s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.75s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.74s/it]#015evaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]#015evaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]#015evaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]#015evaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.73s/it]#015evaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.73s/it]#015evaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(4.4970, device='cuda:0') eval_epoch_loss=tensor(1.5034, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.5034149885177612\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=5.2910, train_epoch_loss=1.6660, epcoh time 797.5969220479999s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.1947340965270996\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.361227035522461\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.53s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.53s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.7350053787231445\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.6510413885116577\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.1598637104034424\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.170655608177185\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8989670276641846\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.9854786396026611\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5049667358398438\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.5798025131225586\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.8239845037460327\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.8383996486663818\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.7544931173324585\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]#015Training Epoch1:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2938711643218994\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.7629008293151855\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.732714056968689\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1822447776794434\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.380798101425171\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]#015Training Epoch1:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]#015Training Epoch1:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.643332600593567\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]#015Training Epoch1:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.6371750831604004\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.5255944728851318\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:11,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.5552771091461182\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4914265871047974\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.5865572690963745\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.0947409868240356\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.4904541969299316\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.6070908308029175\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.1315925121307373\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.1766453981399536\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.6756410598754883\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]#015Training Epoch1:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.8439213037490845\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.5254237651824951\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.9161413311958313\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.5322099924087524\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.4884692430496216\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.0450122356414795\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.2649297714233398\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.0380514860153198\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.0979650020599365\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.1319854259490967\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.3268271684646606\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 2.24891996383667\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]#015Training Epoch1:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.4729303121566772\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.5212318897247314\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.5586386919021606\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.6489317417144775\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:34,  8.76s/it]#015Training Epoch1:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:34,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.6844203472137451\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:16,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.7454984188079834\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:16,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:16,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:16,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.8081238269805908\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]#015Training Epoch1:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.6353144645690918\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.4206571578979492\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.3780344724655151\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.247428059577942\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.0003366470336914\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.827212393283844\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.194515347480774\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:05,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:05,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:05,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:05,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1953195333480835\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:56,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.6402256488800049\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.4602283239364624\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.6605679988861084\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.0968154668807983\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.7177611589431763\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.7814785242080688\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.1921900510787964\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.1767257452011108\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]#015Training Epoch1:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.7335214614868164\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.76s/it]#015Training Epoch1:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.5538148880004883\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.8403371572494507\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.6390981674194336\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.397795557975769\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.2705994844436646\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.2076977491378784\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.2664945125579834\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.73s/it]#015Training Epoch1:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.3476111888885498\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]#015Training Epoch1:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 2.2779500484466553\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 2.417924404144287\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 2.3657188415527344\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 2.108217239379883\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 2.0807414054870605\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.613823652267456\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]#015Training Epoch1:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.73222815990448\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 2.054861307144165\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.6556622982025146\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]#015Training Epoch1:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 2.1070005893707275\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.5153849124908447\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.441673994064331\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.1832605600357056\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.8671637773513794\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.548485517501831\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.73s/it]#015Training Epoch1:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 2.174506425857544\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.5240094661712646\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 6 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]#015evaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]#015evaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.71s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.71s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]#015evaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]#015evaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.72s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.72s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.72s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.72s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:14<00:11,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.74s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(4.4110, device='cuda:0') eval_epoch_loss=tensor(1.4841, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.4841008186340332\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=4.5343, train_epoch_loss=1.5117, epcoh time 794.9406732070001s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.0943926572799683\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 1/91 [00:08<12:23,  8.26s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.26s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.26s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:39,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.309586524963379\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:38,  8.62s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6953728199005127\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:38,  8.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:38,  8.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:38,  8.62s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.5933805704116821\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0954705476760864\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1322917938232422\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]#015Training Epoch2:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8643386363983154\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.946764349937439\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.4548667669296265\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.4958422183990479\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.73s/it]#015Training Epoch2:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.793517827987671\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.8081943988800049\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]#015Training Epoch2:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.717923879623413\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2724193334579468\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.7195262908935547\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.6934031248092651\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.1421194076538086\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.3215742111206055\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.6165012121200562\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.5404201745986938\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.4822821617126465\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:01,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.5085583925247192\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:01,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:01,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:01,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4514577388763428\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:53,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.5271724462509155\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.73s/it]#015Training Epoch2:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9842099547386169\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.4622642993927002\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.5848932266235352\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.0940818786621094\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.74s/it]#015Training Epoch2:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.0625032186508179\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:02,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:02,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:02,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:02,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.611758828163147\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:53,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:53,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:53,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:53,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.8145865201950073\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:44,  8.74s/it]#015Training Epoch2:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:44,  8.74s/it]#015Training Epoch2:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.490169644355774\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.8797269463539124\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.5033910274505615\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.4489036798477173\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.9690130352973938\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.2009929418563843\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.9510461688041687\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]#015Training Epoch2:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.0109906196594238\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0759979486465454\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 40/91 [05:49<07:25,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.2670612335205078\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 2.2107794284820557\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.414627194404602\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.483616828918457\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.5063790082931519\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.5952821969985962\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:35,  8.80s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.6461337804794312\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 48/91 [06:59<06:16,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 48/91 [06:59<06:16,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 48/91 [06:59<06:16,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.7221431732177734\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  53%|#033[34m█████▎    #033[0m| 48/91 [06:59<06:16,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.771818995475769\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.5999258756637573\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.381098747253418\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.3405077457427979\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 52/91 [07:34<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 52/91 [07:34<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 52/91 [07:34<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  57%|#033[34m█████▋    #033[0m| 52/91 [07:34<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.1877115964889526\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.9553866386413574\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.8028894662857056\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 56/91 [08:09<05:06,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.131150722503662\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 56/91 [08:09<05:06,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 56/91 [08:09<05:06,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  62%|#033[34m██████▏   #033[0m| 56/91 [08:09<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1455262899398804\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.6009162664413452\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▍   #033[0m| 59/91 [08:35<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.416243314743042\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▍   #033[0m| 59/91 [08:35<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▍   #033[0m| 59/91 [08:35<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  65%|#033[34m██████▍   #033[0m| 59/91 [08:35<04:39,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.6204537153244019\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.041635274887085\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]#015Training Epoch2:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.6835339069366455\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 63/91 [09:10<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 63/91 [09:10<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.7488933801651\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 63/91 [09:10<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  69%|#033[34m██████▉   #033[0m| 63/91 [09:10<04:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.0565226078033447\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.087165117263794\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.6927144527435303\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▎  #033[0m| 67/91 [09:45<03:30,  8.77s/it]#015Training Epoch2:  74%|#033[34m███████▎  #033[0m| 67/91 [09:45<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.4886913299560547\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▎  #033[0m| 67/91 [09:45<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  74%|#033[34m███████▎  #033[0m| 67/91 [09:45<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]#015Training Epoch2:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.8070012331008911\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.6099804639816284\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.355385184288025\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 71/91 [10:20<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.2505666017532349\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 71/91 [10:20<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 71/91 [10:20<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 71/91 [10:20<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.1644306182861328\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]#015Training Epoch2:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.2085480690002441\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.3068304061889648\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 75/91 [10:55<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 75/91 [10:55<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 2.2468178272247314\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 75/91 [10:55<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  82%|#033[34m████████▏ #033[0m| 75/91 [10:55<02:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 2.3922715187072754\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 2.340075731277466\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 2.072399616241455\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 79/91 [11:30<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 79/91 [11:30<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 2.0538575649261475\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 79/91 [11:30<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  87%|#033[34m████████▋ #033[0m| 79/91 [11:30<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.5623657703399658\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.6976921558380127\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 2.00492262840271\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 83/91 [12:05<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.6309261322021484\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 83/91 [12:05<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 83/91 [12:05<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  91%|#033[34m█████████ #033[0m| 83/91 [12:05<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]#015Training Epoch2:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 2.077268600463867\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.4434248208999634\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▍#033[0m| 86/91 [12:31<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▍#033[0m| 86/91 [12:31<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.3722524642944336\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▍#033[0m| 86/91 [12:31<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  95%|#033[34m█████████▍#033[0m| 86/91 [12:31<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.1288961172103882\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 87/91 [12:40<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 87/91 [12:40<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 87/91 [12:40<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  96%|#033[34m█████████▌#033[0m| 87/91 [12:40<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.843307614326477\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.504712462425232\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 90/91 [13:06<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 2.145411729812622\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 90/91 [13:06<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 90/91 [13:06<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:  99%|#033[34m█████████▉#033[0m| 90/91 [13:06<00:08,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.4948104619979858\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 91/91 [13:15<00:00,  8.74s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 6 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]#015evaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.24s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:33,  4.24s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.94s/it]#015evaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.94s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.94s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.94s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.85s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]#015evaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.79s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]#015evaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]#015evaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.75s/it]#015evaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.75s/it]#015evaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.73s/it]#015evaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.73s/it]#015evaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(4.4086, device='cuda:0') eval_epoch_loss=tensor(1.4835, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.4835472106933594\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=4.3204, train_epoch_loss=1.4633, epcoh time 795.6019418149999s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]#015Training Epoch3:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   1%|#033[34m          #033[0m| 1/91 [00:08<12:24,  8.27s/it]#015Training Epoch3:   1%|#033[34m          #033[0m| 1/91 [00:08<12:24,  8.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   1%|#033[34m          #033[0m| 1/91 [00:08<12:24,  8.27s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.0063306093215942\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   1%|#033[34m          #033[0m| 1/91 [00:08<12:24,  8.27s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   2%|#033[34m▏         #033[0m| 2/91 [00:17<12:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.2736526727676392\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   2%|#033[34m▏         #033[0m| 2/91 [00:17<12:40,  8.54s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   2%|#033[34m▏         #033[0m| 2/91 [00:17<12:40,  8.55s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   2%|#033[34m▏         #033[0m| 2/91 [00:17<12:40,  8.55s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.6434708833694458\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:39,  8.63s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.68s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.5445753335952759\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:34,  8.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:35,  8.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:28,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:28,  8.70s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0485248565673828\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:28,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:28,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.0984865427017212\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:20,  8.71s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8436172008514404\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:11,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:12,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.9204180240631104\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]#015Training Epoch3:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:54,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.4070618152618408\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:54,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:54,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:54,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.4380303621292114\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.7550395727157593\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]#015Training Epoch3:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.7831141948699951\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.68327796459198\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:23,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2460274696350098\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.6921697854995728\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.6629315614700317\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.108015775680542\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]#015Training Epoch3:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2735404968261719\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.596700668334961\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.438227653503418\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.449015736579895\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:11,  8.74s/it]#015Training Epoch3:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  23%|#033[34m██▎       #033[0m| 21/91 [03:03<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.4757686853408813\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:02,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4257888793945312\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.4696449041366577\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:46,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  27%|#033[34m██▋       #033[0m| 25/91 [03:38<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.9103649854660034\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  27%|#033[34m██▋       #033[0m| 25/91 [03:38<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  27%|#033[34m██▋       #033[0m| 25/91 [03:38<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  27%|#033[34m██▋       #033[0m| 25/91 [03:38<09:37,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.4331508874893188\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.5633056163787842\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:19,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.0556707382202148\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:10,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.978397011756897\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]#015Training Epoch3:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.5558916330337524\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.7929829359054565\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.4627809524536133\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  35%|#033[34m███▌      #033[0m| 32/91 [04:39<08:35,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.8481535911560059\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.4769879579544067\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.4042361974716187\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:08,  8.73s/it]#015Training Epoch3:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:08,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.907202959060669\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  40%|#033[34m███▉      #033[0m| 36/91 [05:14<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.1386284828186035\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.8829066157341003\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.9037582874298096\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.0258742570877075\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]#015Training Epoch3:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.2199392318725586\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 2.186903953552246\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:58,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.3637815713882446\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:59,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.4500024318695068\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.461456537246704\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.5618468523025513\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.6183338165283203\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]#015Training Epoch3:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.6986662149429321\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:06,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.7405942678451538\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.5572649240493774\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.346694827079773\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 51/91 [07:25<05:49,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.307063341140747\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.1303530931472778\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:32,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.9189674854278564\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]#015Training Epoch3:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.7809231281280518\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  60%|#033[34m██████    #033[0m| 55/91 [08:00<05:14,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 1.0631335973739624\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.1050435304641724\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.5697532892227173\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]#015Training Epoch3:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.3764344453811646\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.5875027179718018\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]#015Training Epoch3:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.9989268779754639\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:22,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.6541913747787476\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.7184113264083862\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.9283205270767212\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.983406126499176\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:47,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.6552766561508179\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]#015Training Epoch3:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  73%|#033[34m███████▎  #033[0m| 66/91 [09:36<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.4273289442062378\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.7792633771896362\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.5808212757110596\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.3137586116790771\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  77%|#033[34m███████▋  #033[0m| 70/91 [10:11<03:03,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.2268558740615845\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.1306899785995483\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.146490216255188\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]#015Training Epoch3:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.2668036222457886\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 74/91 [10:46<02:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 2.214879035949707\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 2.371999740600586\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 2.319223165512085\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.78s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 2.040783166885376\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  86%|#033[34m████████▌ #033[0m| 78/91 [11:21<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 2.026524066925049\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:44,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.5212340354919434\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.6636888980865479\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]#015Training Epoch3:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.970418930053711\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  90%|#033[34m█████████ #033[0m| 82/91 [11:56<01:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.6158713102340698\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 2.052337408065796\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.371846318244934\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.3301280736923218\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.0749120712280273\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.8200328350067139\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.4617525339126587\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 2.1163747310638428\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]#015Training Epoch3:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.4684275388717651\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 6 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]#015evaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.91s/it]#015evaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.91s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.91s/it]#015evaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.91s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.83s/it]#015evaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.83s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:11,  3.79s/it]#015evaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:11,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:11,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:11,  3.79s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<00:59,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:33<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.73s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.73s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:40,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.73s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.73s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:48<00:37,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]#015evaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.74s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]#015evaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.71s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(4.4275, device='cuda:0') eval_epoch_loss=tensor(1.4878, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=4.1462, train_epoch_loss=1.4222, epcoh time 795.0857972630001s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   1%|#033[34m          #033[0m| 1/91 [00:08<12:21,  8.24s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.9134001135826111\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   1%|#033[34m          #033[0m| 1/91 [00:08<12:21,  8.24s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   1%|#033[34m          #033[0m| 1/91 [00:08<12:21,  8.24s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   1%|#033[34m          #033[0m| 1/91 [00:08<12:22,  8.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.2433258295059204\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   2%|#033[34m▏         #033[0m| 2/91 [00:16<12:38,  8.52s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.5920709371566772\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:37,  8.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:37,  8.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:37,  8.61s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   3%|#033[34m▎         #033[0m| 3/91 [00:25<12:37,  8.61s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.4973055124282837\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:33,  8.66s/it]#015Training Epoch4:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:33,  8.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:33,  8.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   4%|#033[34m▍         #033[0m| 4/91 [00:34<12:33,  8.66s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.0157779455184937\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]#015Training Epoch4:   5%|#033[34m▌         #033[0m| 5/91 [00:43<12:27,  8.69s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.0624302625656128\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 6/91 [00:51<12:19,  8.70s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8194910287857056\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:11,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:11,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:11,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   8%|#033[34m▊         #033[0m| 7/91 [01:00<12:11,  8.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.8960541486740112\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   9%|#033[34m▉         #033[0m| 8/91 [01:09<12:03,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.3652880191802979\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  10%|#033[34m▉         #033[0m| 9/91 [01:18<11:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.3983612060546875\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]#015Training Epoch4:  11%|#033[34m█         #033[0m| 10/91 [01:26<11:46,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.719153642654419\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  12%|#033[34m█▏        #033[0m| 11/91 [01:35<11:37,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.7627495527267456\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  13%|#033[34m█▎        #033[0m| 12/91 [01:44<11:32,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.6560907363891602\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:22,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:22,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:22,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  14%|#033[34m█▍        #033[0m| 13/91 [01:53<11:22,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.2224335670471191\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  15%|#033[34m█▌        #033[0m| 14/91 [02:01<11:13,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.6694447994232178\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  16%|#033[34m█▋        #033[0m| 15/91 [02:10<11:04,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.6360597610473633\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  18%|#033[34m█▊        #033[0m| 16/91 [02:19<10:55,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.0734014511108398\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]#015Training Epoch4:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 17/91 [02:28<10:46,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2300714254379272\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  20%|#033[34m█▉        #033[0m| 18/91 [02:36<10:38,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.578943133354187\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  21%|#033[34m██        #033[0m| 19/91 [02:45<10:28,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.3452515602111816\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:20,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:20,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:20,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 20/91 [02:54<10:20,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  23%|#033[34m██▎       #033[0m| 21/91 [03:02<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.4202746152877808\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  23%|#033[34m██▎       #033[0m| 21/91 [03:02<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  23%|#033[34m██▎       #033[0m| 21/91 [03:02<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  23%|#033[34m██▎       #033[0m| 21/91 [03:02<10:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.4467123746871948\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  24%|#033[34m██▍       #033[0m| 22/91 [03:11<10:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.4031668901443481\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  25%|#033[34m██▌       #033[0m| 23/91 [03:20<09:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.4122332334518433\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  26%|#033[34m██▋       #033[0m| 24/91 [03:29<09:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.8559772968292236\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  27%|#033[34m██▋       #033[0m| 25/91 [03:37<09:36,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.4094796180725098\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  29%|#033[34m██▊       #033[0m| 26/91 [03:46<09:27,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.5451372861862183\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 27/91 [03:55<09:18,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.0196183919906616\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  31%|#033[34m███       #033[0m| 28/91 [04:04<09:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.8916499614715576\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  32%|#033[34m███▏      #033[0m| 29/91 [04:12<09:01,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:51,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:51,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:51,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.5120409727096558\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 30/91 [04:21<08:52,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.7709588408470154\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  34%|#033[34m███▍      #033[0m| 31/91 [04:30<08:43,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  35%|#033[34m███▌      #033[0m| 32/91 [04:38<08:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.4339549541473389\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  35%|#033[34m███▌      #033[0m| 32/91 [04:38<08:34,  8.73s/it]#015Training Epoch4:  35%|#033[34m███▌      #033[0m| 32/91 [04:38<08:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  35%|#033[34m███▌      #033[0m| 32/91 [04:38<08:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.8173688054084778\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  36%|#033[34m███▋      #033[0m| 33/91 [04:47<08:26,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.448223352432251\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]#015Training Epoch4:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 34/91 [04:56<08:17,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.3603333234786987\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  38%|#033[34m███▊      #033[0m| 35/91 [05:05<08:09,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  40%|#033[34m███▉      #033[0m| 36/91 [05:13<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.862555980682373\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  40%|#033[34m███▉      #033[0m| 36/91 [05:13<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  40%|#033[34m███▉      #033[0m| 36/91 [05:13<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  40%|#033[34m███▉      #033[0m| 36/91 [05:13<08:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.0896750688552856\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 37/91 [05:22<07:51,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.8316080570220947\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  42%|#033[34m████▏     #033[0m| 38/91 [05:31<07:42,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.73s/it]#015Training Epoch4:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.7919741868972778\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  43%|#033[34m████▎     #033[0m| 39/91 [05:40<07:33,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.9855568408966064\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 40/91 [05:48<07:24,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.1779706478118896\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.72s/it]#015Training Epoch4:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  45%|#033[34m████▌     #033[0m| 41/91 [05:57<07:16,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 2.162771224975586\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.72s/it]#015Training Epoch4:  46%|#033[34m████▌     #033[0m| 42/91 [06:06<07:07,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  47%|#033[34m████▋     #033[0m| 43/91 [06:14<06:58,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  47%|#033[34m████▋     #033[0m| 43/91 [06:14<06:58,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.3117213249206543\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  47%|#033[34m████▋     #033[0m| 43/91 [06:14<06:58,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  47%|#033[34m████▋     #033[0m| 43/91 [06:15<06:58,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.415308952331543\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 44/91 [06:23<06:50,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.415218710899353\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  49%|#033[34m████▉     #033[0m| 45/91 [06:32<06:43,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.5306415557861328\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  51%|#033[34m█████     #033[0m| 46/91 [06:41<06:33,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.5975706577301025\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 47/91 [06:50<06:24,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.675002098083496\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  53%|#033[34m█████▎    #033[0m| 48/91 [06:58<06:15,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.7104274034500122\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  54%|#033[34m█████▍    #033[0m| 49/91 [07:07<06:07,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.5187803506851196\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  55%|#033[34m█████▍    #033[0m| 50/91 [07:16<05:58,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.3137714862823486\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 51/91 [07:24<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 51/91 [07:24<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 51/91 [07:24<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 51/91 [07:24<05:49,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.271860122680664\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  57%|#033[34m█████▋    #033[0m| 52/91 [07:33<05:40,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.0728247165679932\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  58%|#033[34m█████▊    #033[0m| 53/91 [07:42<05:31,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.8852266669273376\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 54/91 [07:51<05:23,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  60%|#033[34m██████    #033[0m| 55/91 [07:59<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  60%|#033[34m██████    #033[0m| 55/91 [07:59<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.7553161382675171\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  60%|#033[34m██████    #033[0m| 55/91 [07:59<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  60%|#033[34m██████    #033[0m| 55/91 [07:59<05:14,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.993465006351471\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  62%|#033[34m██████▏   #033[0m| 56/91 [08:08<05:06,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.0640376806259155\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 57/91 [08:17<04:57,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 1.5427987575531006\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  64%|#033[34m██████▎   #033[0m| 58/91 [08:26<04:48,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 1.340445637702942\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  65%|#033[34m██████▍   #033[0m| 59/91 [08:34<04:39,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.561285138130188\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  66%|#033[34m██████▌   #033[0m| 60/91 [08:43<04:30,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.9599452018737793\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 61/91 [08:52<04:21,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.630668044090271\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  68%|#033[34m██████▊   #033[0m| 62/91 [09:01<04:13,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.6878615617752075\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  69%|#033[34m██████▉   #033[0m| 63/91 [09:09<04:04,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.8098780512809753\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 64/91 [09:18<03:55,  8.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.9055048823356628\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  71%|#033[34m███████▏  #033[0m| 65/91 [09:27<03:46,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  73%|#033[34m███████▎  #033[0m| 66/91 [09:35<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  73%|#033[34m███████▎  #033[0m| 66/91 [09:35<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.611382246017456\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  73%|#033[34m███████▎  #033[0m| 66/91 [09:35<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  73%|#033[34m███████▎  #033[0m| 66/91 [09:35<03:38,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.382426142692566\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]#015Training Epoch4:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  74%|#033[34m███████▎  #033[0m| 67/91 [09:44<03:30,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 1.7557958364486694\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  75%|#033[34m███████▍  #033[0m| 68/91 [09:53<03:21,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.5541253089904785\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  76%|#033[34m███████▌  #033[0m| 69/91 [10:02<03:12,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  77%|#033[34m███████▋  #033[0m| 70/91 [10:10<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  77%|#033[34m███████▋  #033[0m| 70/91 [10:10<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 1.2809557914733887\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  77%|#033[34m███████▋  #033[0m| 70/91 [10:10<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  77%|#033[34m███████▋  #033[0m| 70/91 [10:10<03:03,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.2044404745101929\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 71/91 [10:19<02:54,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.102299690246582\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  79%|#033[34m███████▉  #033[0m| 72/91 [10:28<02:45,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.087670087814331\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  80%|#033[34m████████  #033[0m| 73/91 [10:37<02:37,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.225508689880371\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 74/91 [10:45<02:28,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 2.1916468143463135\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  82%|#033[34m████████▏ #033[0m| 75/91 [10:54<02:19,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 2.3447930812835693\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  84%|#033[34m████████▎ #033[0m| 76/91 [11:03<02:11,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 2.2946178913116455\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  85%|#033[34m████████▍ #033[0m| 77/91 [11:12<02:02,  8.77s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 78/91 [11:20<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 78/91 [11:20<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 2.0096981525421143\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 78/91 [11:20<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  86%|#033[34m████████▌ #033[0m| 78/91 [11:20<01:53,  8.76s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 2.005418539047241\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  87%|#033[34m████████▋ #033[0m| 79/91 [11:29<01:45,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]#015Training Epoch4:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.491626262664795\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  88%|#033[34m████████▊ #033[0m| 80/91 [11:38<01:36,  8.75s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.6356403827667236\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]#015Training Epoch4:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 81/91 [11:47<01:27,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.9338616132736206\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.74s/it]#015Training Epoch4:  90%|#033[34m█████████ #033[0m| 82/91 [11:55<01:18,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 1.597536325454712\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  91%|#033[34m█████████ #033[0m| 83/91 [12:04<01:09,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 2.0280323028564453\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  92%|#033[34m█████████▏#033[0m| 84/91 [12:13<01:01,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.3132942914962769\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 85/91 [12:22<00:52,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]#015Training Epoch4:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.2890244722366333\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  95%|#033[34m█████████▍#033[0m| 86/91 [12:30<00:43,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.0319980382919312\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  96%|#033[34m█████████▌#033[0m| 87/91 [12:39<00:34,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.7967557907104492\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  97%|#033[34m█████████▋#033[0m| 88/91 [12:48<00:26,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.4186387062072754\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  98%|#033[34m█████████▊#033[0m| 89/91 [12:57<00:17,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 2.0845983028411865\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:  99%|#033[34m█████████▉#033[0m| 90/91 [13:05<00:08,  8.74s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 1.4458928108215332\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 91/91 [13:14<00:00,  8.73s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 6 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/23 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]#015evaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.22s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 1/23 [00:04<01:32,  4.23s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 2/23 [00:07<01:22,  3.93s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 3/23 [00:11<01:16,  3.84s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 4/23 [00:15<01:12,  3.80s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 5/23 [00:19<01:07,  3.77s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]#015evaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 6/23 [00:22<01:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 7/23 [00:26<01:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 8/23 [00:30<00:56,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 9/23 [00:34<00:52,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 10/23 [00:37<00:48,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 11/23 [00:41<00:44,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:41,  3.74s/it]#015evaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:41,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:41,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 12/23 [00:45<00:41,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:49<00:37,  3.74s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:49<00:37,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:49<00:37,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 13/23 [00:49<00:37,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.74s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 14/23 [00:52<00:33,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.74s/it]#015evaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 15/23 [00:56<00:29,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]#015evaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 16/23 [01:00<00:26,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]#015evaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 17/23 [01:03<00:22,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]#015evaluating Epoch:  78%|#033[32m███████▊  #033[0m| 18/23 [01:07<00:18,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]#015evaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 19/23 [01:11<00:14,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 20/23 [01:15<00:11,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.74s/it]#015evaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.74s/it]#015evaluating Epoch:  91%|#033[32m█████████▏#033[0m| 21/23 [01:18<00:07,  3.74s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]#015evaluating Epoch:  96%|#033[32m█████████▌#033[0m| 22/23 [01:22<00:03,  3.73s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.72s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.72s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.72s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 23/23 [01:26<00:00,  3.75s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(4.4615, device='cuda:0') eval_epoch_loss=tensor(1.4955, device='cuda:0')\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=3.9934, train_epoch_loss=1.3846, epcoh time 794.7513517499997s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 4.4570536613464355\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.48957097530365\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 4.441117286682129\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.4876315593719482\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 795.5953372166\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 2.2160367938001855\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.42it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2024-05-28 23:29:04,995 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-28 23:29:04,995 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-28 23:29:04,995 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-28 23:29:10 Uploading - Uploading generated training model\n",
      "2024-05-28 23:29:53 Completed - Training job completed\n",
      "Training seconds: 4933\n",
      "Billable seconds: 4933\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-3-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", chat_dataset=\"False\",epoch=\"5\", max_input_length=\"1024\",enable_fsdp=True)\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3889d9-1567-41ad-9375-fb738db629fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Studio Kernel Dying issue:  If your studio kernel dies and you lose reference to the estimator object, please see section [6. Studio Kernel Dead/Creating JumpStart Model from the training Job](#6.-Studio-Kernel-Dead/Creating-JumpStart-Model-from-the-training-Job) on how to deploy endpoint using the training job name and the model id. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-997\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-994\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhich famous musicians played a Fender Stratocaster?\\n\\n### Input:\\nA–E\\nBillie Joe Armstrong (born 1972), lead singer and guitarist of Green Day, uses a heavily stickered Fernandes Stratocaster copy nicknamed \"Blue\". Armstrong modified this guitar with a Bill Lawrence humbucking pickup on the bridge position. After sustaining damage from mud during their performance in Woodstock '94, the bridge pickup was replaced with a Seymour Duncan JB. Blue was used on the recording of every Green Day album until Warning, and during live performances of Green Day's early work, such as their songs from Dookie. Armstrong also used a Fender Stratocaster from the Fender Custom Shop while recording Nimrod.\\nRandy Bachman (born 1943), a founding member of both The Guess Who and Bachman–Turner Overdrive (BTO) who recently fronted the project \"Randy Bachman's Jazz Thing.\" After a visit to a chiropractor, Bachman was persuaded to switch from a Gibson Les Paul to a lighter Stratocaster. He modified the pickups on his first Strat, putting a Gibson pickup at the neck and a Telecaster pickup at the bridge, while leaving the Stratocaster pickup in the middle. Randy favored Stratocasters and custom Strat-style guitars throughout his years with BTO. Though his bands are mostly known for their simplistic rock-radio anthems, Bachman's soloing often revealed complex melodies and jazz-inflected phrasing. Among his Stratocasters used are a '63 standard and a '71 four-bolt hardtail. He has listed guitar influences as varied as Lenny Breau, Leslie West, Wes Montgomery and Hank Marvin.\\n\\nJeff Beck in Amsterdam, 1979.\\nJeff Beck (born 1944-2023) - a Grammy award-winning rock guitarist, Beck was known for playing for various bands such as the Yardbirds and his own group The Jeff Beck Group. Beck primarily played a Stratocaster and also has a signature Strat. He was noted for his innovative use of the Stratocaster's vibrato system. Up to 1975 Beck had been, primarily, a Les Paul player. In an interview with Jas Obrecht about switching to the Stratocaster, Beck stated, \"With a Les Paul you just wind up sounding like someone else. With the Strat I finally sound like me.\"\\nAdrian Belew (born 1949), is an American guitarist, singer, songwriter, multi-instrumentalist and record producer. He is perhaps best known for his work as a member of the progressive rock group King Crimson. He has also worked extensively as a session and touring musician, most famously with Talking Heads, David Bowie, Frank Zappa, and Nine Inch Nails. During much of his career, Belew made extensive use of a weathered-looking Stratocaster, later memorialized in song as \"The Battered Strat.\" This guitar was relic'ed by Seymour Duncan.\\n\\nRitchie Blackmore in 1977.\\nRitchie Blackmore (born 1945), a founding member of both Deep Purple and Rainbow, and currently a member of the band Blackmore's Night. After starting his career using various Höfner and Gibson guitars, Blackmore switched to a Stratocaster in the late 1960s after seeing Jimi Hendrix perform with one. Blackmore's Stratocasters are modified; the middle pickup is lowered and not used (sometimes disconnected completely) and his Stratocaster fingerboards are all scalloped from the 10th fret up. Through the early/mid 1970s Blackmore was notorious for onstage abuse of his guitars, sometimes destroying them completely. By the late 1970s the guitarist had found a Stratocaster model he was content with and it remained his main stage and studio guitar up until it had to be refretted.\\nTommy Bolin (1951-1976), a versatile guitarist who is noted for his influence in genres ranging from acoustic blues to hard rock and jazz fusion. He was the lead guitarist for Zephyr, James Gang and Deep Purple. He also had a successful solo career, and collaborated with artists like Billy Cobham, Alphonse Mouzon and The Good Rats. Bolin played by ear and was known for his improvisational skill. His primary guitar was a stock 1963 Stratocaster.\\n\\nJoe Bonamassa in 2016.\\nJoe Bonamassa (born 1977), a blues rock guitarist, has used Stratocasters throughout his career. When he was 12 years old, Bonamassa played a crimson 1972 Fender Stratocaster. Bonamassa is known for his extensive collection of vintage amplifiers and guitars. In 2018, Bonamassa has said that he has more than 1000 guitars, a large fraction of which are Fender Stratocasters.\\nBill Carson (1926–2007), a country and western guitarist credited by Fender as \"the man for whom the Stratocaster was designed.\"\\nEric Clapton (born 1945), an English rock guitarist, originally played Gibson guitars early in his career. While he was still a member of Cream, Clapton bought his first Stratocaster, Brownie, in 1969, which was later used on \"Layla\". Blackie, a composite of three different guitars, went into service in 1970 and was regularly played until its retirement in 1985. It was sold at charity auction for $959,500 in 2004. In 1988, Fender introduced the Eric Clapton Stratocaster, the first model in their Signature series. Clapton has been a long-standing client of the Fender Custom Shop.[citation needed]\\nKurt Cobain (1967–1994), lead singer and guitarist of grunge band Nirvana, used Fender Stratocasters throughout his career, using the guitar in the music video for \"Smells Like Teen Spirit\" and in the band's famous performance at the 1992 Reading Festival. Cobain's most well-known Stratocaster has a sticker on the body with the text \"VANDALISM: BEAUTIFUL AS A ROCK IN A COP'S FACE.\"\\n\\nEric Clapton in a Switzerland concert on June 19, 1977.\\nRy Cooder (born 1947), a guitarist, singer and composer who is well known for his interest in American folk music, his collaborations with other notable musicians, and his work on many film soundtracks. Cooder's bottleneck slide guitar playing, heard on such works as the soundtrack to the 1984 film Paris, Texas, influenced other guitarists such as Bonnie Raitt and Chris Rea and contributed to the popularity of the Stratocaster as a slide guitar. He uses a '60s Stratocaster for such playing.\\nRobert Cray (born 1953), a long-time blues guitarist and singer, Cray plays a '64 Strat and had his own Signature model made in 1990. The signature model, manufactured by the Fender Custom Shop, combines aspects of Cray's '59 Strat and the '64, omits the standard Stratocaster whammy bar, and includes custom pickups.\\nDick Dale (1937–2019), considered a pioneer of surf rock, was one of the first owners of a Stratocaster; his was given to him personally by Leo Fender in 1955. He has been revolutionary in experimenting with the sound of the guitar by using heavy reverb and a unique fast-picking style as heard on \"Misirlou\".\\nThe Edge (born 1961), lead guitarist of U2, known for his percussive, melodic playing and use of delay, has used the Stratocaster as one of his main guitars throughout his career.\\nF–J\\n\\nJohn Frusciante in 2006.\\nJohn Frusciante (born 1970), the current guitarist of Red Hot Chili Peppers, Frusciante used many pre-70s Strats, with the most notable being his worn 1962 Stratocaster. Frusciante used Stratocasters in every Red Hot Chili Peppers album he was involved with, including Mother's Milk, Blood Sugar Sex Magik,and Californication.\\n\\nRory Gallagher in 1987\\nRory Gallagher (1948–1995), an Irish blues rock guitarist, often credited as one of the most influential rock and blues guitarists of all time. Gallagher is well known for his worn 1961 sunburst Stratocaster. He described his battered Stratocaster as \"a part of my psychic makeup\". When asked about its importance, Gallagher said, \"B.B. King has owned over 100 Lucilles, but I only own one Strat, and it hasn't got a name.\" Gallagher's Stratocaster has also been reproduced by the Fender Custom shop, to the exact specs of the original one.\\nLowell George (1945–1979), primary guitarist and singer of Little Feat. Lowell was proficient on slide guitar employing his trademark tone which he achieved through use of compression and open tunings helping to define his soulful sound as well as giving him the means to play his extended melodic lines. Additionally, he used to swap the bridge pickups of his Stratocasters for Telecaster bridge pickups.\\n\\nDavid Gilmour in 2006.\\nDavid Gilmour (born 1946), as a solo artist and guitar player for Pink Floyd, Gilmour is credited for his unique, blues-based compositional approach and expressive soloing. Author Tony Bacon stated \"his solo on 'Comfortably Numb' remains for many a definitive Strat moment.\" Gilmour's guitar of choice is a custom modified Fender Stratocaster. He is the owner of Strat #0001, which was manufactured in 1954 but was not the first Stratocaster made since Fender does not use sequential serial numbers. Gilmour is considered to be one of the more influential Stratocaster players since the instrument's invention. David's signature black Stratocaster, used frequently in 1970s concerts and on the blockbuster albums The Dark Side of the Moon, Wish You Were Here, Animals and The Wall, is featured in a recent book by his long-time guitar tech Phil Taylor, titled Pink Floyd, The Black Strat—A History of David Gilmour's Black Stratocaster. The \"Black Strat\" was retired in the 1980s in favour of a Candy Apple Red American Vintage Stratocaster fitted with EMG noiseless single-coil pickups as seen on the Delicate Sound of Thunder and Pulse tours. The Black Strat was briefly used on the documentary Classic Albums: Dark Side of the Moon before being put on display at the Hard Rock Cafe in Miami, Florida. It was finally brought out of retirement by David in 2005 and fitted with a '83 Fender Stratocaster neck for the Pink Floyd reunion at the Live 8 concert. David subsequently used it again for his \"On An Island\" album and tour in 2006 and when he played \"Comfortably Numb\" with Roger Waters on his tour of \"The Wall\" on May 12, 2011, in London and also played most of the leads on the final Pink Floyd album The Endless River and his 2015 solo album Rattle That Lock and its tour.\\n\\nBuddy Guy in 1992.\\nBuddy Guy (born 1936), an American blues guitarist and singer, Guy is well known for playing the Stratocaster throughout his long career. He is also known for his wild showmanship; Jimi Hendrix and Stevie Ray Vaughan both pointed to Guy as an influence on both their playing and their stage shows. Fender has issued several different variations of a Buddy Guy Signature Stratocaster since the early 1990s; the guitars generally have gold Lace Sensor pickups and modified circuitry.\\nAlbert Hammond Jr. (born 1980), guitarist for The Strokes, uses a white Fender Stratocaster as his main guitar for recording and live use. Hammond bought the guitar in 1999 for $400, and used it to record albums such as Is This It and Room on Fire. In 2018, Fender released a signature model of Hammond's guitar, featuring a larger headstock and a modified pickup wiring scheme.\\nGeorge Harrison (1943–2001), lead guitarist for the Beatles. Harrison and John Lennon obtained matching Sonic Blue Stratocasters in 1965. Unlike Lennon, Harrison employed his Stratocaster more often, using it as his main guitar during the recording sessions for Rubber Soul, Sgt. Pepper's Lonely Hearts Club Band, and the White Album. In 1967, Harrison hand-painted his Stratocaster with a psychedelic paint job, using Day-Glo paint on the body and his wife Pattie Boyd's nail polish on the headstock. The guitar's nickname, \"Rocky\", is painted on the headstock. Harrison can be seen playing Rocky in the Magical Mystery Tour film as well as The Concert for Bangla Desh.\\n\\nJimi Hendrix in 1967.\\nJimi Hendrix (1942–1970), known for developing blues in a modern context, Hendrix's main stage guitar through most of his short career was a Fender Stratocaster. Although Hendrix played left-handed, he played a conventional right-handed Stratocaster flipped upside down, because he preferred to have the control knobs in the top position. Hendrix was responsible for a large increase in the Stratocaster's popularity during his career. In reference to his famed on-stage Stratocaster burning on the Monterey Pop Festival, Hendrix is quoted as saying, \"The time I burned my guitar it was like a sacrifice. You sacrifice the things you love. I love my guitar.\" In 1990, the white Stratocaster used by Hendrix at the 1969 Woodstock Festival sold in a Sotheby's auction for $270,000, a record price at the time. In 1997 Fender produced a limited edition Hendrix tribute model Stratocaster.\\nBuddy Holly (1936–1959), identified as \"the first Strat hero.\" A statue of Holly in his home town of Lubbock, Texas, portrays him playing his Stratocaster, and the guitar is also engraved on his tombstone. Although the initial release of the Stratocaster came in 1954, the guitar did not begin to achieve popularity until Holly appeared on The Ed Sullivan Show in 1957 playing a maple-neck Strat. Holly was also pictured on the cover of The Crickets' 1957 album The \"Chirping\" Crickets with a sunburst Stratocaster, inspiring The Shadows' Hank Marvin to adopt the guitar.\\nErnie Isley (born 1952), member of the American musical ensemble The Isley Brothers has developed three custom Zeal Stratocasters from Fender Custom Shop, using his personal design.\\nEric Johnson (born 1954), a Grammy Award-winning guitarist from Austin, Texas, Johnson has played Stratocasters regularly during his career and has played many different types of music. He has participated in developing an Eric Johnson signature Stratocaster model with Fender, which can be bought with both maple and rosewood necks.\\nK–P\\n\\nMark Knopfler in a Hamburg concert on May 28, 2006\\n\\nRocky Kramer performing live in 2018\\n\\nYngwie Malmsteen in Barcelona in 2008 concert\\nEd King (1949–2018) is known for his work with the southern rock band Lynyrd Skynyrd from 1972 to 1975. He used a 1959 model with a black refinish and tortoise pickguard for most recordings and live performances at that time, and also a 1973 model which he used when writing the hit \"Sweet Home Alabama\".\\nMark Knopfler (born 1949), known for his work with British rock band Dire Straits. Knopfler is known for his very particular and unique fingerstyle playing. The song \"Sultans of Swing\", from Dire Straits' debut album in 1978, was a huge hit that showed the characteristic tone and technique displayed on Knopfler's red Stratocaster. He used the Fender Stratocaster throughout his entire career, as a member of Dire Straits and his solo career. Fender now produces his Signature Stratocaster.\\nGreg Koch (born 1966), known for his incendiary guitar work. Koch was a Fender clinician and ambassador. He played the Stratocaster for many years and even recorded an album called Strat's Got Your Tongue. He is known for his love of Fender guitars.\\nRocky Kramer (born 1990) is known for being a Norwegian \"Master Guitarist,\" now living in the United States. Kramer has been described as a guitar virtuoso \"setting fire to the atmosphere with incandescent licks,\" as well as \"ne of the strongest and most poignant guitarists since Hendrix.\" Kramer plays and endorses Fender Stratocaster guitars.\\nBruce Kulick (born 1953), long-time member and lead guitarist of Kiss and Grand Funk Railroad. Kulick stated on his personal website that he used a Fender Power Stratocaster, a model with a humbucking pickup in place of the single-coil bridge pickup, to add a harmony solo line to his song, \"What Love's All About.\" Kulick used a 1989 yellow Fender Strat Plus, during the recording of the 1992 Kiss Revenge album, including for the hit single, \"God Gave Rock 'n Roll to You II.\" Revenge reached the Top 20 in several countries.\\nMichael Landau (born 1958), friend of Steve Lukather and prolific session guitarist of the 1980s, has used many Stratocasters in his career and is working with Fender as of 2016 to create a Michael Landau Signature Stratocaster.\\nJohn Lennon (1940–1980), the Beatles' rhythm guitarist, acquired matching Stratocasters with bandmate George Harrison during the 1965 sessions for Help!. However, Lennon rarely used his Stratocaster, which was notably played on \"Nowhere Man\" and during the Sgt. Pepper sessions. A different Strat was used on the Imagine album. John Lennon acquired a candy apple red \"Strat\" with 22 carat gold electroplated brass hardware around 1980. A photo of him playing this guitar in bed one morning in late 1980, shortly before his death, was used an inner sleeve of the album The John Lennon Collection.\\nAlex Lifeson (born 1953), the guitarist for Rush since 1968, first recorded with a black Stratocaster on the Rush 1977 album A Farewell to Kings. In 1979, he modified the '77 Strat with a '57 classic humbucker, a Floyd Rose tremolo unit (first ever made), a Gibson toggle switch on the lower bout, and rewired with master volume/tone. He used that same guitar for the leads and direct recording for 1979's \"Permanent Waves.\" In late 1980, Alex Lifeson acquired two more Strats in red and white, modifying them exactly the same as the former.\\nYngwie Malmsteen (born 1963), known for his work in the neo-classical metal genre. Influenced by an array of musicians, Malmsteen is regarded as highly influential for his use of heavy classical-style chord progressions, interesting phrases and arpeggio sweeps. He is known for playing Stratocasters with scalloped fretboards.\\nHank Marvin (born 1941), the lead guitarist of The Shadows, Marvin is reputed to be the owner of the first Fender Stratocaster in the UK (given to him by Cliff Richard). The guitar was finished in a shade of Fiesta Red, sometimes referred to as 'Salmon Pink'. This guitar, with its tremolo arm, contributed to the Shadows' distinctive sound. Guitarists such as David Gilmour and Mark Knopfler credit Marvin and The Shadows, who had \"the first Strat that came to England\", with influencing their own decisions to buy Stratocasters.\\nJohn Mayer (born 1977), a Grammy Award-winning singer/songwriter, has played Stratocasters throughout his career and has had a Fender Artist Series Stratocaster made in both standard and limited edition form. Mayer's use of the Stratocaster in a wide range of musical genres is noted as a testament to the guitar's versatility. After tensions with Fender, he partnered with PRS Guitars to develop the PRS Silver Sky, a guitar heavily based on the Fender Stratocaster.\\nMike Oldfield (born 1953), a British guitarist who plays a wide range of guitars and instruments. His \"Salmon-pink\" strat, bought at the time of his hit Moonlight Shadow, is his favorite guitar.\\nQ–Z\\n\\nStevie Ray Vaughan performing in 1983\\nTrevor Rabin (born 1954), a South African (now has American citizenship) rock guitarist and film score composer. Most well known for his time with Yes (1982-1995; 2015–present), Rabin owns and plays several Stratocasters, and considers it his go-to instrument.\\nBonnie Raitt (born 1949), an American blues/R&amp;B guitarist, singer, and songwriter, plays a 1965 Stratocaster nicknamed brownie, a 1963 sunburst Strat that used to be owned by Robin Trower as well as her signature Strat.\\nRobbie Robertson (born 1943), guitarist and principal songwriter for The Band. Robertson's main guitar choice was a Stratocaster, despite using a Telecaster early in his career. For The Last Waltz Robertson had a Stratocaster bronzed especially for his use in the film. More recently Robertson made a very rare live appearance at Eric Clapton's 2007 Crossroads Guitar Festival using a Stratocaster.\\nNile Rodgers (born 1952), an American musician known for his contributions with Chic and unique playing style that makes extensive use of the chop chord, has a 1960 Stratocaster affectionately dubbed as \"The Hitmaker\" for its presence on many hit singles.\\nKenny Wayne Shepherd (born 1977 Kenneth Wayne Brobst), lead guitarist and lead/backup vocalist for The Kenny Wayne Shepherd Band. Born in Shreveport, Louisiana, Kenny started his playing career at age 16, while attending Caddo Magnet High School, and has performed internationally with many of the great blues legends.\\nRichard Thompson (born 1949), an English musician best known for his finger-style guitar playing and songwriting, was a founding member of Fairport Convention before becoming a solo artist. For many years Thompson played a '59 Sunburst Stratocaster, with a maple '55 neck. That guitar is currently unserviceable and Thompson now uses a '64 sunburst Stratocaster with a rosewood fingerboard.\\nPete Townshend (born 1945), the guitarist for The Who, used a Fender Stratocaster during the recording sessions for \"I Can See for Miles\" and The Who Sell Out. During the Monterey Pop Festival in 1967, Townshend smashed a Stratocaster after the Who's set, which was immediately followed by the Jimi Hendrix Experience's performance where Hendrix also destroys a Stratocaster. Townshend has exclusively used a modified version of the Fender Eric Clapton's Signature Stratocaster since 1989.\\nRobin Trower (born 1945), a British rock guitarist known for his work in the band Procol Harum and his successful solo career, has his own Signature Stratocaster made by Fender. \"The sight of him onstage with his signature Stratocaster is as characteristic to his fans as his classic songs.\"\\n\\nIke Turner in 1997.\\nIke Turner (1931-2007), an American guitarist, musician, songwriter and record producer known for his work with the Ike &amp; Tina Turner Revue and the Kings of Rhythm. Turner was an early adopter of the Stratocaster, buying one on its release in 1954. Unaware that the guitar's tremolo arm could be used to subtle effect, Turner used it to play screaming, swooping and diving solos. Turner explained his technique by saying: \"I thought it was to make the guitar scream—people got so excited when I used that thing.\" Turner was also known to play Telecasters and Jaguars. In 2004 Fender Custom Shop produced an Ike Turner Signature Stratocaster, limited to 100.\\nRitchie Valens (1941–1959), a pioneer of rock and roll mostly famous for his Latin Rock song \"La Bamba\", played with a sunburst Strat.\\nEddie Van Halen (1955–2020), guitarist of hard rock band Van Halen, is notable for his \"Frankenstrat\", a crudely modified Stratocaster copy with the single-coil bridge pickup replaced with a PAF humbucker. This modification made the Frankenstrat one of the earliest Superstrats, which are guitars with a Stratocaster body but different features. A replica of his guitar is located in the National Museum of American History part of the Smithsonian Institution in Washington, D.C.\\nStevie Ray Vaughan (1954–1990), known for his Texas blues style guitar playing, was an American blues guitarist who is considered highly influential for his driving guitar style soaked with rhythmic style playing. Vaughan was noted for playing a Stratocaster equipped with a left-handed vibrato system. He was also known for tuning his guitar down a half-step and using heavy .13 gauge strings.\\nCory Wong (born 1985/86), rhythm guitarist for Vulfpeck, and known for his clean funk 'lead rhythm' guitar style, used Stratocasters throughout his career. In 2021, Fender released the Cory Wong Signature Stratocaster. A unique feature of this model is the 'fourth position panic button', which uses one of the tone controls to \"lock\" the guitar in fourth position, and disables the selector switch.\\nRonnie Wood (born 1947), guitarist playing with the Rolling Stones since 1975. Wood's main stage instruments are 1954 and 1955 Sunburst Strats. He also uses a 52 Telecaster.\\nJames \"J.Y.\" Young (born 1949), guitarist of multi-platinum rock band Styx, has been using a Stratocaster as his principal guitar almost exclusively since 1967.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>There are few guitars as iconic as the Fender Stratocaster.  This guitar was originally developed in the early 1950's and has taken the world by storm.  To name a few very notable Stratocaster players the list would have to include Jimi Hendrix, Stevie Ray Vaughn, Eric Clapton, John Mayer, Yngwie Malmsteen, Buddy Guy and John Frusciante.  As you can see this list spans many genres and speaks to the flexibility of this instrument.</td>\n",
       "      <td>The famous musicians who played a Fender Stratocaster are:\\n\\n1. Billie Joe Armstrong (Green Day)\\n2. Randy Bachman (The Guess Who, Bachman–Turner Overdrive)\\n3. Jeff Beck (The Yardbirds, The Jeff Beck Group)\\n4. Adrian Belew (King Crimson, Talking Heads, David Bowie, Frank Zappa, Nine Inch Nails)\\n5. Ritchie Blackmore (Deep Purple, Rainbow, Blackmore's Night)\\n6. Tommy Bol</td>\n",
       "      <td>The Fender Stratocaster is a versatile guitar that has been used by many famous musicians across a wide range of musical genres.  Some notable musicians who have used the Stratocaster include Jimi Hendrix, Eric Clapton, David Gilmour, Mark Knopfler, Stevie Ray Vaughan, Eddie Van Halen, and Ronnie Wood.  The Stratocaster has been used in many different musical genres, including rock, blues, jazz, and country.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease provide details on  Brisbane International tournament with given text as a reference\\n\\n### Input:\\nThe Brisbane International established in 2009 is a professional tennis tournament played on outdoor hardcourts in Brisbane, Queensland in Australia. It is a WTA Premier tournament of the WTA Tour and was part of the ATP World Tour 250 series of the ATP World Tour until 2019, when it was dropped from the ATP Tour.\\n\\nThe tournament is held annually in January at the Queensland Tennis Centre just before the first Grand Slam tournament of the season, the Australian Open (part of the Australian Open Series). It is owned by Tennis Australia.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>1. The Brisbane International, which began in 2009, is an outdoor hardcourt professional tennis competition held in Brisbane, Queensland, Australia.\\n2. It is a WTA Premier competition on the WTA Tour, and up until 2019, it was a part of the ATP World Tour 250 series on the ATP World Tour.\\n3. The competition is conducted each year in January at the Queensland Tennis Center, right before the season's first Grand Slam event, the Australian Open. (part of the Australian Open Series).\\n4. Tennis Australia is the owner.</td>\n",
       "      <td>The Brisbane International is a professional tennis tournament that has been held annually in Brisbane, Queensland, Australia since 2009. The tournament is played on outdoor hardcourts at the Queensland Tennis Centre and is a WTA Premier event on the WTA Tour. Prior to 2019, it was also an ATP World Tour 250 event on the ATP World Tour. The tournament is held in January, just before the Australian Open, and is owned by Tennis Australia. It is part of the Australian Open</td>\n",
       "      <td>The Brisbane International established in 2009 is a professional tennis tournament played on outdoor hardcourts in Brisbane, Queensland in Australia. It is a WTA Premier tournament of the WTA Tour and was part of the ATP World Tour 250 series of the ATP World Tour until 2019, when it was dropped from the ATP Tour.\\n\\nThe tournament is held annually in January at the Queensland Tennis Centre just before the first Grand Slam tournament of the season, the Australian Open (part of the Australian Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat treatments is Iodine-125 used for?\\n\\n### Input:\\nIodine-125 (125I) is a radioisotope of iodine which has uses in biological assays, nuclear medicine imaging and in radiation therapy as brachytherapy to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumors. It is the second longest-lived radioisotope of iodine, after iodine-129.\\nIts half-life is 59.49 days and it decays by electron capture to an excited state of tellurium-125. This state is not the metastable 125mTe, but rather a lower energy state that decays immediately by gamma decay with a maximum energy of 35 keV. Some of the excess energy of the excited 125Te may be internally converted ejected electrons (also at 35 keV), or to x-rays (from electron bremsstrahlung), and also a total of 21 Auger electrons, which are produced at the low energies of 50 to 500 electron volts. Eventually, stable ground state 125Te is produced as the final decay product.\\nIn medical applications, the internal conversion and Auger electrons cause little damage outside the cell which contains the isotope atom. The X-rays and gamma rays are of low enough energy to deliver a higher radiation dose selectively to nearby tissues, in \"permanent\" brachytherapy where the isotope capsules are left in place (125I competes with palladium-103 in such uses)\\nBecause of its relatively long half-life and emission of low-energy photons which can be detected by gamma-counter crystal detectors, 125I is a preferred isotope for tagging antibodies in radioimmunoassay and other gamma-counting procedures involving proteins outside the body. The same properties of the isotope make it useful for brachytherapy, and for certain nuclear medicine scanning procedures, in which it is attached to proteins (albumin or fibrinogen), and where a half-life longer than that provided by 123I is required for diagnostic or lab tests lasting several days.\\nIodine-125 can be used in scanning/imaging the thyroid, but iodine-123 is preferred for this purpose, due to better radiation penetration and shorter half-life (13 hours). 125I is useful for glomerular filtration rate (GFR) testing in the diagnosis or monitoring of patients with kidney disease. Iodine-125 is used therapeutically in brachytherapy treatments of tumors. For radiotherapy ablation of tissues that absorb iodine (such as the thyroid), or that absorb an iodine-containing radiopharmaceutical, the beta-emitter iodine-131 is the preferred isotope.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Iodine-125 is used in biological assays, nuclear medicine imaging and in radiation therapy as brachytherapy to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumors.</td>\n",
       "      <td>Iodine-125 is used to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumors, as part of brachytherapy. It is also used in biological assays, nuclear medicine imaging, and in radiation therapy to deliver a higher radiation dose selectively to nearby tissues. Additionally, it is used in gamma-counting procedures involving proteins outside the body, and in certain nuclear medicine scanning procedures. Furthermore, it is used in glomerular filtration rate (GFR</td>\n",
       "      <td>Iodine-125 is used in biological assays, nuclear medicine imaging and in radiation therapy as brachytherapy to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is radioactive decay?\\n\\n### Input:\\nRadioactive decay (also known as nuclear decay, radioactivity, radioactive disintegration, or nuclear disintegration) is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha decay (α-decay), beta decay (β-decay), and gamma decay (γ-decay), all of which involve emitting one or more particles. The weak force is the mechanism that is responsible for beta decay, while the other two are governed by the electromagnetism and nuclear force. A fourth type of common decay is electron capture, in which an unstable nucleus captures an inner electron from one of the electron shells. The loss of that electron from the shell results in a cascade of electrons dropping down to that lower shell resulting in emission of discrete X-rays from the transitions. A common example is iodine-125 commonly used in medical settings.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Radioactive decay is the process by which an unstable atomic nucleus loses energy by radiation.  It is also known by terms such as nuclear decay, radioactivity, radioactive disintegration, or nuclear disintegration.</td>\n",
       "      <td>Radioactive decay is the process by which an unstable atomic nucleus loses energy by radiation, resulting in the emission of particles such as alpha, beta, and gamma rays. This process occurs when an atom contains unstable nuclei, which are considered radioactive. There are four main types of radioactive decay: alpha decay, beta decay, gamma decay, and electron capture. These types of decay are governed by different forces, including the weak force, electromagnetism, and nuclear force. Radioactive decay is an important concept</td>\n",
       "      <td>Radioactive decay is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha decay (α-decay), beta decay (β-decay), and gamma decay (γ-decay), all of which involve emitting one or more particles. The weak force is the mechanism that is responsible for beta decay, while the other two are governed by the electromagnetism and nuclear force. A fourth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the main characteristics of a Haiku poem?\\n\\n### Input:\\nHaiku (俳句, listen (help·info)) is a type of short form poetry originally from Japan. Traditional Japanese haiku consist of three phrases composed of 17 phonetic units (called on in Japanese, which are similar to syllables) in a 5, 7, 5 pattern; that include a kireji, or \"cutting word\"; and a kigo, or seasonal reference. Similar poems that do not adhere to these rules are generally classified as senryū.\\n\\nHaiku originated as an opening part of a larger Japanese poem called renga. These haiku written as an opening stanza were known as hokku and over time they began to be written as stand-alone poems. Haiku was given its current name by the Japanese writer Masaoka Shiki at the end of the 19th century.\\n\\nOriginally from Japan, haiku today are written by authors worldwide. Haiku in English and haiku in other languages have different styles and traditions while still incorporating aspects of the traditional haiku form. Non-Japanese haiku vary widely on how closely they follow traditional elements. Additionally, a minority movement within modern Japanese haiku (現代俳句, gendai-haiku), supported by Ogiwara Seisensui and his disciples, has varied from the tradition of 17 on as well as taking nature as their subject.\\n\\nIn Japanese, haiku are traditionally printed as a single line, while haiku in English often appear as three lines, although variations exist. There are several other forms of Japanese poetry related to haiku, such as tanka, as well as other art forms that incorporate haiku, such as haibun and haiga.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>Haiku is a form of poetry which is composed of 17 syllables arranged in a 5 - 7 - 5 pattern. A Haiku includes a seasonal reference.</td>\n",
       "      <td>The main characteristics of a Haiku poem are:\\n\\n1. **Syllable count:** 5 syllables in the first line, 7 syllables in the second line, and 5 syllables in the third line, for a total of 17 syllables.\\n2. **Kireji (cutting word):** A word that divides the poem into two parts, often used to create a sense of tension or contrast.\\n3. **Kigo (seasonal reference):** A</td>\n",
       "      <td>Haiku is a type of short form poetry originally from Japan. Traditional Japanese haiku consist of three phrases composed of 17 phonetic units (called on in Japanese, which are similar to syllables) in a 5, 7, 5 pattern; that include a kireji, or \"cutting word\"; and a kigo, or seasonal reference. Similar poems that do not adhere to these rules are generally classified as senryū.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=false\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[\"generated_text\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    responses_after_finetuning.append(finetuned_response[\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-21-34-49-698\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-21-34-49-701\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-21-34-49-701\n",
      "INFO:sagemaker:Deleting model with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-997\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-994\n",
      "INFO:sagemaker:Deleting endpoint with name: meta-textgeneration-llama-3-8b-instruct-2024-05-28-23-44-10-994\n"
     ]
    }
   ],
   "source": [
    "# Delete resources\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5df7e-95a5-47dc-b5d2-0178ebfc6b6f",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "---\n",
    "\n",
    "####  Fine-tune the Model on a New Dataset\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edf0c9-3c95-4e4b-932e-5d8a839d4070",
   "metadata": {},
   "source": [
    "#### 2.3. Example fine-tuning with Domain-Adaptation dataset format\n",
    "---\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Please uncomment the following code to fine-tune the model on dataset in domain adaptation format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c93a9-ebe2-4966-a5d6-af4c053f69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "\n",
    "# estimator = JumpStartEstimator(model_id=model_id,  environment={\"accept_eula\": \"true\"},instance_type = \"ml.g5.24xlarge\")\n",
    "# estimator.set_hyperparameters(instruction_tuned=\"False\", epoch=\"5\")\n",
    "# estimator.fit({\"training\": f\"s3://jumpstart-cache-prod-{boto3.Session().region_name}/training-datasets/sec_amazon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "Sagemaker team have tested the scripts on the following instances types from the Llama 2 fine tuning. We expect Llama 3 performance also to follow the instance types. We will perform more thorough testing soon.\n",
    "\n",
    "- 8B: ml.g5.12xlarge, ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### 6. Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
