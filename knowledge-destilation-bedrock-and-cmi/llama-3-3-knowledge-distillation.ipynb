{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response-Based Knowledge Distillation with QA Specialization\n",
    "\n",
    "## Using Amazon SageMaker JumpStart for LLM distilation (Llama 3.2 90B â†’ Llama 3.2 1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates an end-to-end workflow for knowledge distillation using Amazon SageMaker JumpStart and Amazon Bedrock. The process involves distilling knowledge from a large language model (90B parameters) to a smaller model (1B parameters) while maintaining performance on specialized QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Overview of Knowledge Distillation\n",
    "Knowledge distillation is a model compression technique where a smaller model (student) learns to mimic the behavior of a larger model (teacher). This approach helps:\n",
    "- Reduce computational requirements\n",
    "- Lower inference costs\n",
    "- Maintain acceptable performance levels\n",
    "- Enable deployment on resource-constrained environments\n",
    "### SageMaker JumpStart Benefits\n",
    "Amazon SageMaker JumpStart provides:\n",
    "- Pre-trained models optimized for AWS infrastructure\n",
    "- Simplified model deployment and fine-tuning workflows\n",
    "- Integration with other AWS services like Amazon S3 and Amazon CloudWatch\n",
    "- Built-in security features and compliance controls\n",
    "- Automated model optimization and deployment pipelines\n",
    "### Project Goals and Objectives\n",
    "This implementation aims to:\n",
    "1. Create an efficient, smaller model specialized for QA tasks\n",
    "2. Maintain high accuracy on domain-specific questions\n",
    "3. Reduce inference costs\n",
    "4. Demonstrate AWS best practices for model optimization\n",
    "5. Enable deployment through Amazon Bedrock Custom Model Import\n",
    "6. Evaluate performance of model using Custom Model Import instead of running it in Sagemaker Endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker jmespath datasets transformers jinja2 ipywidgets boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "### AWS Account Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section configures the necessary AWS resources including:\n",
    "\n",
    "- SageMaker session and default bucket\n",
    "- IAM roles and permissions\n",
    "- Region-specific settings\n",
    "- Required SDK versions and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "# AWS SDK imports\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker import hyperparameters, metric_definitions\n",
    "from sagemaker.parameter import ContinuousParameter, CategoricalParameter, IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "# Data processing and ML imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Custom modules (assuming these exist in your environment)\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"utils\"] = utils\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "# Import custom functions\n",
    "from utils import (\n",
    "    download_artifacts, \n",
    "    remove_field_from_json, \n",
    "    upload_artifacts, \n",
    "    cleanup_local_files, \n",
    "    wait_for_model_availability, \n",
    "    test_image_processing\n",
    ")\n",
    "from iam_role_helper import create_or_update_role\n",
    "\n",
    "# Initialize key AWS clients\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-west-2')\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Set default configurations\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #change the name of the role if you are running locally\n",
    "    role = iam.get_role(RoleName='sagemaker-execution-role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "region=sess.boto_region_name\n",
    "\n",
    "prefix = \"llama-qa-distillation\"\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Configuration\n",
    "Configures IAM roles with required permissions for:\n",
    "- Amazon Bedrock model access\n",
    "- S3 bucket operations for model artifacts\n",
    "- CloudWatch logging capabilities\n",
    "- Cross-service permissions for SageMaker\n",
    "\n",
    "Key components:\n",
    "1. Trust relationships for service principals\n",
    "2. Permission policies for resource access\n",
    "3. Cross-account access configurations\n",
    "4. Logging and monitoring permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM Role Configuration for Amazon Bedrock Custom Model Import\n",
    "\n",
    "# 1. Setup Basic Variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']  # Get current AWS account ID\n",
    "region = \"us-west-2\"  # Note: Custom Model Import (CMI) only works in us-west-2 and us-east-1\n",
    "training_bucket = sagemaker_session_bucket  # S3 bucket where training artifacts are stored\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"  # Name for the IAM role we'll create\n",
    "\n",
    "# 2. Define Trust Relationship Policy\n",
    "# This policy defines which AWS services can assume this role\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        # Allow Bedrock service to assume this role\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                # Ensure requests only come from our account\n",
    "                \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                # Limit to specific Bedrock model import jobs\n",
    "                \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "            }\n",
    "        },\n",
    "        # Allow Lambda service to assume this role (if needed for auxiliary functions)\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Define Permission Policy\n",
    "# This policy defines what AWS resources the role can access\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        # Allow S3 access for model artifacts\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],  # Read-only access to S3\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{training_bucket}\",  # Access to bucket\n",
    "                f\"arn:aws:s3:::{training_bucket}/*\"  # Access to objects in bucket\n",
    "            ],\n",
    "            \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}  # Restrict to our account\n",
    "        },\n",
    "        # Allow CloudWatch Logs access for monitoring\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"  # Access to CloudWatch Logs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Create or Update the IAM Role\n",
    "bedrock_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=permission_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model Selection in Amazon Bedrock\n",
    "\n",
    "### Model Selection Criteria\n",
    "When choosing a foundation model in Amazon Bedrock for knowledge distillation, several key factors should be considered:\n",
    "\n",
    "#### 1. Model Architecture and Size\n",
    "The Meta Llama 3 405B model offers several advantages as a teacher model:\n",
    "- Larger parameter count provides richer knowledge representation\n",
    "- Enhanced ability to capture complex patterns and relationships\n",
    "- Superior performance on specialized tasks like medical QA\n",
    "- Better few-shot learning capabilities\n",
    "#### 2. Cost-Performance Trade-offs\n",
    "Amazon Bedrock's pay-per-use pricing model enables:\n",
    "- No upfront infrastructure costs\n",
    "- Payment only for actual inference time\n",
    "- Flexible scaling based on demand\n",
    "- Cost optimization through batch processing\n",
    "\n",
    "Reference: [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "#### 3. Specialized Knowledge Transfer\n",
    "The 405B model is particularly suitable for knowledge distillation because:\n",
    "- Higher accuracy on complex medical terminology\n",
    "- Better understanding of scientific context\n",
    "- More nuanced response generation\n",
    "- Improved zero-shot performance on domain-specific tasks\n",
    "#### 4. Operational Considerations\n",
    "Benefits of using Bedrock for the teacher model:\n",
    "- Serverless architecture eliminates infrastructure management\n",
    "- Built-in auto-scaling\n",
    "- High availability across AWS regions\n",
    "- Simplified API integration\n",
    "### Model Configuration\n",
    "The Llama 3 405B model in Bedrock can be configured with:\n",
    "- Temperature settings for response diversity\n",
    "- Maximum token length for comprehensive answers\n",
    "- Top-p and top-k sampling parameters\n",
    "- Custom prompt templates for specialized tasks\n",
    "\n",
    "Reference: [Amazon Bedrock Llama Model Configuration](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html)\n",
    "\n",
    "### Integration with Knowledge Distillation\n",
    "The workflow leverages Bedrock's advantages:\n",
    "1. Generate high-quality training data through batch inference\n",
    "2. Create specialized QA pairs for student model training\n",
    "3. Maintain quality while reducing computational requirements\n",
    "4. Enable seamless deployment through Custom Model Import\n",
    "\n",
    "Reference: \n",
    "- [Amazon Bedrock Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)\n",
    "- [Amazon Bedrock Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html)\n",
    "\n",
    "### Best Practices\n",
    "When using the teacher model:\n",
    "1. Implement proper error handling and retry mechanisms\n",
    "2. Use batch processing for dataset generation\n",
    "3. Monitor usage and costs through AWS CloudWatch\n",
    "4. Implement appropriate security controls and encryption\n",
    "\n",
    "For more information on model selection and configuration, see:\n",
    "- [Choose the best foundational model for your AI applications](https://community.aws/content/2fKJW0z9PEIKec94DZwtYigCF7i/choose-the-best-foundational-model-for-your-ai-applications?lang=en)\n",
    "- [Llama Technical Documentation](https://www.llama.com/docs/overview/)\n",
    "- [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple models available on Bedrock depending the region. In our case we would focus on llama 3.1 405b instruct that is available in us-west-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock', region_name=\"us-west-2\")\n",
    "model_id='meta.llama3-1-405b-instruct-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference with Bedrock Runtime\n",
    "\n",
    "This section demonstrates how to perform inference using the Bedrock Runtime client with the Llama model.\n",
    "\n",
    "**Note**: The Bedrock runtime client is specifically for model inference, separate from the main Bedrock client used for model management.\n",
    "\n",
    "\n",
    "Inference Helper Function.\n",
    "\n",
    "This function handles the core interaction with the Bedrock Runtime API, including error handling and response formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brt = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n",
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    try:\n",
    "        response = brt.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Setup and Model Parameters.\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "- temperature: Lower values make output more focused and deterministic\n",
    "- top_p: Controls diversity of token selection\n",
    "- max_gen_len: Limits response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "\n",
    "question = \"\"\"Is a mandatory general surgery rotation necessary in the surgical clerkship?\"\"\"\n",
    "user_message = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\": user_message,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_gen_len\": 512,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Configuration and Invocation\n",
    "- Uses Llama 3 405B parameter model\n",
    "- Expects and returns JSON formatted data\n",
    "- Response includes generated text in the \"generation\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Generation\n",
    "\n",
    "This section explains how to prepare and process the PubMedQA dataset for knowledge distillation using AWS services.\n",
    "\n",
    "### Overview\n",
    "The PubMedQA dataset is a large-scale question-answering dataset focused on biomedical research literature. We'll use Amazon S3 for storage and SageMaker Processing Jobs for data preparation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Details\n",
    "**PubMedQA Dataset**\n",
    "- Source: [PubMedQA GitHub Repository](https://github.com/pubmedqa/pubmedqa/tree/master)\n",
    "- Citation: \n",
    ">\n",
    "> Jin, Q., Dhingra, B., Liu, Z., Cohen, W., & Lu, X. (2019). In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 2567-2577.\n",
    "- Format: JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Steps\n",
    "\n",
    "#### 1. Dataset Download and Validation\n",
    "This module handles downloading and processing PubMedQA dataset from GitHub for use with \n",
    "Amazon SageMaker and Amazon Bedrock knowledge distillation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_github_json(url):\n",
    "    try:\n",
    "        # Convert regular GitHub URL to raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        return requests.get(raw_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://github.com/pubmedqa/pubmedqa/blob/master/data/ori_pqal.json\"\n",
    "data = get_github_json(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Processing and JSONL Conversion\n",
    "This section demonstrates how to process the PubMedQA dataset in jsonl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "qa_index=list(data.keys())\n",
    "for i in qa_index:\n",
    "    keys_to_get = ['QUESTION', 'CONTEXTS','LONG_ANSWER']\n",
    "    result = {k: data[i].get(k) for k in keys_to_get}\n",
    "    dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_dataset='dataset.jsonl'\n",
    "with open(output_file_dataset, 'w') as outfile:\n",
    "    for sample in dataset:\n",
    "        # Create the complete record for batch inference\n",
    "        batch_record = {\n",
    "            \"question\": sample['QUESTION'],\n",
    "            \"answers\": sample['LONG_ANSWER']\n",
    "        }\n",
    "        \n",
    "        outfile.write(json.dumps(batch_record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Teacher Model for QA Generation\n",
    "\n",
    "Explains the process of:\n",
    "\n",
    "- Generating synthetic QA pairs\n",
    "- Batch processing with Bedrock\n",
    "- Data augmentation strategies\n",
    "- Quality control measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Processing vs Real-Time Inference\n",
    "\n",
    "Based on performance testing and cost analysis, Amazon Bedrock's batch processing capabilities offer significant advantages over real-time inference:\n",
    "\n",
    "1. **Performance Benefits**\n",
    "   - Higher throughput for large-scale processing\n",
    "   - Reduced risk of API throttling\n",
    "   - More efficient resource utilization\n",
    "\n",
    "2. **Cost Optimization**\n",
    "   - Lower per-request costs compared to real-time inference\n",
    "   - Better resource allocation and scheduling\n",
    "   - Reduced overhead from connection management\n",
    "\n",
    "3. **Operational Advantages**\n",
    "   - Built-in retry mechanisms\n",
    "   - Simplified monitoring and logging\n",
    "   - Better handling of large datasets\n",
    "\n",
    "For this implementation, we leverage Bedrock's batch processing to optimize both performance and cost efficiency while maintaining processing quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset for Bedrock Batch Processing\n",
    "\n",
    "This code creates a JSONL file formatted specifically for Amazon Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Converts QA dataset into Bedrock's required batch processing format\n",
    "- **Key Operations**:\n",
    "  - Formats prompts using Llama 3's instruction template\n",
    "  - Assigns unique IDs to each record\n",
    "  - Sets inference parameters (temperature, max length, etc.)\n",
    "  - Creates JSONL output with required Bedrock structure\n",
    "\n",
    "The resulting file enables efficient batch processing of multiple questions through Bedrock's batch inference API, optimizing for throughput and cost efficiency.\n",
    "\n",
    "> **Note**: The template uses Llama 3's specific tokens (`<|begin_of_text|>`, `<|eot_id|>`) for proper model instruction formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "def create_bedrock_batch_dataset(dataset, output_file='bedrock_batch_dataset.jsonl'):\n",
    "    # Simplified prompt template for Llama 3 instruction format\n",
    "    prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{question}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Generate a unique record ID (11 characters)\n",
    "            record_id = str(uuid.uuid4())[:11]\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = prompt_template.format(\n",
    "                question=sample[\"QUESTION\"]\n",
    "            )\n",
    "\n",
    "            # Create the model input body for Llama 3\n",
    "            body = {\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 1024,\n",
    "                \"temperature\": 0.0,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "\n",
    "            # Create the complete record for batch inference\n",
    "            batch_record = {\n",
    "                \"recordId\": record_id,\n",
    "                \"modelInput\": body\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(batch_record) + '\\n')\n",
    "\n",
    "# Usage\n",
    "create_bedrock_batch_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Batch Dataset to Amazon S3\n",
    "\n",
    "This code handles the upload of the prepared batch dataset to Amazon S3, a necessary step before running Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Transfers the local JSONL file to S3 for Bedrock access\n",
    "- **Components**:\n",
    "  - Uses SageMaker's `S3Uploader` utility for simplified file transfer\n",
    "  - Organizes files under a structured prefix (`distillation/batch/data`)\n",
    "  - Automatically handles S3 path formatting and permissions\n",
    "\n",
    "> **Note**: The S3 location will be referenced in subsequent Bedrock batch inference job configurations. Ensure the Bedrock role has appropriate S3 read permissions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "# Define source and destination paths\n",
    "local_path_batch_file = 'bedrock_batch_dataset.jsonl'\n",
    "s3_prefix_batch = 'distillation/batch/data'  # This will be the folder in S3\n",
    "\n",
    "# Upload the file\n",
    "s3_path_batch = S3Uploader.upload(\n",
    "    local_path=local_path_batch_file,\n",
    "    desired_s3_uri=f's3://{bucket}/{s3_prefix_batch}',\n",
    ")\n",
    "\n",
    "print(f\"File uploaded successfully to: {s3_path_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Batch Inference Configuration\n",
    "\n",
    "This section configures and launches a batch inference job using Amazon Bedrock for large-scale QA processing:\n",
    "\n",
    "#### Configuration Components\n",
    "- **Input Configuration**: Points to the JSONL dataset in S3\n",
    "- **Output Configuration**: Specifies where Bedrock will store inference results\n",
    "- **Job Settings**: \n",
    "  - Unique job name using timestamp\n",
    "  - Model ARN for Llama 3.3\n",
    "  - IAM role for execution permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix=\"output\"\n",
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": s3_path_batch\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket}/{s3_prefix_batch}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime  # This is the correct import\n",
    "jobName = 'batch-job-ga' + str(int(datetime.now().timestamp()))\n",
    "response=bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role,\n",
    "    modelId='arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-405b-instruct-v1:0',\n",
    "    #modelId='meta.llama3-1-405b-instruct-v1:0',\n",
    "    \n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see Amazon Bedrock Batch Inference documentation.\n",
    "Reference: [Amazon Bedrock Batch Inference](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Bedrock Batch Job Status\n",
    "\n",
    "This code implements a job status monitoring loop for the Bedrock batch inference:\n",
    "\n",
    "- **Purpose**: Tracks batch job progress until completion or failure\n",
    "- **Key Operations**:\n",
    "  - Extracts job ARN and ID for tracking\n",
    "  - Polls job status every 5 minutes\n",
    "  - Provides real-time status updates\n",
    "  - Handles completion and failure scenarios\n",
    "\n",
    "> **Note**: Consider implementing this monitoring pattern in AWS Lambda or Step Functions for production workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobArn)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Bedrock Batch Results for Training\n",
    "\n",
    "This section handles the retrieval and processing of batch inference results from S3 for model training:\n",
    "#### Data Flow\n",
    "1. **Retrieval**: Fetches batch results from S3\n",
    "2. **Processing**: Extracts model generations from JSON responses\n",
    "3. **Formatting**: Prepares data for JumpStart/Bedrock training format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve batch results from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve batch results from S3\n",
    "job_id='yqlsjasksdyv'\n",
    "s3 = boto3.client('s3')\n",
    "prefix = f\"{s3_prefix_batch}/{output_prefix}/{job_id}/\"\n",
    "print(f\"prefix: {bucket}/{prefix}\")\n",
    "object_key = f\"{prefix}{local_path_batch_file}.out\"\n",
    "response = s3.get_object(Bucket=bucket, Key=object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and extract teacher model responses\n",
    "json_data = response['Body'].read().decode('utf-8')\n",
    "teacher_answer=[]\n",
    "for line in json_data.splitlines():\n",
    "        data = json.loads(line)\n",
    "        print(data['modelOutput']['generation'])\n",
    "        teacher_answer.append(data['modelOutput']['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines the original dataset with teacher model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_item, teacher in zip(dataset, teacher_answer):\n",
    "    data_item['TEACHER_ANSWER'] = teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: This paired dataset forms the foundation for training the student model to mimic the teacher's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data for SageMaker JumpStart\n",
    "\n",
    "This section formats the QA dataset for fine-tuning using SageMaker JumpStart's specific requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formatting Process\n",
    "1. **Template Creation**\n",
    "   - Defines Llama 3's instruction format\n",
    "   - Includes system message and conversation structure\n",
    "   - Maintains special tokens for model context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dataset Transformation**\n",
    "   - Converts QA pairs to instruction format\n",
    "   - Structures teacher responses as completions\n",
    "   - Creates JSONL format required by JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_jumpstart_dataset(dataset, output_file='train.jsonl', template_file='template.json'):\n",
    "    # Create the template file required by JumpStart for Q&A format\n",
    "    template = {\n",
    "        \"prompt\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        \"completion\": \"{response}\"\n",
    "    }\n",
    "    \n",
    "    # Save the template file\n",
    "    with open(template_file, 'w') as f:\n",
    "        json.dump(template, f)\n",
    "\n",
    "    # Process the dataset and create the training file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Format the data in the same structure as the synthetic data\n",
    "            training_entry = {\n",
    "                \"instruction\": sample[\"QUESTION\"],\n",
    "                \"response\": sample[\"TEACHER_ANSWER\"].strip()\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(training_entry) + '\\n')\n",
    "            \n",
    "def verify_jsonl(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if i == 0:  # Print first example\n",
    "                    print(\"Sample entry:\")\n",
    "                    print(json.dumps(data, indent=2))\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset files for JumpStart fine-tuning\n",
    "create_jumpstart_dataset(dataset)\n",
    "verify_jsonl('train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Format\n",
    "- **Input**: Question-answer pairs with teacher model responses\n",
    "- **Output**: JSONL file containing:\n",
    "  - Instruction prompt with special tokens (`<|begin_of_text|>`)\n",
    "  - Question text\n",
    "  - Teacher model response\n",
    "  - End of text markers (`<|eot_id|>`)\n",
    "\n",
    "> **Important**: Follows [JumpStart Data Format Guidelines](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning-instruction-based.html).\n",
    "\n",
    "#### Validation Process\n",
    "The `verify_jsonl()` function checks:\n",
    "- JSONL format validity\n",
    "- Special token placement\n",
    "- Complete instruction/response pairs\n",
    "\n",
    "Example format:\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"What is the role of antibiotics in treating viral infections?\",\n",
    "  \"response\": \"Antibiotics are not effective against viral infections...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model Configuration (LLAMA 3.2 1B)\n",
    "\n",
    "This section covers the setup and configuration of the student model using Amazon SageMaker JumpStart:\n",
    "\n",
    "### Model Selection Criteria\n",
    "- Base model: LLAMA 3 1B\n",
    "- Optimized for knowledge distillation\n",
    "- Suitable for QA tasks\n",
    "- Efficient inference characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Upload\n",
    "\n",
    "The following code uploads the prepared training dataset and template to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "# Configure S3 paths with SageMaker defaults\n",
    "default_bucket_prefix = sagemaker.Session().default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "# Upload training files to S3\n",
    "local_data_file = \"train.jsonl\"\n",
    "template_file=\"template.json\"\n",
    "train_data_location = f\"s3://{bucket}{default_bucket_prefix_path}/oasst_top1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(template_file,train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")\n",
    "print(f\"template saved on:{train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Model Selection in SageMaker JumpStart\n",
    "\n",
    "This section implements an interactive model selection interface and configures training metrics:\n",
    "\n",
    "#### Model Selection Process\n",
    "- **Purpose**: Enables selection of appropriate student model from JumpStart's catalog\n",
    "- **Focus**: Text generation models suitable for knowledge distillation\n",
    "- **Default**: LLAMA 3 2.1B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Create interactive model selector\n",
    "try:\n",
    "    dropdown = Dropdown(\n",
    "        options=list_jumpstart_models(\"search_keywords includes Text Generation\"),\n",
    "        value=\"meta-textgeneration-llama-3-2-1b-instruct\",\n",
    "        description=\"Select a JumpStart text generation model:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "    display(dropdown)\n",
    "except:\n",
    "    dropdown = None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dropdown:\n",
    "    student_model_id = dropdown.value\n",
    "else:\n",
    "    # Provide model id as meta-textgeneration-llama-3-1-405b-instruct-fp8 for the instruct variant\n",
    "    model_id = \"meta-textgeneration-llama-3-2-1b-instruct\"\n",
    "model_version_student = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric Setup\n",
    "- **Purpose**: Establishes standardized metrics for training evaluation\n",
    "- **Implementation**: Leverages SageMaker's built-in metric definitions\n",
    "- **Scope**: Covers training, validation, and system metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import metric_definitions\n",
    "print(metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Job Hyperparameter Configuration\n",
    "\n",
    "This section retrieves and configures the default hyperparameters for the student model training:\n",
    "\n",
    "#### Hyperparameter Setup\n",
    "- **Purpose**: Initializes model training configuration\n",
    "- **Source**: Uses JumpStart's optimized defaults\n",
    "- **Scope**: Includes learning rates, batch sizes, and model-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters_student = hyperparameters.retrieve_default(\n",
    "    model_id=student_model_id, model_version=model_version_student,\n",
    ")\n",
    "\n",
    "print(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Customization\n",
    "This section modifies default hyperparameters for knowledge distillation training:\n",
    "\n",
    "#### Parameter Adjustments\n",
    "- **Purpose**: Customizes training configuration for instruction-based learning\n",
    "- **Key Modifications**:\n",
    "  - Sets single epoch for initial testing\n",
    "  - Configures for instruction tuning\n",
    "  - Establishes fixed random seed for reproducibility\n",
    "  - Defines maximum input length constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters_student[\"epoch\"] = \"1\"\n",
    "my_hyperparameters_student['chat_dataset']=\"False\"\n",
    "my_hyperparameters_student['instruction_tuned']=\"True\"\n",
    "my_hyperparameters_student['seed']=\"10\"# this could help us to have the same results\n",
    "my_hyperparameters_student['max_input_length']=\"1024\"# this could help us to have the same results\n",
    "\n",
    "\n",
    "hyperparameters.validate(\n",
    "    model_id=student_model_id, model_version=model_version_student, hyperparameters=my_hyperparameters_student\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Configuration\n",
    "\n",
    "This section configures and executes automated hyperparameter optimization using SageMaker's Hyperparameter Tuning Jobs:\n",
    "\n",
    "#### Parameter Search Space Configuration\n",
    "- **Purpose**: Defines ranges for key training parameters\n",
    "- **Implementation**: Uses SageMaker's parameter types for optimization\n",
    "- **Scope**: Covers learning dynamics and LoRA-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import ContinuousParameter, CategoricalParameter,IntegerParameter\n",
    "\n",
    "# Define hyperparameter ranges without as_json_range\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.0005, scaling_type=\"Logarithmic\"),\n",
    "    'lora_r': CategoricalParameter(['4', '8', '12', '16']),\n",
    "    'lora_alpha': CategoricalParameter(['16', '32', '48', '64']),\n",
    "    'lora_dropout': ContinuousParameter(0.01, 0.2),\n",
    "    'per_device_train_batch_size': CategoricalParameter(['2', '4', '6', '8']),\n",
    "    'gradient_accumulation_steps': CategoricalParameter(['1', '2', '3', '4']),\n",
    "    'max_steps': CategoricalParameter(['50', '75', '100']),\n",
    "    'warmup_steps': CategoricalParameter(['5', '7', '10']),\n",
    "    'num_train_epochs': CategoricalParameter(['1', '2'])\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "\n",
    "metric_defs=metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b\", model_version='1.1.1',)\n",
    "print(metric_defs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced Metric Tracking\n",
    "- **Purpose**: Monitors both training and resource utilization metrics\n",
    "- **Implementation**: Combines default and custom GPU memory metrics\n",
    "- **Scope**: Enables comprehensive performance monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_metrics = [\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'train:loss', 'Regex': 'train_loss = ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = metric_defs + memory_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Job Configuration\n",
    "\n",
    "This section configures the hyperparameter optimization job using SageMaker's tuning capabilities:\n",
    "\n",
    "#### Configuration Components\n",
    "- **Purpose**: Automates hyperparameter optimization for model training\n",
    "- **Strategy**: Uses Bayesian optimization for efficient parameter search\n",
    "- **Scale**: Manages multiple training jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Create the estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=my_hyperparameters_student,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    metric_definitions=combined_metrics,  # Add metric definitions here\n",
    "    enable_sagemaker_metrics=True  # Enable SageMaker metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='huggingface-textgeneration:train-loss',\n",
    "    metric_definitions=combined_metrics,\n",
    "    objective_type='Minimize',\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=4,#Adjust depending the available instances\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy='Bayesian',\n",
    "    base_tuning_job_name='llm-llama-3-2-1b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the hyperparameter tuning job\n",
    "tuner.fit({\"training\": train_data_location}, wait=True)\n",
    "# First, wait for the tuning job to complete\n",
    "tuner.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices for Hyperparameter Tuning**\n",
    ">\n",
    "> 1. **Resource Management**\n",
    ">    - Set `max_parallel_jobs` based on quota limits\n",
    ">    - Choose appropriate instance types (`ml.g5.2xlarge`)\n",
    ">    - Monitor GPU memory utilization\n",
    ">    - Consider cost optimization with spot instances\n",
    ">\n",
    "> 2. **Job Configuration**\n",
    ">    - Use descriptive `base_tuning_job_name`\n",
    ">    - Enable SageMaker metrics for monitoring\n",
    ">    - Set appropriate stopping conditions\n",
    ">    - Configure proper objective metrics\n",
    ">\n",
    "> 3. **Optimization Strategy**\n",
    ">    - Start with Bayesian optimization\n",
    ">    - Define meaningful parameter ranges\n",
    ">    - Balance exploration vs exploitation\n",
    ">    - Monitor convergence patterns\n",
    ">\n",
    "> See [Hyperparameter Tuning Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Best Training Results\n",
    "\n",
    "This section explains how to access and analyze the best performing model from the hyperparameter tuning job:\n",
    "#### Accessing Best Model\n",
    "- **Purpose**: Retrieves optimal hyperparameters and model artifacts\n",
    "- **Implementation**: Uses SageMaker's tuning job APIs\n",
    "- **Output**: Best performing model configuration and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Overview\n",
    "1. Get best training job name from tuner\n",
    "2. Retrieve detailed job information using SageMaker client\n",
    "3. Extract optimized hyperparameters\n",
    "4. Access performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job)['HyperParameters']\n",
    "print(\"Best hyperparameters: \\n\")\n",
    "pprint.pprint(best_hyperparameters_student_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job_1 = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")\n",
    "\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_1)['HyperParameters']\n",
    "print(f\"Best hyperparameters: {best_hyperparameters_student_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices**:\n",
    "> 1. **Result Analysis**\n",
    ">    - Review convergence patterns\n",
    ">    - Compare against baseline metrics\n",
    ">    - Document optimal parameters\n",
    ">\n",
    "> 2. **Model Management**\n",
    ">    - Save best configuration\n",
    ">    - Track experiment metadata\n",
    ">    - Document performance characteristics\n",
    ">\n",
    "> For more information, see [Analyzing Hyperparameter Tuning Results](https://sagemaker-examples.readthedocs.io/en/latest/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Optimized Hyperparameters\n",
    "\n",
    "This section configures and launches a training job using the best hyperparameters from tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Components\n",
    "1. **Hyperparameter Setup**\n",
    "   - Uses optimized parameters from tuning\n",
    "   - Extends training epochs for full model convergence\n",
    "   - Configures training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)\n",
    "best_hyperparameters_student_1['num_train_epochs']=10\n",
    "best_hyperparameters_student_1['epoch']=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **TensorBoard Integration**\n",
    "   - **Purpose**: Enables real-time training visualization\n",
    "   - **Storage**: Configures S3 location for logs\n",
    "   - **Access**: Enables Studio integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "# Create proper TensorBoard output configuration\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f's3://{bucket}/tensorboard-logs/llama3-model-distillation',\n",
    "    container_local_output_path='/opt/ml/output/tensorboard'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Metric Tracking Configuration**\n",
    "   - **Training Metrics**: Loss, perplexity, epoch statistics\n",
    "   - **Resource Metrics**: GPU/CPU memory utilization\n",
    "   - **Performance Metrics**: Throughput and timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "student_model_id = \"meta-textgeneration-llama-3-2-1b\"\n",
    "model_version_student = \"*\"\n",
    "\n",
    "estimator_student = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=best_hyperparameters_student_1,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\",\n",
    "        \"TENSORBOARD_LOGGING\": \"true\",\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    "    tensorboard_output_config=tensorboard_output_config  # Use the proper config object\n",
    ")\n",
    "# Define metrics to track\n",
    "metric_definitions = [\n",
    "    # Training Metrics\n",
    "    {'Name': 'train:loss', 'Regex': 'step .* is completed and loss is ([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:perplexity', 'Regex': 'train_perplexity=([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:epoch_loss', 'Regex': 'train_epoch_loss=([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    {'Name': 'eval:loss', 'Regex': 'eval_epoch_loss=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    {'Name': 'eval:perplexity', 'Regex': 'eval_ppl=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Performance Metrics\n",
    "    {'Name': 'epoch_time', 'Regex': 'epcoh time ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_throughput', 'Regex': '([0-9\\\\.]+)it/s'},\n",
    "    \n",
    "    # Memory Usage\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'cpu:peak_memory', 'Regex': 'CPU Total Peak Memory consumed during the train \\\\(max\\\\): ([0-9\\\\.]+) GB'}\n",
    "]\n",
    "# Add metrics to estimator\n",
    "estimator_student.metric_definitions = metric_definitions\n",
    "# Launch TensorBoard in SageMaker Studio\n",
    "tensorboard_callback = {\n",
    "    'Config': {\n",
    "        'TrainingJobName': 'llama-3-2-1b-model-distilation'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Training Launch**\n",
    "   - **Implementation**: Uses JumpStart estimator\n",
    "   - **Monitoring**: Enables comprehensive logging\n",
    "   - **Visualization**: Integrates with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_student.fit({\"training\": train_data_location},\n",
    "    wait=True,\n",
    "    logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Best Practices**:\n",
    "> 1. **Training Monitoring**\n",
    ">    - Track all defined metrics\n",
    ">    - Monitor resource utilization\n",
    ">    - Review TensorBoard visualizations\n",
    ">\n",
    "> 2. **Resource Management**\n",
    ">    - Configure appropriate instance types\n",
    ">    - Monitor memory usage\n",
    ">    - Track training progress\n",
    ">\n",
    "> 3. **Output Management**\n",
    ">    - Organize TensorBoard logs\n",
    ">    - Maintain training artifacts\n",
    ">    - Document training results\n",
    "\n",
    "For more information, see:\n",
    "- [SageMaker Training Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html)\n",
    "- [TensorBoard Integration](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Deployment\n",
    "\n",
    "This section covers the deployment and testing of the trained student model using Amazon Bedrock Custom Model Import (CMI):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Import Process\n",
    "- **Purpose**: Deploys trained model to Bedrock for serverless inference\n",
    "- **Implementation**: Automates model import and configuration\n",
    "- **Benefits**: Enables seamless integration with AWS AI services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Configuration\n",
    "1. **Model Preparation**\n",
    "   - Retrieves training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training job name and model URI\n",
    "training_job_name = estimator_student._current_job_name\n",
    "model_uri_1 = estimator_student.model_data['S3DataSource']['S3Uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Deployment Process**\n",
    "   - Configures import job parameters\n",
    "   - Sets up unique model identifiers\n",
    "   - Creates import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME = 'us-west-2'\n",
    "bedrock = boto3.client(service_name='bedrock',\n",
    "                       region_name=REGION_NAME)\n",
    "# Generate a uni\n",
    "timestamp = int(time.time())\n",
    "random_number = random.randint(1000, 9999)\n",
    "JOB_NAME = f\"meta3-import-model-{timestamp}-{random_number}\"\n",
    "\n",
    "ROLE_ARN = bedrock_role_arn\n",
    "IMPORTED_MODEL_NAME = f\"llama3_1_student_1_llama_1b_{timestamp}-{random_number}\"\n",
    "S3_URI = model_uri_1\n",
    "\n",
    "# createModelImportJob API\n",
    "create_job_response = bedrock.create_model_import_job(\n",
    "    jobName=JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=ROLE_ARN,\n",
    "    modelDataSource={\n",
    "        \"s3DataSource\": {\n",
    "            \"s3Uri\": model_uri_1\n",
    "        }\n",
    "    },\n",
    ")\n",
    "job_arn = create_job_response.get(\"jobArn\")\n",
    "print(f\"Model import job created with ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Monitors deployment status\n",
    " - Validates model availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_filter = IMPORTED_MODEL_NAME  # Replace with your model name\n",
    "model_info = wait_for_model_availability(model_name_filter,max_attempts=30,delay=60)\n",
    "#\n",
    "if model_info:\n",
    "    model_arn_1=model_info[\"modelArn\"]\n",
    "    print(\"Model is now available in Bedrock.\")\n",
    "else:\n",
    "    print(\"Failed to find the model in Bedrock within the specified attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Testing Configuration**\n",
    "   - Sets up runtime client\n",
    "   - Configures retry policies\n",
    "   - Implements error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import json\n",
    "\n",
    "REGION_NAME = 'us-west-2'\n",
    "MODEL_ID= model_arn_1\n",
    "#MODEL_ID='arn:aws:bedrock:us-west-2:786045444066:imported-model/d6ky0o73eq1l'\n",
    "\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100, \n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "message = \"Hello, what it is the weather in seattle?\"\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "br_runtime = session.client(service_name = 'bedrock-runtime', \n",
    "                                 region_name=REGION_NAME, \n",
    "                                 config=config)\n",
    "    \n",
    "try:\n",
    "    invoke_response = br_runtime.invoke_model(modelId=MODEL_ID, \n",
    "                                            body=json.dumps({'prompt': message}), \n",
    "                                            accept=\"application/json\", \n",
    "                                            contentType=\"application/json\")\n",
    "    invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    print(json.dumps(invoke_response, indent=4))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(e.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Bedrock Model Deployment with Custom Model Import\n",
    "\n",
    "> **1. Import Configuration**\n",
    "> - Use descriptive, unique model names with timestamps\n",
    "> - Configure appropriate IAM roles and permissions\n",
    "> - Implement robust error handling mechanisms\n",
    "> - Set appropriate timeout values\n",
    "> - Validate model artifacts before import\n",
    "\n",
    "> **2. Deployment Monitoring**\n",
    "> - Track import job status regularly\n",
    "> - Implement automated status checks\n",
    "> - Set up CloudWatch alerts\n",
    "> - Monitor resource utilization\n",
    "> - Track deployment metrics\n",
    "\n",
    "> **3. Testing Strategy**\n",
    "> - Implement comprehensive test cases\n",
    "> - Validate model responses\n",
    "> - Monitor inference latency\n",
    "> - Track error rates and types\n",
    "> - Test with various input formats\n",
    "\n",
    "### Key Benefits of Bedrock Deployment\n",
    "\n",
    "#### Operational Benefits\n",
    "- **Serverless Infrastructure**\n",
    "  - No server management required\n",
    "  - Automatic scaling capabilities\n",
    "  - Pay-per-use pricing model\n",
    "\n",
    "- **Management Simplification**\n",
    "  - Automated deployments\n",
    "  - Built-in monitoring\n",
    "  - Simplified updates\n",
    "\n",
    "#### Technical Benefits\n",
    "- **Performance**\n",
    "  - Optimized inference\n",
    "  - Low-latency responses\n",
    "  - Automatic resource scaling\n",
    "\n",
    "- **Integration**\n",
    "  - Seamless AWS service connectivity\n",
    "  - Built-in security features\n",
    "  - Standardized APIs\n",
    "\n",
    "#### Cost Benefits\n",
    "- Pay-per-invocation pricing\n",
    "- No minimum commitments\n",
    "- Resource-efficient scaling\n",
    "#### Additional Resources\n",
    "- [Bedrock Custom Model Import Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)\n",
    "- [Bedrock Custom Model Import Pricing Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/import-model-calculate-cost.html)\n",
    "- [Model Monitoring Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Environment Setup\n",
    "\n",
    "#### Configuration Requirements\n",
    "1. **FMBench YML template creation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_template(cmi_arn):\n",
    "    \"\"\"Returns a template string for a single experiment\"\"\"\n",
    "    return f'''  - name: {cmi_arn}\n",
    "    # model_id is interpreted in conjunction with the deployment_script, so if you\n",
    "    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n",
    "    # if deploying directly from HuggingFace this would be a HuggingFace model id\n",
    "    # see the DJL serving deployment script in the code repo for reference.    \n",
    "    model_id: {cmi_arn}\n",
    "    model_version: \n",
    "    model_name: {cmi_arn}\n",
    "    ep_name: {cmi_arn}\n",
    "    instance_type: {cmi_arn}\n",
    "    image_uri:\n",
    "    deploy: no\n",
    "    instance_count:\n",
    "    deployment_script:\n",
    "    inference_script: bedrock_predictor.py\n",
    "    inference_spec:\n",
    "      split_input_and_parameters: no\n",
    "      parameter_set: bedrock\n",
    "      stream: True\n",
    "      start_token:\n",
    "      stop_token: \"<|eot_id|>\"\n",
    "    payload_files:\n",
    "    - payload_en_1-500.jsonl\n",
    "    - payload_en_500-1000.jsonl\n",
    "    - payload_en_1000-2000.jsonl\n",
    "    - payload_en_2000-3000.jsonl\n",
    "    - payload_en_3000-3840.jsonl\n",
    "    concurrency_levels:\n",
    "    - 1\n",
    "    env:'''\n",
    "\n",
    "def create_config_file(template_file, output_file, cmi_arn_list):\n",
    "    # Read the template file\n",
    "    with open(template_file, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find the experiments section\n",
    "    start_marker = \"experiments:\"\n",
    "    start_idx = content.find(start_marker)\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        raise ValueError(\"Could not find experiments section in template file\")\n",
    "    \n",
    "    # Split content at experiments section\n",
    "    header = content[:start_idx + len(start_marker)]\n",
    "    \n",
    "    # Create experiments content\n",
    "    experiments = \"\\n\"  # Start with newline after \"experiments:\"\n",
    "    \n",
    "    # Add default Llama experiment\n",
    "    llama_experiment = create_experiment_template(\"us.meta.llama3-2-90b-instruct-v1:0\")\n",
    "    experiments += llama_experiment + \"\\n\"\n",
    "    \n",
    "    # Add experiments for each CMI ARN\n",
    "    for cmi_arn in cmi_arn_list:\n",
    "        experiments += create_experiment_template(cmi_arn) + \"\\n\"\n",
    "    \n",
    "    # Add the report section\n",
    "    report_section = '''\n",
    "report:\n",
    "  latency_budget: 2\n",
    "  cost_per_10k_txn_budget: 100\n",
    "  error_rate_budget: 0\n",
    "  per_inference_request_file: per_inference_request_results.csv\n",
    "  all_metrics_file: all_metrics.csv\n",
    "  txn_count_for_showing_cost: 10000\n",
    "  v_shift_w_single_instance: 0.025\n",
    "  v_shift_w_gt_one_instance: 0.025'''\n",
    "    \n",
    "    # Combine all parts\n",
    "    final_content = header + experiments + report_section\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(final_content)\n",
    "    \n",
    "    print(f\"Created new config file: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "template_file = 'config-bedrock-llama3-template.yml'\n",
    "output_file = 'config-bedrock-llama3-1-8b_cmi_distilation_comp.yml'\n",
    "\n",
    "# List of CMI ARNs to create experiments for\n",
    "cmi_arn_list = [\n",
    "    model_arn_1,\n",
    "    \n",
    "    # Add more ARNs as needed\n",
    "]\n",
    "\n",
    "create_config_file(template_file, output_file, cmi_arn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create S3 bucket for reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_reports=f's3://{bucket}/model-evaluation' \n",
    "print(f'You need to use the next bucket for performance evaluations:\\n{s3_reports}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **SageMaker Studio Environment**\n",
    "   - Use SageMaker Studio Code Editor\n",
    "   - Minimum instance: `ml.t3.xlarge`\n",
    "   - Storage: 50GB minimum\n",
    "   - Required IAM role permissions:\n",
    "     ```json\n",
    "     {\n",
    "         \"Effect\": \"Allow\",\n",
    "         \"Principal\": {\n",
    "             \"Service\": \"sagemaker.amazonaws.com\"\n",
    "         },\n",
    "         \"Action\": \"sts:AssumeRole\"\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **FMBench Environment Setup**\n",
    "   ```bash\n",
    "   # Create and activate conda environment\n",
    "   conda create --name fmbench_python311 -y python=3.11 ipykernel\n",
    "   source activate fmbench_python311\n",
    "   \n",
    "   # Install FMBench\n",
    "   pip install -U fmbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Setup\n",
    "\n",
    "1. **Directory Configuration**\n",
    "   ```bash\n",
    "   # Set working directory\n",
    "   mkdir fmbench \n",
    "   export EVAL_DIR=\"tmp\"\n",
    "   mkdir -p $EVAL_DIR\n",
    "\n",
    "   # Download FMBench dependencies\n",
    "   curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$EVAL_DIR\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Evaluation Execution**\n",
    "```bash\n",
    "   # Run evaluation\n",
    "    fmbench --config-file $EVAL_DIR/fmbench-read/configs/bedrock/config-bedrock-llama3-1-8b_cmi_distilation_comp.yml \\\n",
    "        --local-mode yes \\\n",
    "        --write-bucket s3://{your-bucket}/model-evaluation \\\n",
    "        --tmp-dir $EVAL_DIR > $EVAL_DIR/fmbench.log 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Monitor Progress**\n",
    "```bash\n",
    "   # View live logs\n",
    "   tail -f $EVAL_DIR/fmbench.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Collection**\n",
    "- Evaluation metrics stored in: $EVAL_DIR/fmbench-write/\n",
    "- Results automatically uploaded to: s3://{your-bucket}/model-evaluation/\n",
    "- Report artifacts include:\n",
    "    - Performance metrics (CSV)\n",
    "    - Visualization plots (PNG)\n",
    "    - Interactive dashboards (HTML)\n",
    "    - Raw evaluation data (JSON)\n",
    "\n",
    "    **Note**: Replace {your-bucket} with your S3 bucket name. Ensure the IAM role has appropriate S3 permissions.\n",
    "\n",
    "**Accesing Results**\n",
    "```python\n",
    "\n",
    "# Example code to load evaluation results (to be implemented)\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_evaluation_results(bucket, prefix):\n",
    "    # Load evaluation results from S3\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to load evaluation results (to be implemented)\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_evaluation_results(bucket, prefix):\n",
    "    # Load evaluation results from S3\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Testing (Teacher vs Student)\n",
    "This section presents the results from FMBench evaluation comparing the teacher model (Llama 70B) and student model (Llama 1B).\n",
    "\n",
    "#### Evaluation Metrics\n",
    "| Model | Judge Accuracy (Cohere) | Judge Accuracy (Claude) | Judge Accuracy (Llama) | Majority Voting |\n",
    "|-------|------------------------|------------------------|---------------------|-----------------|\n",
    "| Teacher (70B) | 96.52% | 92.75% | 91.02% | 93.02% |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "> **Note**: Model evaluations performed by 3 LLM judges using ground truth comparison\n",
    "\n",
    "\n",
    "#### Testing Methodology\n",
    "- Dataset: Multiple QA datasets from LongBench\n",
    "- Prompt lengths: 500-3840 tokens\n",
    "- Concurrency levels: 1-4\n",
    "- Evaluation criteria: Accuracy, latency, cost\n",
    "[Display accuracy_trajectory_per_payload.png]\n",
    "*Figure 1: Accuracy across different prompt lengths*\n",
    "\n",
    "#### Performance Comparison\n",
    "\n",
    "**Latency Metrics**\n",
    "| Model | p50 Latency | p95 Latency | p99 Latency | Transactions/min |\n",
    "|-------|-------------|-------------|-------------|------------------|\n",
    "| Teacher (70B) | 5.27s | 5.27s | 5.27s | 2 |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "[Display tokens_vs_latency.png]\n",
    "*Figure 2: Token processing latency comparison*\n",
    "\n",
    "\n",
    "\n",
    "- Performance comparison\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Analysis\n",
    "\n",
    "#### Latency Measurements\n",
    "- Time to First Token (TTFT)\n",
    "- Time Per Output Token (TPOT)\n",
    "- Overall response latency\n",
    "[Display concurrency_vs_inference_latency.png]\n",
    "*Figure 3: Concurrency vs Inference Latency*\n",
    "\n",
    "#### Throughput Analysis\n",
    "| Model | Prompt Token Throughput | Completion Token Throughput |\n",
    "|-------|------------------------|---------------------------|\n",
    "| Teacher (70B) | 203 tokens/s | 2 tokens/s |\n",
    "| Student (3B) | [Pending] | [Pending] |\n",
    "\n",
    "#### Cost Comparison\n",
    "| Model | Price per Transaction | Price per Token | Cost per 10k Transactions |\n",
    "|-------|---------------------|----------------|------------------------|\n",
    "| Teacher (70B) | $0.002875 | $0.00000072 | $28.75 |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "[Display business_summary.png]\n",
    "*Figure 4: Price Performance Comparison*\n",
    "\n",
    "#### Quality Metrics\n",
    "Error rates and model accuracy across different prompt lengths:\n",
    "\n",
    "[Display error_rates.png]\n",
    "*Figure 5: Error Rates by Model and Concurrency*\n",
    "\n",
    "> **Note**: Full interactive versions of these visualizations are available in the evaluation report at `s3://{bucket}/fmbench-results/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/aws-samples/foundation-model-benchmarking-tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment(Jumpstart or Bedrock)\n",
    "This section talks about differences using Jumpstart deployment vs BedRock\n",
    "### Endpoint Configuration \n",
    "\n",
    "#### SageMaker JumpStart Endpoints\n",
    "- Provides complete infrastructure control through endpoint configurations\n",
    "- Supports custom containers and model serving code\n",
    "- Enables A/B testing through production variants\n",
    "- Requires endpoint management and maintenance\n",
    "\n",
    "#### Bedrock Custom Model Import\n",
    "- Offers serverless deployment with minimal configuration\n",
    "- Streamlines deployment through model import workflow\n",
    "- Integrates automatically with AWS AI services\n",
    "- Manages infrastructure automatically\n",
    "\n",
    "### Scaling and Cost Management \n",
    "\n",
    "#### SageMaker JumpStart\n",
    "- Instance-based pricing with reserved capacity\n",
    "- Auto-scaling based on custom metrics\n",
    "- Granular control over instance types and counts\n",
    "- Best for consistent, high-throughput workloads\n",
    "\n",
    "#### Bedrock Custom Model Import\n",
    "- Pay-per-invocation pricing model\n",
    "- Built-in automatic scaling\n",
    "- No minimum commitment required\n",
    "- Optimal for variable workload patterns\n",
    "\n",
    "### Monitoring Setup \n",
    "\n",
    "#### SageMaker JumpStart\n",
    "- CloudWatch integration for custom metrics\n",
    "- Model monitoring for drift detection\n",
    "- Detailed logging and debugging capabilities\n",
    "- Advanced endpoint metrics and alarms\n",
    "#### Bedrock Custom Model Import\n",
    "- Simplified monitoring through AWS Console\n",
    "- Built-in performance metrics\n",
    "- Automated operational monitoring\n",
    "- Streamlined logging integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Best Practices\n",
    "\n",
    "### Resource Termination\n",
    "\n",
    "1. Delete the Bedrock Custom Model\n",
    "First, let's remove the custom model from Amazon Bedrock:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bedrock_custom_model(model_name):\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    try:\n",
    "        bedrock_client.delete_imported_model(modelIdentifier=model_name)\n",
    "        print(f\"Successfully deleted Bedrock custom model: {model_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        error_code = error.response['Error']['Code']\n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"Error deleting Bedrock custom model: The provided model name is invalid. Model Name: {model_name}\")\n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            print(f\"Error: The model '{model_name}' was not found in Bedrock.\")\n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            print(\"Error: You do not have permission to delete this model.\")\n",
    "        elif error_code == 'ConflictException':\n",
    "            print(\"Error: The model is currently in use or in a state that doesn't allow deletion.\")\n",
    "        else:\n",
    "            print(f\"Error deleting Bedrock custom model: {error}\")\n",
    "\n",
    "# Replace with your actual model name\n",
    "MODEL_NAME = \"llama3-qa-model\"\n",
    "delete_bedrock_custom_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Delete IAM Roles\n",
    "Now, let's remove the IAM roles we created specifically for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_iam_role(role_name):\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        # Delete inline policies\n",
    "        inline_policies = iam.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy in inline_policies:\n",
    "            iam.delete_role_policy(RoleName=role_name, PolicyName=policy)\n",
    "            \n",
    "        # Detach managed policies\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "            \n",
    "        # Delete permissions boundary if it exists\n",
    "        try:\n",
    "            iam.delete_role_permissions_boundary(RoleName=role_name)\n",
    "        except iam.exceptions.NoSuchEntityException:\n",
    "            pass\n",
    "        \n",
    "        # Finally delete the role\n",
    "        iam.delete_role(RoleName=role_name)\n",
    "        print(f\"Successfully deleted IAM role: {role_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting IAM role: {error}\")\n",
    "\n",
    "# Delete LambdaBedrockExecutionRole\n",
    "delete_iam_role(\"LambdaBedrockExecutionRole\")\n",
    "\n",
    "# Delete Sagemaker_Bedrock_import_role\n",
    "delete_iam_role(\"Sagemaker_Bedrock_import_role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "#### Cost Optimization Tips\n",
    "1. Training Optimization\n",
    "    - Use spot instances for training when possible\n",
    "    - Implement early stopping in training jobs\n",
    "    - Clean up training artifacts promptly\n",
    "    - Monitor training metrics to avoid unnecessary epochs\n",
    "2. Inference Optimization\n",
    "    - Choose between Bedrock and SageMaker based on workload patterns\n",
    "    - Use auto-scaling for SageMaker endpoints\n",
    "    - Consider batch processing for large-scale inference\n",
    "    - Monitor and adjust instance sizes based on utilization\n",
    "3. Storage Management\n",
    "    - Implement S3 lifecycle policies for training artifacts\n",
    "    - Clean up temporary datasets after training\n",
    "    - Use appropriate storage classes for different data types\n",
    "#### JumpStart Best Practices\n",
    "1. Model Selection\n",
    "2. Training Configuration\n",
    "3. Data Management\n",
    "4. Knowledge Distillation Specific\n",
    "5. Production Deployment\n",
    "#### Security Best Practices\n",
    "1. Access Control\n",
    "2. Monitoring and Compliance\n",
    "\n",
    "For more information, see:\n",
    "\n",
    "- SageMaker Best Practices[link]\n",
    "\n",
    "- Bedrock Security[link]\n",
    "\n",
    "- AWS Machine Learning Security[link]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results \n",
    "### Lessons Learned \n",
    "### Future Improvements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
