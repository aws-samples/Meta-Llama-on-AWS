{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response-Based Knowledge Distillation with QA Specialization\n",
    "\n",
    "## Using Amazon SageMaker JumpStart for LLM distilation (70B â†’ 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to perform knowledge distillation from a large language model (70B parameters) to a smaller model (3B parameters) using Amazon SageMaker JumpStart and Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Overview of Knowledge Distillation\n",
    "Knowledge distillation is a model compression technique where a smaller model (student) learns to mimic the behavior of a larger model (teacher). This process helps reduce computational requirements while maintaining acceptable performance levels. \n",
    "### SageMaker JumpStart Benefits \n",
    "Amazon SageMaker JumpStart provides:\n",
    "\n",
    "- Pre-trained models optimized for AWS infrastructure\n",
    "- Simplified model deployment and fine-tuning workflows\n",
    "- Integration with other AWS services\n",
    "- Built-in security and compliance features\n",
    "\n",
    "### Project Goals and Objectives \n",
    "\n",
    "This project aims to:\n",
    "\n",
    "- Create a smaller, more efficient model for QA tasks\n",
    "- Maintain high accuracy on domain-specific questions\n",
    "- Reduce inference costs and latency with Custom Model Import in Bedrock\n",
    "- Demonstrate AWS best practices for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker jmespath datasets transformers jinja2 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf ~/.cache/pip/*\n",
    "!pip3 install fmeval --upgrade-strategy only-if-needed --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "### AWS Account Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section configures the necessary AWS resources including:\n",
    "\n",
    "- SageMaker session and default bucket\n",
    "- IAM roles and permissions\n",
    "- Region-specific settings\n",
    "- Required SDK versions and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import botocore\n",
    "sess = sagemaker.Session()\n",
    "import pprint\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #change the name of the role if you are running locally\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-your-role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "region=sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"llama-qa-distillation\"\n",
    "\n",
    "# Print AWS configuration\n",
    "print(f\"SageMaker Session: {sess}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role configuration\n",
    "\n",
    "Configures IAM roles with:\n",
    "\n",
    "- Bedrock-specific permissions\n",
    "- S3 access for model artifacts\n",
    "- CloudWatch logging capabilities\n",
    "- Required cross-service permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Custom modules\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"iam_role_helper\", \"iam_role_helper.py\")\n",
    "iam_role_manager = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"iam_role_manager\"] = iam_role_manager\n",
    "spec.loader.exec_module(iam_role_manager)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"utils\"] = utils\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "from utils import download_artifacts, remove_field_from_json, upload_artifacts, cleanup_local_files, wait_for_model_availability, test_image_processing\n",
    "from iam_role_helper import create_or_update_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-east-1\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"\n",
    "\n",
    "# Define policies\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "            \"Resource\": [f\"arn:aws:s3:::{training_bucket}\", f\"arn:aws:s3:::{training_bucket}/*\"],\n",
    "            \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "bedrock_role_arn = create_or_update_role(\n",
    "    role_name=role_name,\n",
    "    trust_relationship=trust_relationship,\n",
    "    permission_policy=permission_policy\n",
    ")\n",
    "\n",
    "print(f\"Role ARN: {bedrock_role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model (LLaMA 3.3 70B)\n",
    "### Selecting Model in Bedrock \n",
    "This section covers:\n",
    "\n",
    "- Available foundation models in Amazon Bedrock\n",
    "- Model selection criteria for the teacher model\n",
    "- Configuration of the Llama 70B model\n",
    "- Required permissions and quotas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def list_foundation_models(bedrock_client):\n",
    "    \"\"\"\n",
    "    Gets a list of available Amazon Bedrock foundation models.\n",
    "\n",
    "    :return: The list of available bedrock foundation models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock_client.list_foundation_models()\n",
    "        models = response[\"modelSummaries\"]\n",
    "        logger.info(\"Got %s foundation models.\", len(models))\n",
    "        return models\n",
    "\n",
    "    except ClientError:\n",
    "        logger.error(\"Couldn't list foundation models.\")\n",
    "        raise\n",
    "\n",
    "def create_models_dataframe(models):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame with relevant model information.\n",
    "    \n",
    "    :param models: List of model summaries from Bedrock\n",
    "    :return: pandas DataFrame with model information\n",
    "    \"\"\"\n",
    "    model_data = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_info = {\n",
    "            'Model Name': model['modelName'],\n",
    "            'Provider': model['providerName'],\n",
    "            'Model ID': model['modelId'],\n",
    "            'Input Modalities': ', '.join(model['inputModalities']),\n",
    "            'Output Modalities': ', '.join(model['outputModalities']),\n",
    "            'Customizations Supported': ', '.join(model['customizationsSupported']) if 'customizationsSupported' in model else 'None',\n",
    "            'Inference Types': ', '.join(model['inferenceTypesSupported'])\n",
    "        }\n",
    "        model_data.append(model_info)\n",
    "    \n",
    "    df = pd.DataFrame(model_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name=\"bedrock\",region_name=\"us-east-1\")\n",
    "fm_models = list_foundation_models(bedrock_client)\n",
    "\n",
    "# Create DataFrame\n",
    "models_df = create_models_dataframe(fm_models)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAmazon Bedrock Foundation Models:\")\n",
    "print(models_df.to_string(index=False))\n",
    "\n",
    "# Optionally, you can also save to CSV\n",
    "# models_df.to_csv('bedrock_models.csv', index=False)\n",
    "\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_df['Model ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df[(models_df['Provider']=='Meta') & (models_df['Inference Types']=='INFERENCE_PROFILE')]['Model ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use 'INFERENCE_PROFILE' models you need to create an inference profile, you dont need that for 'ON_DEMAND' models\n",
    "\n",
    "[Note] LLama 3.3 70b only works in us-east-1 not sure if is an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "#only us-east-1 let you use llama 3.3 70b us-west-2 fails\n",
    "bedrock_client = boto3.client('bedrock', region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='meta.llama3-70b-instruct-v1:0'\n",
    "inference_profile_name='llama3-70b-inference'\n",
    "inf_profile_response = bedrock_client.create_inference_profile(\n",
    "    inferenceProfileName=inference_profile_name,\n",
    "    description='Teacher model use for syntetic generation in a Llama distilation project',\n",
    "    modelSource={\n",
    "        'copyFrom': f'arn:aws:bedrock:us-east-1::foundation-model/{model_id}'\n",
    "    },\n",
    "    tags=[\n",
    "        {\n",
    "        'key': 'project',\n",
    "            'value': 'Llama-model-distilation'\n",
    "        },\n",
    "        {\n",
    "        'key': 'model-id',\n",
    "            'value': 'meta.llama3-3-70b-instruct'\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference profile created successfully. ARN: {inf_profile_response['inferenceProfileArn']}\")\n",
    "model_arn=inf_profile_response['inferenceProfileArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_profile_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brt = boto3.client(service_name='bedrock-runtime',region_name='us-east-1')\n",
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    try:\n",
    "        response = brt.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Write me a blog about making strong business decisions as a leader. [/INST]\"\"\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\": prompt_data,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_gen_len\": 512,\n",
    "}\n",
    "\n",
    "modelId = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Generation\n",
    "\n",
    "### Corpus Preparation(Prepare QA + Context dataset)\n",
    "Details about:\n",
    "\n",
    "- Dataset selection and preprocessing\n",
    "- PubMedQA dataset structure and characteristics\n",
    "- Data formatting for knowledge distillation\n",
    "- Quality checks and validation steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreaApproved dataset details:\n",
    "\n",
    "https://github.com/pubmedqa/pubmedqa/tree/master\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PubMedQA: A Dataset for Biomedical Research Question Answering**\n",
    ">\n",
    "> Jin, Q., Dhingra, B., Liu, Z., Cohen, W., & Lu, X. (2019). In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 2567-2577."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_github_json(url):\n",
    "    try:\n",
    "        # Convert regular GitHub URL to raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        return requests.get(raw_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://github.com/pubmedqa/pubmedqa/blob/master/data/ori_pqal.json\"\n",
    "data = get_github_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_index=list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qa_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[qa_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for i in qa_index:\n",
    "    keys_to_get = ['QUESTION', 'CONTEXTS','LONG_ANSWER']\n",
    "    result = {k: data[i].get(k) for k in keys_to_get}\n",
    "    dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Teacher Model for QA Generation(Create more data based on the Questions and the context)\n",
    "\n",
    "Explains the process of:\n",
    "\n",
    "- Generating synthetic QA pairs\n",
    "- Batch processing with Bedrock\n",
    "- Data augmentation strategies\n",
    "- Quality control measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Processing vs Real-Time Inference\n",
    "\n",
    "Based on performance testing and cost analysis, Amazon Bedrock's batch processing capabilities offer significant advantages over real-time inference:\n",
    "\n",
    "1. **Performance Benefits**\n",
    "   - Higher throughput for large-scale processing\n",
    "   - Reduced risk of API throttling\n",
    "   - More efficient resource utilization\n",
    "\n",
    "2. **Cost Optimization**\n",
    "   - Lower per-request costs compared to real-time inference\n",
    "   - Better resource allocation and scheduling\n",
    "   - Reduced overhead from connection management\n",
    "\n",
    "3. **Operational Advantages**\n",
    "   - Built-in retry mechanisms\n",
    "   - Simplified monitoring and logging\n",
    "   - Better handling of large datasets\n",
    "\n",
    "For this implementation, we leverage Bedrock's batch processing to optimize both performance and cost efficiency while maintaining processing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "def create_bedrock_batch_dataset(dataset, output_file='bedrock_batch_dataset.jsonl'):\n",
    "    system_message = \"\"\"You are a specialized biomedical research assistant trained to analyze and answer questions about medical and scientific literature. Your role is to:\n",
    "        Extract and interpret key information from biomedical research papers, clinical studies, and medical literature\n",
    "        Provide accurate, evidence-based responses based solely on the provided research context\n",
    "        Focus on specific medical findings, methodologies, and clinical outcomes\n",
    "        Present complex medical information in clear, understandable terms\n",
    "        Maintain precision when discussing medical terminology, study results, and statistical data\n",
    "        Distinguish between preliminary findings and established conclusions\n",
    "        Reference specific sections of the provided research when answering questions\n",
    "        Acknowledge limitations in studies when relevant\n",
    "        Avoid making medical recommendations or providing diagnosis When responding, only use information explicitly stated in the provided biomedical context.\"\"\"\n",
    "\n",
    "    prompt_template = \"\"\"System: {system}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear and concise answer.\"\"\"\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Generate a unique record ID (11 characters)\n",
    "            record_id = str(uuid.uuid4())[:11]\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = prompt_template.format(\n",
    "                system=system_message,\n",
    "                question=sample[\"QUESTION\"]\n",
    "            )\n",
    "\n",
    "            # Create the model input body for Llama 2\n",
    "            body = {\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 512,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "\n",
    "            # Create the complete record for batch inference\n",
    "            batch_record = {\n",
    "                \"recordId\": record_id,\n",
    "                \"modelInput\": body\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(batch_record) + '\\n')\n",
    "\n",
    "# Usage\n",
    "create_bedrock_batch_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "# Define source and destination paths\n",
    "local_path_batch_file = 'bedrock_batch_dataset.jsonl'\n",
    "s3_prefix_batch = 'distillation/batch/data'  # This will be the folder in S3\n",
    "\n",
    "# Upload the file\n",
    "s3_path_batch = S3Uploader.upload(\n",
    "    local_path=local_path_batch_file,\n",
    "    desired_s3_uri=f's3://{bucket}/{s3_prefix_batch}',\n",
    ")\n",
    "\n",
    "print(f\"File uploaded successfully to: {s3_path_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedrock Batch Job Configuration\n",
    "\n",
    "The following code configures a Bedrock batch inference job using Llama 3.3 as the teacher model to generate the synthetic dataset.\n",
    "\n",
    "> **Note**: This implementation uses direct model responses. Consider enhancing with:\n",
    "> - Additional synthetic data generation methods\n",
    "> - Answer validation against ground truth context\n",
    "> - Quality assurance metrics for generated responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix=\"output\"\n",
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": s3_path_batch\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket}/{s3_prefix_batch}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobName = 'batch-job-ga' + str(int(datetime.now().timestamp()))\n",
    "response=bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role,\n",
    "    #modelId='meta.llama3-3-70b-instruct-v1:0',\n",
    "    modelId='us.meta.llama3-3-70b-instruct-v1:0',\n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobArn)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Formatting for JumpStart/Bedrock\n",
    "Covers:\n",
    "\n",
    "- Required data format for training\n",
    "- Chat template structure\n",
    "- Input/output specifications\n",
    "- Validation procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "prefix = f\"{s3_prefix_batch}/{output_prefix}/{job_id}/\"\n",
    "print(f\"prefix: {bucket}/{prefix}\")\n",
    "object_key = f\"{prefix}{local_path_batch_file}.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.get_object(Bucket=bucket, Key=object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = response['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer=[]\n",
    "for line in json_data.splitlines():\n",
    "        data = json.loads(line)\n",
    "        print(data['modelOutput']['generation'])\n",
    "        teacher_answer.append(data['modelOutput']['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['LONG_ANSWER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_item, teacher in zip(dataset, teacher_answer):\n",
    "    data_item['TEACHER_ANSWER'] = teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Formatting for JumpStart Training\n",
    "\n",
    "Converts input data into JSONL format following SageMaker JumpStart chat template specifications. For more information about JumpStart data formats, see [Training Data Format](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning-instruction-based.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_qa_training_data(dataset, output_file='train.jsonl', max_samples=5000):\n",
    "    \"\"\"\n",
    "    Transform the dataset into JSONL format with a dialog structure including a system message for Llama fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of dictionaries containing 'QUESTION', 'CONTEXTS', 'LONG_ANSWER', 'TEACHER_ANSWER'\n",
    "        output_file: Output JSONL file path\n",
    "        max_samples: Maximum number of samples to include\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"\"\"You are a specialized biomedical research assistant trained to analyze and answer questions about medical and scientific literature. Your role is to:\n",
    "    - Extract and interpret key information from biomedical research papers, clinical studies, and medical literature\n",
    "    - Provide accurate, evidence-based responses based solely on the provided research context\n",
    "    - Focus on specific medical findings, methodologies, and clinical outcomes\n",
    "    - Present complex medical information in clear, understandable terms\n",
    "    - Maintain precision when discussing medical terminology, study results, and statistical data\n",
    "    - Distinguish between preliminary findings and established conclusions\n",
    "    - Reference specific sections of the provided research when answering questions\n",
    "    - Acknowledge limitations in studies when relevant\n",
    "    - Avoid making medical recommendations or providing diagnosis\n",
    "    When responding, only use information explicitly stated in the provided biomedical context.\"\"\"\n",
    "\n",
    "    # Limit the number of samples if specified\n",
    "    #dataset = dataset[:max_samples] if max_samples else dataset\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            try:\n",
    "                # Create the dialog structure with system message\n",
    "                dialog = [\n",
    "                    {\n",
    "                        \"content\": f\"<<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{item['QUESTION']}\",\n",
    "                        \"role\": \"user\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"content\": item['TEACHER_ANSWER'],\n",
    "                        \"role\": \"assistant\"\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # Create the JSON object\n",
    "                json_object = {\n",
    "                    \"dialog\": dialog\n",
    "                }\n",
    "                \n",
    "                # Write the JSON line\n",
    "                f.write(json.dumps(json_object) + '\\n')\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping item due to missing key: {e}\")\n",
    "                continue\n",
    "\n",
    "def verify_jsonl(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if i == 0:  # Print first example\n",
    "                    print(\"Sample entry:\")\n",
    "                    print(json.dumps(data, indent=2))\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {i+1}: {e}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "create_qa_training_data(dataset, output_file='train.jsonl', max_samples=5000)\n",
    "verify_jsonl('train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model (LLaMA 3B)\n",
    "Selecting Student Model in JumpStart\n",
    "Details about:\n",
    "\n",
    "- Available model options in JumpStart\n",
    "- Selection criteria for student model\n",
    "- Configuration parameters\n",
    "- Resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "\n",
    "default_bucket_prefix = sagemaker.Session().default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{bucket}{default_bucket_prefix_path}/oasst_top1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Student Model in JumpStart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "\n",
    "try:\n",
    "    dropdown = Dropdown(\n",
    "        options=list_jumpstart_models(\"search_keywords includes Text Generation\"),\n",
    "        value=\"meta-textgeneration-llama-3-2-1b\",\n",
    "        description=\"Select a JumpStart text generation model:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "    display(dropdown)\n",
    "except:\n",
    "    dropdown = None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dropdown:\n",
    "    student_model_id = dropdown.value\n",
    "else:\n",
    "    # Provide model id as meta-textgeneration-llama-3-1-405b-instruct-fp8 for the instruct variant\n",
    "    model_id = \"meta-textgeneration-llama-3-2-1b\"\n",
    "model_version_student = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import metric_definitions\n",
    "print(metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b-instruct\", model_version='1.1.1',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Training Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters_student = hyperparameters.retrieve_default(\n",
    "    model_id=student_model_id, model_version=model_version_student,\n",
    ")\n",
    "\n",
    "print(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters_student[\"epoch\"] = \"1\"\n",
    "my_hyperparameters_student['chat_dataset']=\"True\"\n",
    "my_hyperparameters_student['instruction_tuned']=\"False\"\n",
    "my_hyperparameters_student['seed']=\"10\"# this could help us to have the same results\n",
    "\n",
    "hyperparameters.validate(\n",
    "    model_id=student_model_id, model_version=model_version_student, hyperparameters=my_hyperparameters_student\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import ContinuousParameter, CategoricalParameter,IntegerParameter\n",
    "\n",
    "# Define hyperparameter ranges without as_json_range\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.0005, scaling_type=\"Logarithmic\"),\n",
    "    'lora_r': CategoricalParameter(['4', '8', '12', '16']),\n",
    "    'lora_alpha': CategoricalParameter(['16', '32', '48', '64']),\n",
    "    'lora_dropout': ContinuousParameter(0.01, 0.2),\n",
    "    'per_device_train_batch_size': CategoricalParameter(['2', '4', '6', '8']),\n",
    "    'gradient_accumulation_steps': CategoricalParameter(['1', '2', '3', '4']),\n",
    "    'max_steps': CategoricalParameter(['50', '75', '100']),\n",
    "    'warmup_steps': CategoricalParameter(['5', '7', '10']),\n",
    "    'num_train_epochs': CategoricalParameter(['1', '2'])\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "\n",
    "metric_defs=metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-1b\", model_version='1.1.1',)\n",
    "print(metric_defs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_metrics = [\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'train:loss', 'Regex': 'train_loss = ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = metric_defs + memory_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Create the estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=my_hyperparameters_student,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    metric_definitions=combined_metrics,  # Add metric definitions here\n",
    "    enable_sagemaker_metrics=True  # Enable SageMaker metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='huggingface-textgeneration:train-loss',\n",
    "    metric_definitions=combined_metrics,\n",
    "    objective_type='Minimize',\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=4,#Adjust depending the available instances\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy='Bayesian',\n",
    "    base_tuning_job_name='llm-llama-3-2-1b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the hyperparameter tuning job\n",
    "tuner.fit({\"training\": train_data_location}, wait=True)\n",
    "# First, wait for the tuning job to complete\n",
    "tuner.wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job)['HyperParameters']\n",
    "print(\"Best hyperparameters: \\n\")\n",
    "pprint.pprint(best_hyperparameters_student_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_id_2='meta-textgeneration-llama-3-2-3b'\n",
    "metric_defs=metric_definitions.retrieve_default(model_id=\"meta-textgeneration-llama-3-2-3b\", model_version='1.1.1',)\n",
    "\n",
    "# Define hyperparameter ranges for tuning\n",
    "hyperparameter_ranges = {\n",
    "    # Learning rate - logarithmic scale for better exploration\n",
    "    'learning_rate': ContinuousParameter(1e-5, 5e-4, scaling_type=\"Logarithmic\"),\n",
    "    \n",
    "    # LoRA specific parameters\n",
    "    'lora_r': CategoricalParameter(['8', '16', '32', '64']),  # Higher ranks possible with 24GB GPUs\n",
    "    'lora_alpha': CategoricalParameter(['16', '32', '64', '128']),  # Scaled with lora_r\n",
    "    'lora_dropout': ContinuousParameter(0.05, 0.2),  # Wider range for regularization\n",
    "    \n",
    "    # Batch size optimization for 4x A10G GPUs\n",
    "    'per_device_train_batch_size': CategoricalParameter(['4', '8', '16']),  # Larger due to 4 GPUs\n",
    "    'gradient_accumulation_steps': CategoricalParameter(['1', '2', '4', '8']),  # Adjusted for total batch size\n",
    "    \n",
    "    # Training dynamics\n",
    "    'max_steps': CategoricalParameter(['200', '400', '600']),  # More steps for initial evaluation\n",
    "    'warmup_steps': CategoricalParameter(['20', '40', '60']),  # 10% of max_steps\n",
    "    'num_train_epochs': CategoricalParameter(['1', '2'])  # Initial epochs for evaluation\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Create the estimator\n",
    "estimator_2 = JumpStartEstimator(\n",
    "    model_id=student_model_id_2,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=my_hyperparameters_student,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    metric_definitions=combined_metrics,  # Add metric definitions here\n",
    "    enable_sagemaker_metrics=True  # Enable SageMaker metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter tuner\n",
    "tuner_2=HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='huggingface-textgeneration:train-loss',\n",
    "    metric_definitions=combined_metrics,\n",
    "    objective_type='Minimize',\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=4,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy='Bayesian',\n",
    "    base_tuning_job_name='llm-llama-3-2-3b',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the hyperparameter tuning job\n",
    "tuner_2.fit({\"training\": train_data_location}, wait=True)\n",
    "# First, wait for the tuning job to complete\n",
    "tuner_2.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job\n",
    "best_training_job_1 = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")\n",
    "\n",
    "\n",
    "# Get the best hyperparameters using the SageMaker client\n",
    "best_hyperparameters_student_1 = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_1)['HyperParameters']\n",
    "print(f\"Best hyperparameters: {best_hyperparameters_student_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Training Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(best_hyperparameters_student_1)\n",
    "best_hyperparameters_student_1['num_train_epochs']=10\n",
    "best_hyperparameters_student_1['epoch']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "# Create proper TensorBoard output configuration\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=f's3://{bucket}/tensorboard-logs/llama3-model-distillation',\n",
    "    container_local_output_path='/opt/ml/output/tensorboard'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "student_model_id = \"meta-textgeneration-llama-3-2-1b\"\n",
    "model_version_student = \"*\"\n",
    "\n",
    "estimator_student = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=best_hyperparameters_student_1,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\",\n",
    "        \"TENSORBOARD_LOGGING\": \"true\",\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    "    tensorboard_output_config=tensorboard_output_config  # Use the proper config object\n",
    ")\n",
    "# Define metrics to track\n",
    "metric_definitions = [\n",
    "    # Training Metrics\n",
    "    {'Name': 'train:loss', 'Regex': 'step .* is completed and loss is ([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:perplexity', 'Regex': 'train_perplexity=([0-9\\\\.]+)'},\n",
    "    {'Name': 'train:epoch_loss', 'Regex': 'train_epoch_loss=([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    {'Name': 'eval:loss', 'Regex': 'eval_epoch_loss=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    {'Name': 'eval:perplexity', 'Regex': 'eval_ppl=tensor\\\\(([0-9\\\\.]+)'},\n",
    "    \n",
    "    # Performance Metrics\n",
    "    {'Name': 'epoch_time', 'Regex': 'epcoh time ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_throughput', 'Regex': '([0-9\\\\.]+)it/s'},\n",
    "    \n",
    "    # Memory Usage\n",
    "    {'Name': 'gpu:memory_allocated', 'Regex': 'Max CUDA memory allocated was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:memory_reserved', 'Regex': 'Max CUDA memory reserved was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'gpu:peak_active_memory', 'Regex': 'Peak active CUDA memory was ([0-9\\\\.]+) GB'},\n",
    "    {'Name': 'cpu:peak_memory', 'Regex': 'CPU Total Peak Memory consumed during the train \\\\(max\\\\): ([0-9\\\\.]+) GB'}\n",
    "]\n",
    "# Add metrics to estimator\n",
    "estimator_student.metric_definitions = metric_definitions\n",
    "# Launch TensorBoard in SageMaker Studio\n",
    "tensorboard_callback = {\n",
    "    'Config': {\n",
    "        'TrainingJobName': 'llama-3-2-3b-model-distilation'\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_student.fit({\"training\": train_data_location},\n",
    "    wait=True,\n",
    "    logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "### Deploying Student Model Endpoint or Bedrock CMI \n",
    "Explains:\n",
    "\n",
    "- Deployment options (SageMaker endpoints vs Bedrock)\n",
    "- Configuration requirements\n",
    "- Monitoring setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model import in Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training job name and model URI\n",
    "training_job_name = estimator_student._current_job_name\n",
    "model_uri = estimator_student.model_data['S3DataSource']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME = 'us-east-1'\n",
    "bedrock = boto3.client(service_name='bedrock',\n",
    "                       region_name=REGION_NAME)\n",
    "# Generate a uni\n",
    "timestamp = int(time.time())\n",
    "random_number = random.randint(1000, 9999)\n",
    "JOB_NAME = f\"meta3-import-model-{timestamp}-{random_number}\"\n",
    "\n",
    "ROLE_ARN = bedrock_role_arn\n",
    "IMPORTED_MODEL_NAME = f\"llama3_1_student_{timestamp}-{random_number}\"\n",
    "S3_URI = model_uri\n",
    "\n",
    "# createModelImportJob API\n",
    "create_job_response = bedrock.create_model_import_job(\n",
    "    jobName=JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=ROLE_ARN,\n",
    "    modelDataSource={\n",
    "        \"s3DataSource\": {\n",
    "            \"s3Uri\": model_uri\n",
    "        }\n",
    "    },\n",
    ")\n",
    "job_arn = create_job_response.get(\"jobArn\")\n",
    "print(f\"Model import job created with ARN: {job_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_filter = IMPORTED_MODEL_NAME  # Replace with your model name\n",
    "model_info = wait_for_model_availability(model_name_filter,max_attempts=30,delay=60)\n",
    "#\n",
    "if model_info:\n",
    "    model_arn=model_info[\"modelArn\"]\n",
    "    print(\"Model is now available in Bedrock.\")\n",
    "else:\n",
    "    print(\"Failed to find the model in Bedrock within the specified attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "REGION_NAME = 'us-east-1'\n",
    "MODEL_ID= model_arn\n",
    "\n",
    "config = Config(\n",
    "    retries={\n",
    "        'total_max_attempts': 100, \n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "message = \"Hello, what it is the weather in seattle?\"\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "br_runtime = session.client(service_name = 'bedrock-runtime', \n",
    "                                 region_name=REGION_NAME, \n",
    "                                 config=config)\n",
    "    \n",
    "try:\n",
    "    invoke_response = br_runtime.invoke_model(modelId=MODEL_ID, \n",
    "                                            body=json.dumps({'prompt': message}), \n",
    "                                            accept=\"application/json\", \n",
    "                                            contentType=\"application/json\")\n",
    "    invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    print(json.dumps(invoke_response, indent=4))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(e.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Environment Setup\n",
    "\n",
    "#### Configuration Requirements\n",
    "\n",
    "1. **SageMaker Studio Environment**\n",
    "   - Use SageMaker Studio Code Editor\n",
    "   - Minimum instance: `ml.t3.xlarge`\n",
    "   - Storage: 50GB minimum\n",
    "   - Required IAM role permissions:\n",
    "     ```json\n",
    "     {\n",
    "         \"Effect\": \"Allow\",\n",
    "         \"Principal\": {\n",
    "             \"Service\": \"sagemaker.amazonaws.com\"\n",
    "         },\n",
    "         \"Action\": \"sts:AssumeRole\"\n",
    "     }\n",
    "     ```\n",
    "\n",
    "2. **FMBench Environment Setup**\n",
    "   ```bash\n",
    "   # Create and activate conda environment\n",
    "   conda create --name fmbench_python311 -y python=3.11 ipykernel\n",
    "   source activate fmbench_python311\n",
    "   \n",
    "   # Install FMBench\n",
    "   pip install -U fmbench\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Setup\n",
    "\n",
    "1. **Directory Configuration**\n",
    "   ```bash\n",
    "   # Set working directory\n",
    "   mkdir fmbench \n",
    "   export EVAL_DIR=\"tmp\"\n",
    "   mkdir -p $EVAL_DIR\n",
    "\n",
    "   # Download FMBench dependencies\n",
    "   curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$EVAL_DIR\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Evaluation Execution**\n",
    "```bash\n",
    "   # Run evaluation\n",
    "    fmbench --config-file $EVAL_DIR/fmbench-read/configs/llama3/config-bedrock-llama3.yml \\\n",
    "        --local-mode yes \\\n",
    "        --write-bucket s3://{your-bucket}/model-evaluation \\\n",
    "        --tmp-dir $EVAL_DIR > $EVAL_DIR/fmbench.log 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Monitor Progress**\n",
    "```bash\n",
    "   # View live logs\n",
    "   tail -f $EVAL_DIR/fmbench.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Collection**\n",
    "- Evaluation metrics stored in: $EVAL_DIR/fmbench-write/\n",
    "- Results automatically uploaded to: s3://{your-bucket}/model-evaluation/\n",
    "- Report artifacts include:\n",
    "    - Performance metrics (CSV)\n",
    "    - Visualization plots (PNG)\n",
    "    - Interactive dashboards (HTML)\n",
    "    - Raw evaluation data (JSON)\n",
    "\n",
    "    **Note**: Replace {your-bucket} with your S3 bucket name. Ensure the IAM role has appropriate S3 permissions.\n",
    "\n",
    "**Accesing Results**\n",
    "```python\n",
    "\n",
    "# Example code to load evaluation results (to be implemented)\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_evaluation_results(bucket, prefix):\n",
    "    # Load evaluation results from S3\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to load evaluation results (to be implemented)\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def load_evaluation_results(bucket, prefix):\n",
    "    # Load evaluation results from S3\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Testing (Teacher vs Student)\n",
    "This section presents the results from FMBench evaluation comparing the teacher model (Llama 70B) and student model (Llama 1B).\n",
    "\n",
    "#### Evaluation Metrics\n",
    "| Model | Judge Accuracy (Cohere) | Judge Accuracy (Claude) | Judge Accuracy (Llama) | Majority Voting |\n",
    "|-------|------------------------|------------------------|---------------------|-----------------|\n",
    "| Teacher (70B) | 96.52% | 92.75% | 91.02% | 93.02% |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "> **Note**: Model evaluations performed by 3 LLM judges using ground truth comparison\n",
    "\n",
    "\n",
    "#### Testing Methodology\n",
    "- Dataset: Multiple QA datasets from LongBench\n",
    "- Prompt lengths: 500-3840 tokens\n",
    "- Concurrency levels: 1-4\n",
    "- Evaluation criteria: Accuracy, latency, cost\n",
    "[Display accuracy_trajectory_per_payload.png]\n",
    "*Figure 1: Accuracy across different prompt lengths*\n",
    "\n",
    "#### Performance Comparison\n",
    "\n",
    "**Latency Metrics**\n",
    "| Model | p50 Latency | p95 Latency | p99 Latency | Transactions/min |\n",
    "|-------|-------------|-------------|-------------|------------------|\n",
    "| Teacher (70B) | 5.27s | 5.27s | 5.27s | 2 |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "[Display tokens_vs_latency.png]\n",
    "*Figure 2: Token processing latency comparison*\n",
    "\n",
    "\n",
    "\n",
    "- Performance comparison\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Analysis\n",
    "\n",
    "#### Latency Measurements\n",
    "- Time to First Token (TTFT)\n",
    "- Time Per Output Token (TPOT)\n",
    "- Overall response latency\n",
    "[Display concurrency_vs_inference_latency.png]\n",
    "*Figure 3: Concurrency vs Inference Latency*\n",
    "\n",
    "#### Throughput Analysis\n",
    "| Model | Prompt Token Throughput | Completion Token Throughput |\n",
    "|-------|------------------------|---------------------------|\n",
    "| Teacher (70B) | 203 tokens/s | 2 tokens/s |\n",
    "| Student (3B) | [Pending] | [Pending] |\n",
    "\n",
    "#### Cost Comparison\n",
    "| Model | Price per Transaction | Price per Token | Cost per 10k Transactions |\n",
    "|-------|---------------------|----------------|------------------------|\n",
    "| Teacher (70B) | $0.002875 | $0.00000072 | $28.75 |\n",
    "| Student (3B) | [Pending] | [Pending] | [Pending] |\n",
    "\n",
    "[Display business_summary.png]\n",
    "*Figure 4: Price Performance Comparison*\n",
    "\n",
    "#### Quality Metrics\n",
    "Error rates and model accuracy across different prompt lengths:\n",
    "\n",
    "[Display error_rates.png]\n",
    "*Figure 5: Error Rates by Model and Concurrency*\n",
    "\n",
    "> **Note**: Full interactive versions of these visualizations are available in the evaluation report at `s3://{bucket}/fmbench-results/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/aws-samples/foundation-model-benchmarking-tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Deployment(Jumpstart or Bedrock)\n",
    "This section talks about differences using Jumpstart deployment vs BedRock\n",
    "### Endpoint Configuration \n",
    "\n",
    "#### SageMaker JumpStart Endpoints\n",
    "- Provides complete infrastructure control through endpoint configurations\n",
    "- Supports custom containers and model serving code\n",
    "- Enables A/B testing through production variants\n",
    "- Requires endpoint management and maintenance\n",
    "\n",
    "#### Bedrock Custom Model Import\n",
    "- Offers serverless deployment with minimal configuration\n",
    "- Streamlines deployment through model import workflow\n",
    "- Integrates automatically with AWS AI services\n",
    "- Manages infrastructure automatically\n",
    "\n",
    "### Scaling and Cost Management \n",
    "\n",
    "#### SageMaker JumpStart\n",
    "- Instance-based pricing with reserved capacity\n",
    "- Auto-scaling based on custom metrics\n",
    "- Granular control over instance types and counts\n",
    "- Best for consistent, high-throughput workloads\n",
    "\n",
    "#### Bedrock Custom Model Import\n",
    "- Pay-per-invocation pricing model\n",
    "- Built-in automatic scaling\n",
    "- No minimum commitment required\n",
    "- Optimal for variable workload patterns\n",
    "\n",
    "### Monitoring Setup \n",
    "\n",
    "#### SageMaker JumpStart\n",
    "- CloudWatch integration for custom metrics\n",
    "- Model monitoring for drift detection\n",
    "- Detailed logging and debugging capabilities\n",
    "- Advanced endpoint metrics and alarms\n",
    "#### Bedrock Custom Model Import\n",
    "- Simplified monitoring through AWS Console\n",
    "- Built-in performance metrics\n",
    "- Automated operational monitoring\n",
    "- Streamlined logging integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cleanup and Best Practices\n",
    "\n",
    "### Resource Termination\n",
    "\n",
    "1. Delete the Bedrock Custom Model\n",
    "First, let's remove the custom model from Amazon Bedrock:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bedrock_custom_model(model_name):\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    try:\n",
    "        bedrock_client.delete_imported_model(modelIdentifier=model_name)\n",
    "        print(f\"Successfully deleted Bedrock custom model: {model_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        error_code = error.response['Error']['Code']\n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"Error deleting Bedrock custom model: The provided model name is invalid. Model Name: {model_name}\")\n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            print(f\"Error: The model '{model_name}' was not found in Bedrock.\")\n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            print(\"Error: You do not have permission to delete this model.\")\n",
    "        elif error_code == 'ConflictException':\n",
    "            print(\"Error: The model is currently in use or in a state that doesn't allow deletion.\")\n",
    "        else:\n",
    "            print(f\"Error deleting Bedrock custom model: {error}\")\n",
    "\n",
    "# Replace with your actual model name\n",
    "MODEL_NAME = \"llama3-qa-model\"\n",
    "delete_bedrock_custom_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Delete IAM Roles\n",
    "Now, let's remove the IAM roles we created specifically for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_iam_role(role_name):\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        # Delete inline policies\n",
    "        inline_policies = iam.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy in inline_policies:\n",
    "            iam.delete_role_policy(RoleName=role_name, PolicyName=policy)\n",
    "            \n",
    "        # Detach managed policies\n",
    "        attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "            \n",
    "        # Delete permissions boundary if it exists\n",
    "        try:\n",
    "            iam.delete_role_permissions_boundary(RoleName=role_name)\n",
    "        except iam.exceptions.NoSuchEntityException:\n",
    "            pass\n",
    "        \n",
    "        # Finally delete the role\n",
    "        iam.delete_role(RoleName=role_name)\n",
    "        print(f\"Successfully deleted IAM role: {role_name}\")\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(f\"Error deleting IAM role: {error}\")\n",
    "\n",
    "# Delete LambdaBedrockExecutionRole\n",
    "delete_iam_role(\"LambdaBedrockExecutionRole\")\n",
    "\n",
    "# Delete Sagemaker_Bedrock_import_role\n",
    "delete_iam_role(\"Sagemaker_Bedrock_import_role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "#### Cost Optimization Tips\n",
    "1. Training Optimization\n",
    "    - Use spot instances for training when possible\n",
    "    - Implement early stopping in training jobs\n",
    "    - Clean up training artifacts promptly\n",
    "    - Monitor training metrics to avoid unnecessary epochs\n",
    "2. Inference Optimization\n",
    "    - Choose between Bedrock and SageMaker based on workload patterns\n",
    "    - Use auto-scaling for SageMaker endpoints\n",
    "    - Consider batch processing for large-scale inference\n",
    "    - Monitor and adjust instance sizes based on utilization\n",
    "3. Storage Management\n",
    "    - Implement S3 lifecycle policies for training artifacts\n",
    "    - Clean up temporary datasets after training\n",
    "    - Use appropriate storage classes for different data types\n",
    "#### JumpStart Best Practices\n",
    "1. Model Selection\n",
    "2. Training Configuration\n",
    "3. Data Management\n",
    "4. Knowledge Distillation Specific\n",
    "5. Production Deployment\n",
    "#### Security Best Practices\n",
    "1. Access Control\n",
    "2. Monitoring and Compliance\n",
    "\n",
    "For more information, see:\n",
    "\n",
    "- SageMaker Best Practices[link]\n",
    "\n",
    "- Bedrock Security[link]\n",
    "\n",
    "- AWS Machine Learning Security[link]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results \n",
    "### Lessons Learned \n",
    "### Future Improvements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
