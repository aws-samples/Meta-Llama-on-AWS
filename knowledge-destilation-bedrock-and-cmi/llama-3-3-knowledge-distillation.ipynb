{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response-Based Knowledge Distillation with QA Specialization\n",
    "\n",
    "## Using Amazon SageMaker JumpStart for LLM distilation (70B â†’ 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Overview of Knowledge Distillation \n",
    "### SageMaker JumpStart Benefits \n",
    "### Project Goals and Objectives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker jmespath datasets transformers jinja2 fmeval ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "### AWS Account Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #change the name of the role if you are running locally\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20220929T161862')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "region=sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"llama-qa-distillation\"\n",
    "\n",
    "# Print AWS configuration\n",
    "print(f\"SageMaker Session: {sess}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role configuration\n",
    "\n",
    "[TODO] This section includes adding permisions to current role based on requirements for bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Teacher Model (LLaMA 3.3 70B)\n",
    "### Selecting Model in Bedrock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def list_foundation_models(bedrock_client):\n",
    "    \"\"\"\n",
    "    Gets a list of available Amazon Bedrock foundation models.\n",
    "\n",
    "    :return: The list of available bedrock foundation models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock_client.list_foundation_models()\n",
    "        models = response[\"modelSummaries\"]\n",
    "        logger.info(\"Got %s foundation models.\", len(models))\n",
    "        return models\n",
    "\n",
    "    except ClientError:\n",
    "        logger.error(\"Couldn't list foundation models.\")\n",
    "        raise\n",
    "\n",
    "def create_models_dataframe(models):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame with relevant model information.\n",
    "    \n",
    "    :param models: List of model summaries from Bedrock\n",
    "    :return: pandas DataFrame with model information\n",
    "    \"\"\"\n",
    "    model_data = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_info = {\n",
    "            'Model Name': model['modelName'],\n",
    "            'Provider': model['providerName'],\n",
    "            'Model ID': model['modelId'],\n",
    "            'Input Modalities': ', '.join(model['inputModalities']),\n",
    "            'Output Modalities': ', '.join(model['outputModalities']),\n",
    "            'Customizations Supported': ', '.join(model['customizationsSupported']) if 'customizationsSupported' in model else 'None',\n",
    "            'Inference Types': ', '.join(model['inferenceTypesSupported'])\n",
    "        }\n",
    "        model_data.append(model_info)\n",
    "    \n",
    "    df = pd.DataFrame(model_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name=\"bedrock\",region_name=\"us-east-2\")\n",
    "fm_models = list_foundation_models(bedrock_client)\n",
    "\n",
    "# Create DataFrame\n",
    "models_df = create_models_dataframe(fm_models)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nAmazon Bedrock Foundation Models:\")\n",
    "print(models_df.to_string(index=False))\n",
    "\n",
    "# Optionally, you can also save to CSV\n",
    "# models_df.to_csv('bedrock_models.csv', index=False)\n",
    "\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_df['Model ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df[(models_df['Provider']=='Meta') & (models_df['Inference Types']=='INFERENCE_PROFILE')]['Model ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use 'INFERENCE_PROFILE' models you need to create an inference profile, you dont need that for 'ON_DEMAND' models\n",
    "\n",
    "[Note] LLama 3.3 70b only works in us-east-2 not sure if is an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "#only us-east-2 let you use llama 3.3 70b us-west-2 fails\n",
    "bedrock_client = boto3.client('bedrock', region_name=\"us-east-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='meta.llama3-3-70b-instruct-v1:0'\n",
    "inference_profile_name='llama3-3-70b-inference'\n",
    "inf_profile_response = bedrock_client.create_inference_profile(\n",
    "    inferenceProfileName=inference_profile_name,\n",
    "    description='Teacher model use for syntetic generation in a Llama distilation project',\n",
    "    modelSource={\n",
    "        'copyFrom': f'arn:aws:bedrock:us-east-2::foundation-model/{model_id}'\n",
    "    },\n",
    "    tags=[\n",
    "        {\n",
    "        'key': 'project',\n",
    "            'value': 'Llama-model-distilation'\n",
    "        },\n",
    "        {\n",
    "        'key': 'model-id',\n",
    "            'value': 'meta.llama3-3-70b-instruct'\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference profile created successfully. ARN: {inf_profile_response['inferenceProfileArn']}\")\n",
    "model_arn=inf_profile_response['inferenceProfileArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_profile_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brt = boto3.client(service_name='bedrock-runtime',region_name='us-east-2')\n",
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    try:\n",
    "        response = brt.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Write me a blog about making strong business decisions as a leader. [/INST]\"\"\"\n",
    "\n",
    "body = {\n",
    "    \"prompt\": prompt_data,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_gen_len\": 512,\n",
    "}\n",
    "\n",
    "modelId = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, modelId, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Generation\n",
    "\n",
    "### Corpus Preparation(Prepare QA + Context dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreaApproved dataset details:\n",
    "\n",
    "https://github.com/pubmedqa/pubmedqa/tree/master\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PubMedQA: A Dataset for Biomedical Research Question Answering**\n",
    ">\n",
    "> Jin, Q., Dhingra, B., Liu, Z., Cohen, W., & Lu, X. (2019). In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 2567-2577."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_github_json(url):\n",
    "    try:\n",
    "        # Convert regular GitHub URL to raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        return requests.get(raw_url).json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://github.com/pubmedqa/pubmedqa/blob/master/data/ori_pqal.json\"\n",
    "data = get_github_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_index=list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qa_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[qa_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for i in qa_index:\n",
    "    keys_to_get = ['QUESTION', 'CONTEXTS','LONG_ANSWER']\n",
    "    result = {k: data[i].get(k) for k in keys_to_get}\n",
    "    dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Teacher Model for QA Generation(Create more data based on the Questions and the context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing the best option is to use batch in Bedrock, sending real time invocations it is slower and can throtle the API and also could be expensive that sendina a batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "def create_bedrock_batch_dataset(dataset, output_file='bedrock_batch_dataset.jsonl'):\n",
    "    system_message = \"\"\"You are a specialized biomedical research assistant trained to analyze and answer questions about medical and scientific literature. Your role is to:\n",
    "        Extract and interpret key information from biomedical research papers, clinical studies, and medical literature\n",
    "        Provide accurate, evidence-based responses based solely on the provided research context\n",
    "        Focus on specific medical findings, methodologies, and clinical outcomes\n",
    "        Present complex medical information in clear, understandable terms\n",
    "        Maintain precision when discussing medical terminology, study results, and statistical data\n",
    "        Distinguish between preliminary findings and established conclusions\n",
    "        Reference specific sections of the provided research when answering questions\n",
    "        Acknowledge limitations in studies when relevant\n",
    "        Avoid making medical recommendations or providing diagnosis When responding, only use information explicitly stated in the provided biomedical context.\"\"\"\n",
    "\n",
    "    prompt_template = \"\"\"System: {system}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear and concise answer.\"\"\"\n",
    "    \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for sample in dataset:\n",
    "            # Generate a unique record ID (11 characters)\n",
    "            record_id = str(uuid.uuid4())[:11]\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = prompt_template.format(\n",
    "                system=system_message,\n",
    "                question=sample[\"QUESTION\"]\n",
    "            )\n",
    "\n",
    "            # Create the model input body for Llama 2\n",
    "            body = {\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 512,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "\n",
    "            # Create the complete record for batch inference\n",
    "            batch_record = {\n",
    "                \"recordId\": record_id,\n",
    "                \"modelInput\": body\n",
    "            }\n",
    "            \n",
    "            outfile.write(json.dumps(batch_record) + '\\n')\n",
    "\n",
    "# Usage\n",
    "create_bedrock_batch_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "# Define source and destination paths\n",
    "local_path_batch_file = 'bedrock_batch_dataset.jsonl'\n",
    "s3_prefix_batch = 'distillation/batch/data'  # This will be the folder in S3\n",
    "\n",
    "# Upload the file\n",
    "s3_path_batch = S3Uploader.upload(\n",
    "    local_path=local_path_batch_file,\n",
    "    desired_s3_uri=f's3://{bucket}/{s3_prefix_batch}',\n",
    ")\n",
    "\n",
    "print(f\"File uploaded successfully to: {s3_path_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code is creating the Bedrock Batch job to create the updated dataset with the teacher knowledge, Llama 3.3 provide the answers to the questions.\n",
    "\n",
    "[TODO] Create more data with other synthetic generation methods or add validation of answers generated by Bedrock using context/answer as a groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix=\"output\"\n",
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": s3_path_batch\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{bucket}/{s3_prefix_batch}/{output_prefix}/\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobName = 'batch-job-ga' + str(int(datetime.now().timestamp()))\n",
    "response=bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role,\n",
    "    #modelId='meta.llama3-3-70b-instruct-v1:0',\n",
    "    modelId='us.meta.llama3-3-70b-instruct-v1:0',\n",
    "    jobName=jobName,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "jobArn = response.get('jobArn')\n",
    "job_id = jobArn.split('/')[1]\n",
    "\n",
    "print(jobArn)\n",
    "\n",
    "status = ''\n",
    "while status not in ['Completed', 'Failed']:\n",
    "    job_response = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)\n",
    "    status = job_response['status']\n",
    "    if status == 'Failed':\n",
    "        print(job_response)\n",
    "    elif status == 'Completed':\n",
    "        print(datetime.now(), \": \", status)\n",
    "        break\n",
    "    else: \n",
    "        print(datetime.now(), \": \", status)\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Formatting for JumpStart/Bedrock (create a dataset compatible with Jumpstart and Bedrock, LLM template format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "prefix = f\"{s3_prefix_batch}/{output_prefix}/{job_id}/\"\n",
    "print(f\"prefix: {bucket}/{prefix}\")\n",
    "object_key = f\"{prefix}{local_path_batch_file}.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.get_object(Bucket=bucket, Key=object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = response['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer=[]\n",
    "for line in json_data.splitlines():\n",
    "        data = json.loads(line)\n",
    "        print(data['modelOutput']['generation'])\n",
    "        teacher_answer.append(data['modelOutput']['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['LONG_ANSWER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_item, teacher in zip(dataset, teacher_answer):\n",
    "    data_item['TEACHER_ANSWER'] = teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function creates the jsonl dataset compatible with SageMaker Jumpstart input format for chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_qa_training_data(dataset, output_file='train.jsonl', max_samples=5000):\n",
    "    \"\"\"\n",
    "    Transform the dataset into JSONL format with a dialog structure including a system message for Llama fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of dictionaries containing 'QUESTION', 'CONTEXTS', 'LONG_ANSWER', 'TEACHER_ANSWER'\n",
    "        output_file: Output JSONL file path\n",
    "        max_samples: Maximum number of samples to include\n",
    "    \"\"\"\n",
    "    # Define the system message\n",
    "    system_message = \"\"\"You are a specialized biomedical research assistant trained to analyze and answer questions about medical and scientific literature. Your role is to:\n",
    "    - Extract and interpret key information from biomedical research papers, clinical studies, and medical literature\n",
    "    - Provide accurate, evidence-based responses based solely on the provided research context\n",
    "    - Focus on specific medical findings, methodologies, and clinical outcomes\n",
    "    - Present complex medical information in clear, understandable terms\n",
    "    - Maintain precision when discussing medical terminology, study results, and statistical data\n",
    "    - Distinguish between preliminary findings and established conclusions\n",
    "    - Reference specific sections of the provided research when answering questions\n",
    "    - Acknowledge limitations in studies when relevant\n",
    "    - Avoid making medical recommendations or providing diagnosis\n",
    "    When responding, only use information explicitly stated in the provided biomedical context.\"\"\"\n",
    "\n",
    "    # Limit the number of samples if specified\n",
    "    #dataset = dataset[:max_samples] if max_samples else dataset\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            try:\n",
    "                # Create the dialog structure with system message\n",
    "                dialog = [\n",
    "                    {\n",
    "                        \"content\": f\"<<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{item['QUESTION']}\",\n",
    "                        \"role\": \"user\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"content\": item['TEACHER_ANSWER'],\n",
    "                        \"role\": \"assistant\"\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # Create the JSON object\n",
    "                json_object = {\n",
    "                    \"dialog\": dialog\n",
    "                }\n",
    "                \n",
    "                # Write the JSON line\n",
    "                f.write(json.dumps(json_object) + '\\n')\n",
    "            except KeyError as e:\n",
    "                print(f\"Skipping item due to missing key: {e}\")\n",
    "                continue\n",
    "\n",
    "def verify_jsonl(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if i == 0:  # Print first example\n",
    "                    print(\"Sample entry:\")\n",
    "                    print(json.dumps(data, indent=2))\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {i+1}: {e}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "create_qa_training_data(dataset, output_file='train.jsonl', max_samples=5000)\n",
    "verify_jsonl('train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Model (LLaMA 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "\n",
    "default_bucket_prefix = sagemaker.Session().default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{bucket}{default_bucket_prefix_path}/oasst_top1\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Student Model in JumpStart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "\n",
    "try:\n",
    "    dropdown = Dropdown(\n",
    "        options=list_jumpstart_models(\"search_keywords includes Text Generation\"),\n",
    "        value=\"meta-textgeneration-llama-3-2-1b\",\n",
    "        description=\"Select a JumpStart text generation model:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "    display(dropdown)\n",
    "except:\n",
    "    dropdown = None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dropdown:\n",
    "    student_model_id = dropdown.value\n",
    "else:\n",
    "    # Provide model id as meta-textgeneration-llama-3-1-405b-instruct-fp8 for the instruct variant\n",
    "    model_id = \"meta-textgeneration-llama-3-2-1b\"\n",
    "model_version_student = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Training Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters_student = hyperparameters.retrieve_default(\n",
    "    model_id=student_model_id, model_version=model_version_student,\n",
    ")\n",
    "\n",
    "print(my_hyperparameters_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters_student[\"epoch\"] = \"1\"\n",
    "my_hyperparameters_student['chat_dataset']=\"True\"\n",
    "my_hyperparameters_student['instruction_tuned']=\"False\"\n",
    "print(my_hyperparameters_student)\n",
    "\n",
    "hyperparameters.validate(\n",
    "    model_id=student_model_id, model_version=model_version_student, hyperparameters=my_hyperparameters_student\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Training Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=student_model_id,\n",
    "    model_version=model_version_student,\n",
    "    hyperparameters=my_hyperparameters_student,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\"\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "### Deploying Student Model Endpoint or Bedrock CMI \n",
    "### Comparative Testing (Teacher vs Student) \n",
    "### Performance Metrics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization and Tuning(This section can me ommited)\n",
    "\n",
    "### Fine-tuning Hyperparameters \n",
    "### JumpStart Model Retraining \n",
    "### Performance Improvement Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Deployment(Jumpstart or Bedrock)\n",
    "\n",
    "### Endpoint Configuration \n",
    "### Scaling and Cost Management \n",
    "### Monitoring Setup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Cleanup and Best Practices\n",
    "\n",
    "### Resource Termination \n",
    "### Cost Optimization Tips \n",
    "### JumpStart Best Practices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results \n",
    "### Lessons Learned \n",
    "### Future Improvements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
